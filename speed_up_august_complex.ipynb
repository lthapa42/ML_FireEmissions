{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e000472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import path\n",
    "import os\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "np.set_printoptions(threshold=100000)\n",
    "from shapely.geometry import Polygon, Point, MultiPoint, LineString, LinearRing\n",
    "from shapely.ops import cascaded_union, unary_union, transform\n",
    "import datetime\n",
    "import math\n",
    "from scipy.ndimage.interpolation import shift\n",
    "import scipy.interpolate as si\n",
    "import shapely.wkt\n",
    "from shapely.validation import explain_validity,make_valid\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "from my_functions import sat_vap_press, vap_press, hot_dry_windy, haines\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from os.path import exists\n",
    "import rasterio\n",
    "from rasterio.windows import get_data_window,Window, from_bounds\n",
    "from rasterio.plot import show\n",
    "from itertools import product\n",
    "\n",
    "from timezonefinder import TimezoneFinder\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "855972d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/lthapa/ML_daily/fire_polygons/august_complex_VIIRS_daily_12Z_day_start.geojson\n",
      "epsg:3347\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/ML_py/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML_py/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML_py/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML_py/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f7bd9cb5adfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m#calculate fuel-weighted FWI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mfuel_loading_fwi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuel_loading_fwi_timeseries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfire_daily\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuel_loading_fwi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mfuel_loading_fwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./fire_features/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfire_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_Daily_FUEL_FWI_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Z_day_start.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#daily averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-98bc34fca193>\u001b[0m in \u001b[0;36mfuel_loading_fwi_timeseries\u001b[0;34m(df, day_start_hour)\u001b[0m\n\u001b[1;32m     11\u001b[0m     fuel_loadings = Parallel(n_jobs=2)(delayed(calculate_loading_grid_intersection)\n\u001b[1;32m     12\u001b[0m                                  \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloadings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                                  for ii in range(len(df)))\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m#df_loadings=pd.DataFrame(fuel_loadings)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML_py/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML_py/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    955\u001b[0m                     \u001b[0;31m# scheduling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML_py/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mabort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \"\"\"Shutdown the workers and restart a new one with the same parameters\n\u001b[1;32m    560\u001b[0m         \"\"\"\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML_py/lib/python3.7/site-packages/joblib/executor.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self, kill_workers)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkill_workers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m# When workers are killed in such a brutal manner, they cannot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML_py/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, kill_workers)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexecutor_manager_thread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0mexecutor_manager_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;31m# To reduce the risk of opening too many files, remove references to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML_py/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML_py/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#process features using my polygons\n",
    "sit209=pd.read_csv('../Query2.txt')\n",
    "sit209.columns = ['INC209R_IDENTIFIER','INCIDENT_NAME','REPORT_FROM_DATE','REPORT_TO_DATE',\n",
    "              'RESTYP_IDENTIFIER', 'RESOURCE_QUANTITY','RESOURCE_PERSONNEL','CODE_NAME',\n",
    "              'INC_IDENTIFIER','PCT_CONTAINED_COMPLETED'] \n",
    "\n",
    "\n",
    "#2020 fires\n",
    "#fire_incidents = ['LAKE','DOLAN','BOBCAT','CREEK','AUGUST COMPLEX']\n",
    "fire_incidents=['AUGUST COMPLEX']\n",
    "\n",
    "path_poly = '/data2/lthapa/ML_daily/fire_polygons/'\n",
    "suffix_poly = 'Z_day_start.geojson'\n",
    "start_time=12\n",
    "for jj in range(len(fire_incidents)):\n",
    "    fire_name = fire_incidents[jj].lower().replace(' ','_')\n",
    "    print(path_poly+fire_name+'_VIIRS_daily_'+str(start_time)+suffix_poly)\n",
    "    fire_daily = gpd.read_file(path_poly+fire_name+'_VIIRS_daily_'+str(start_time)+suffix_poly)\n",
    "    print(fire_daily.crs)\n",
    "    fire_daily=fire_daily.drop(columns=['Current Overpass'])\n",
    "    fire_daily = fire_daily.drop(np.where(fire_daily['geometry']==None)[0])\n",
    "    fire_daily['fire area (ha)'] = fire_daily['geometry'].area/10000 #hectares. from m2\n",
    "    fire_daily.set_geometry(col='geometry', inplace=True) #designate the geometry column\n",
    "    fire_daily = fire_daily.rename(columns={'Current Day':'UTC Day', 'Local Day': str(start_time)+ 'Z Start Day'})\n",
    "    \n",
    "    fire_daily = fire_daily.iloc[np.array(fire_daily['UTC Day'].values,dtype='datetime64')<=np.datetime64('2020-10-31'),:]\n",
    "    \n",
    "    #calculate fuel-weighted FWI\n",
    "    fuel_loading_fwi = fuel_loading_fwi_timeseries(fire_daily.iloc[0:4],12)\n",
    "    print(fuel_loading_fwi)\n",
    "    fuel_loading_fwi.to_csv('./fire_features/'+fire_name+'_Daily_FUEL_FWI_'+str(start_time)+'Z_day_start.csv') #daily averages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b5f085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuel_loading_fwi_timeseries(df,day_start_hour):\n",
    "    loadings = pd.read_csv('FWI_Category_Fuel_Loadings.csv').set_index('VALUE') #we will index by the fuel type!\n",
    "    \"\"\"for ii in range(1):#(len(df))):\n",
    "        print(ii)\n",
    "        today_poly = df.iloc[ii:ii+1] #get the first one, to test it\n",
    "        fire_fuel_intersection = calculate_loading_grid_intersection(today_poly,50, loadings)\n",
    "        print(fire_fuel_intersection)\"\"\"\n",
    "        \n",
    "        \n",
    "    tic = datetime.datetime.now()    \n",
    "    fuel_loadings = Parallel(n_jobs=2)(delayed(calculate_loading_grid_intersection)\n",
    "                                 (df.iloc[ii:ii+1],50, loadings) \n",
    "                                 for ii in range(len(df)))\n",
    "    #df_loadings=pd.DataFrame(fuel_loadings)\n",
    "    \n",
    "    toc = datetime.datetime.now()\n",
    "    print(toc-tic)\n",
    "    df_loadings = pd.concat(fuel_loadings)\n",
    "    return df_loadings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce44a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#poly is the polygon for one timestep (in lcc)\n",
    "#grid is an xarray of the subset of the grid\n",
    "#grid_names is a string array [0:'lat_center_name',1:'lon_center_name',2:'lat_corner_name',3:'lon_corner_name']\n",
    "\n",
    "def calculate_loading_grid_intersection(poly,bf, loadings):\n",
    "    #get the bounds of the buffered polygons\n",
    "    poly_latlon =poly.to_crs(epsg=5070)\n",
    "    bounds = poly_latlon.buffer(bf).bounds\n",
    "    #print(bounds)\n",
    "    \n",
    "    left = bounds['minx'].values\n",
    "    right = bounds['maxx'].values\n",
    "    bottom=bounds['miny'].values\n",
    "    top=bounds['maxy'].values\n",
    "    \n",
    "    print(left, bottom, top,right)\n",
    "    \n",
    "    #load in the subset of the geotiff\n",
    "    with rasterio.open('../FCCS_Fuel_Fire_Danger_Metric.tif') as src:\n",
    "        win = from_bounds(left,bottom,right,top, src.transform)\n",
    "        #print(win)\n",
    "        w_from_latlon = src.read(1, window=win)\n",
    "        w_flat = w_from_latlon.flatten()\n",
    "        \n",
    "    #build the lat/lon vectors associated with the data we pulled    \n",
    "    rows = np.arange(win.row_off,win.row_off+win.height-1)\n",
    "    cols = np.arange(win.col_off,win.col_off+win.width-1)\n",
    "    COLS,ROWS=np.meshgrid(cols,rows)\n",
    "    xs, ys = rasterio.transform.xy(src.transform, ROWS, COLS)\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "    \n",
    "    xs_corner, ys_corner = calculate_grid_cell_corners(xs,xs)\n",
    "    \n",
    "    grid = build_grid_xarray(ys_corner, xs_corner, ys, xs)\n",
    "    \n",
    "    #add the loadings to the grid\n",
    "    loading_new_extreme = loadings.loc[w_flat,'Extreme_N'].values.reshape(w_from_latlon.shape)\n",
    "    loading_new_veryhigh = loadings.loc[w_flat,'VeryHigh_N'].values.reshape(w_from_latlon.shape)\n",
    "    loading_new_high = loadings.loc[w_flat,'High_N'].values.reshape(w_from_latlon.shape)\n",
    "    loading_new_moderate = loadings.loc[w_flat,'Moderate_N'].values.reshape(w_from_latlon.shape)\n",
    "    loading_new_low = loadings.loc[w_flat,'Low_N'].values.reshape(w_from_latlon.shape)\n",
    "    \n",
    "    grid['loading_new_extreme'] = xr.Variable(dims = ('rows_ctr','cols_ctr'),data=loading_new_extreme)\n",
    "    grid['loading_new_veryhigh'] = xr.Variable(dims = ('rows_ctr','cols_ctr'),data=loading_new_veryhigh)\n",
    "    grid['loading_new_high'] = xr.Variable(dims = ('rows_ctr','cols_ctr'),data=loading_new_high)\n",
    "    grid['loading_new_moderate'] = xr.Variable(dims = ('rows_ctr','cols_ctr'),data=loading_new_moderate)\n",
    "    grid['loading_new_low'] = xr.Variable(dims = ('rows_ctr','cols_ctr'),data=loading_new_low)\n",
    "    \n",
    "    #xs=np.array(xs).flatten() \n",
    "    #ys=np.array(ys).flatten()\n",
    "    xs_sub = []\n",
    "    ys_sub = []\n",
    "    inds_row =[]\n",
    "    inds_col = []\n",
    "\n",
    "    #subset the x,y values by bounds of the multipolygon\n",
    "    if poly_latlon['geometry'].values[0].geom_type=='Polygon':\n",
    "        geom_bounds = poly_latlon['geometry'].values[0].bounds\n",
    "        inds_row_sel,inds_col_sel = np.where((xs>geom_bounds[0])&(xs<geom_bounds[2])&\n",
    "                            (ys>geom_bounds[1])&(ys<geom_bounds[3]))\n",
    "        xs_sub.append(xs[inds_row_sel,inds_col_sel])\n",
    "        ys_sub.append(ys[inds_row_sel,inds_col_sel])\n",
    "        inds_row.append(inds_row_sel)\n",
    "        inds_col.append(inds_col_sel)\n",
    "            \n",
    "    elif poly_latlon['geometry'].values[0].geom_type=='MultiPolygon':\n",
    "        for geom in poly_latlon['geometry'].values[0].geoms:\n",
    "            geom_bounds = geom.bounds\n",
    "            inds_row_sel,inds_col_sel = np.where((xs>geom_bounds[0])&(xs<geom_bounds[2])&\n",
    "                            (ys>geom_bounds[1])&(ys<geom_bounds[3]))\n",
    "            xs_sub.append(xs[inds_row_sel,inds_col_sel])\n",
    "            ys_sub.append(ys[inds_row_sel,inds_col_sel])\n",
    "            inds_row.append(inds_row_sel)\n",
    "            inds_col.append(inds_col_sel)\n",
    "            \n",
    "    xs_sub = np.concatenate(xs_sub)\n",
    "    ys_sub = np.concatenate(ys_sub)\n",
    "    inds_row = np.concatenate(inds_row)\n",
    "    inds_col = np.concatenate(inds_col)\n",
    "    \n",
    "    print(len(xs.flatten()),len(xs_sub))\n",
    "    tic = datetime.datetime.now()\n",
    "    results = Parallel(n_jobs=14)(delayed(build_one_fccs_grid_cell)\n",
    "                                 (xs_sub, ys_sub,inds_row, inds_col,kk,15) \n",
    "                                 for kk in range(len(xs_sub)))\n",
    "    toc = datetime.datetime.now()\n",
    "    print(toc-tic)\n",
    "    #format the grid subset into a dataframs\n",
    "    df_grid=gpd.GeoDataFrame(results)\n",
    "    df_grid.columns = ['x_loc', 'y_loc','row','col','geometry']\n",
    "    df_grid.set_geometry(col='geometry',inplace=True,crs='EPSG:5070') #need to say it's in lat/lon before transform to LCC\n",
    "    df_grid=df_grid.to_crs(epsg=3347)\n",
    "\n",
    "    \n",
    "    #intersect the polygon with the grid subset\n",
    "    njobs = 14\n",
    "    stride = math.ceil(len(df_grid)/njobs) #round up because we drop duplicates\n",
    "    tic = datetime.datetime.now()\n",
    "    intersection = Parallel(n_jobs=njobs)(delayed(gpd.overlay)\n",
    "                                    (df_grid.iloc[kk:kk+stride], poly, how='intersection',keep_geom_type=False)\n",
    "                                     for kk in np.arange(0,len(df_grid),stride))\n",
    "    toc = datetime.datetime.now()\n",
    "    print(toc-tic)\n",
    "    \n",
    "    intersection=pd.concat(intersection).drop_duplicates()\n",
    "    intersection['grid intersection area (ha)'] =intersection['geometry'].area/10000\n",
    "    intersection['weights'] = intersection['grid intersection area (ha)']/intersection['fire area (ha)'] \n",
    "    intersection_xr = intersection.set_index(['row', 'col']).to_xarray()    \n",
    "\n",
    "    grid_sub = grid.sel(rows_ctr = np.unique(intersection['row'].values),\n",
    "                        cols_ctr = np.unique(intersection['col'].values))\n",
    "\n",
    "    varis = ['day','loading_new_extreme', 'loading_new_veryhigh','loading_new_high', 'loading_new_moderate', 'loading_new_low']\n",
    "    df_loading = generate_df(varis, len(poly))\n",
    "    \n",
    "    df_loading['day'] = poly['12Z Start Day'].values\n",
    "    for var in varis[1:len(varis)]:\n",
    "        df_loading[var] = np.nansum(intersection_xr['weights'].values*grid_sub[var].values)\n",
    "    \n",
    "    return df_loading\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "830a7c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_one_fccs_grid_cell(x_location, y_location, rows, cols, index, half_cell):\n",
    "    ctrx =x_location[index]\n",
    "    ctry =y_location[index]\n",
    "    \n",
    "    sw = (ctrx-half_cell,ctry-half_cell) #SW\n",
    "    se = (ctrx+half_cell,ctry-half_cell) #SE\n",
    "    nw = (ctrx-half_cell,ctry+half_cell) #NW\n",
    "    ne = (ctrx+half_cell,ctry+half_cell) #NE\n",
    "    cell = Polygon([sw,nw,ne,se])\n",
    "    return ctrx,ctry,rows[index],cols[index],cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "669bd87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes and saves a geodataframe of a grid given the center and corner points for that grid as 2D matrices\n",
    "def build_grid_xarray(LAT_COR, LON_COR, LAT_CTR, LON_CTR):\n",
    "    #loop over the centers\n",
    "    nrows_center = LAT_CTR.shape[0]\n",
    "    ncols_center = LAT_CTR.shape[1]\n",
    "    #print(nrows_center, ncols_center)\n",
    "\n",
    "    nrows_corner = LAT_COR.shape[0]\n",
    "    ncols_corner = LAT_COR.shape[1]\n",
    "    #print(nrows_corner,ncols_corner)\n",
    "    \n",
    "    rows_ctr = np.arange(nrows_center)\n",
    "    cols_ctr = np.arange(ncols_center)\n",
    "    rows_cor = np.arange(nrows_corner)\n",
    "    cols_cor = np.arange(ncols_corner)\n",
    "\n",
    "    \n",
    "    dat_grid = xr.Dataset(\n",
    "        data_vars = dict(\n",
    "            LAT_CTR=(['rows_ctr','cols_ctr'],LAT_CTR),\n",
    "            LON_CTR=(['rows_ctr','cols_ctr'],LON_CTR),\n",
    "            LAT_COR=(['rows_cor','cols_cor'],LAT_COR),\n",
    "            LON_COR=(['rows_cor','cols_cor'],LON_COR),\n",
    "        ),\n",
    "        \n",
    "        coords = dict(\n",
    "            rows_ctr =(['rows_ctr'],rows_ctr),\n",
    "            cols_ctr =(['cols_ctr'],cols_ctr),\n",
    "            rows_cor =(['rows_cor'],rows_cor),\n",
    "            cols_cor =(['cols_cor'],cols_cor),\n",
    "        \n",
    "        )\n",
    "        \n",
    "    )\n",
    "    return(dat_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5fbbda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes and saves a geodataframe of a grid given the center and corner points for that grid as 2D matrices\n",
    "def build_one_gridcell(LAT_COR, LON_COR, LAT_CTR, LON_CTR, loc):\n",
    "    ii=loc[0]\n",
    "    jj=loc[1]\n",
    "\n",
    "    #print(LAT_CTR[ii,jj], LON_CTR[ii,jj]) #ctr\n",
    "    sw = (LON_COR[ii, jj],LAT_COR[ii, jj]) #SW\n",
    "    se =(LON_COR[ii, jj+1],LAT_COR[ii, jj+1]) #SE\n",
    "    nw = (LON_COR[ii+1, jj],LAT_COR[ii+1, jj]) #NW\n",
    "    ne = (LON_COR[ii+1, jj+1],LAT_COR[ii+1, jj+1]) #NE\n",
    "            \n",
    "    poly_cell = Polygon([sw,nw,ne,se])\n",
    "\n",
    "    return LAT_CTR[ii,jj], LON_CTR[ii,jj],ii,jj,poly_cell\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99848efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAT and LON are 2d arrays\n",
    "def calculate_grid_cell_corners(LAT, LON):\n",
    "    #we will assume the very edges of the polygons don't touch the boundary of the domain\n",
    "    lat_corners = (LAT[0:(LAT.shape[0]-1),  0:(LAT.shape[1])-1] + LAT[1:(LAT.shape[0]), 1:(LAT.shape[1])])/2\n",
    "    lon_corners = (LON[0:(LAT.shape[0]-1),  0:(LAT.shape[1])-1] + LON[1:(LAT.shape[0]), 1:(LAT.shape[1])])/2\n",
    "    return lat_corners, lon_corners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0a0ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_df(variables, length):\n",
    "    df = pd.DataFrame()\n",
    "    for vv in variables:\n",
    "        df[vv] = np.zeros(length)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae96629c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c139b7f7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    " #select the region of the fuels tiff file that's associated with the polygon\n",
    "    \n",
    "\n",
    "    print(w_flat.shape)\n",
    "    #build the grid    \n",
    "    rows = np.arange(win.row_off,win.row_off+win.height-1)\n",
    "    cols = np.arange(win.col_off,win.col_off+win.width-1)\n",
    "    COLS,ROWS=np.meshgrid(cols,rows)\n",
    "    xs, ys = rasterio.transform.xy(src.transform, ROWS, COLS)\n",
    "    xs=np.array(xs).flatten()\n",
    "    ys=np.array(ys).flatten()\n",
    "    print(xs.shape)\n",
    "    \n",
    "\n",
    "    points = [Point(xs[jj],ys[jj]) for jj in range(len(xs))]\n",
    "    print(len(points))\n",
    "    \n",
    "    #xs_corner, ys_corner = calculate_grid_cell_corners(xs,ys)\n",
    "    \n",
    "    #grid = build_grid_xarray(ys_corner, xs_corner, ys, xs)\n",
    "    \"\"\"\n",
    "    #add the loadings to the grid\n",
    "    loading_new_extreme = loadings.loc[w_flat,'Extreme_N'].values.reshape(w_from_latlon.shape)\n",
    "    loading_new_veryhigh = loadings.loc[w_flat,'VeryHigh_N'].values.reshape(w_from_latlon.shape)\n",
    "    loading_new_high = loadings.loc[w_flat,'High_N'].values.reshape(w_from_latlon.shape)\n",
    "    loading_new_moderate = loadings.loc[w_flat,'Moderate_N'].values.reshape(w_from_latlon.shape)\n",
    "    loading_new_low = loadings.loc[w_flat,'Low_N'].values.reshape(w_from_latlon.shape)\n",
    "    \n",
    "    grid['loading_new_extreme'] = xr.Variable(dims = ('rows_ctr','cols_ctr'),data=loading_new_extreme)\n",
    "    grid['loading_new_veryhigh'] = xr.Variable(dims = ('rows_ctr','cols_ctr'),data=loading_new_veryhigh)\n",
    "    grid['loading_new_high'] = xr.Variable(dims = ('rows_ctr','cols_ctr'),data=loading_new_high)\n",
    "    grid['loading_new_moderate'] = xr.Variable(dims = ('rows_ctr','cols_ctr'),data=loading_new_moderate)\n",
    "    grid['loading_new_low'] = xr.Variable(dims = ('rows_ctr','cols_ctr'),data=loading_new_low)\n",
    "\n",
    "    #print(grid)\n",
    "       \n",
    "    #get the locs, which are the INSIDE of the centers\n",
    "    df_col = pd.DataFrame({'col_loc':grid['cols_ctr'].values[1:len(grid['cols_ctr'])-1]})\n",
    "    df_row = pd.DataFrame({'row_loc':grid['rows_ctr'].values[1:len(grid['rows_ctr'])-1]})\n",
    "    locs = list(product(df_row['row_loc'], df_col['col_loc']))\n",
    "    print(len(locs))\n",
    "    \n",
    "    #make a geodataframe (in paralell of the rows and cols)\n",
    "    tic = datetime.datetime.now()\n",
    "    print(tic)\n",
    "    results = Parallel(n_jobs=8)(delayed(build_one_gridcell)\n",
    "                                 (grid['LAT_COR'].values, grid['LON_COR'].values,\n",
    "                                  grid['LAT_CTR'].values, grid['LON_CTR'].values,loc) \n",
    "                                 for loc in locs)\n",
    "    \n",
    "    #format the grid subset into a dataframs\n",
    "    df_grid=gpd.GeoDataFrame(results)\n",
    "    df_grid.columns = ['lat', 'lon', 'row', 'col', 'geometry']\n",
    "    df_grid.set_geometry(col='geometry',inplace=True,crs='EPSG:5070') #need to say it's in lat/lon before transform to LCC\n",
    "    df_grid=df_grid.to_crs(epsg=3347)\n",
    "\n",
    "    \n",
    "    #intersect the polygon with the grid subset\n",
    "    njobs = 8\n",
    "    stride = math.ceil(len(df_grid)/njobs) #round up because we drop duplicates\n",
    "    intersection = Parallel(n_jobs=njobs)(delayed(gpd.overlay)\n",
    "                                    (df_grid.iloc[kk:kk+stride], poly, how='intersection',keep_geom_type=False)\n",
    "                                     for kk in np.arange(0,len(df_grid),stride))\n",
    "    \n",
    "    #intersection = gpd.overlay(df_grid,poly,how='intersection',keep_geom_type=False).drop_duplicates()\n",
    "    toc = datetime.datetime.now()\n",
    "    print(toc)\n",
    "    print(toc-tic)\n",
    "    \n",
    "    intersection=pd.concat(intersection).drop_duplicates()\n",
    "    intersection['grid intersection area (ha)'] =intersection['geometry'].area/10000\n",
    "    intersection['weights'] = intersection['grid intersection area (ha)']/intersection['fire area (ha)'] \n",
    "    intersection_xr = intersection.set_index(['row', 'col']).to_xarray()\n",
    "    \n",
    "    grid_sub = grid.sel(rows_ctr = np.unique(intersection['row'].values),\n",
    "                        cols_ctr = np.unique(intersection['col'].values))\n",
    "\n",
    "    varis = ['day','loading_new_extreme', 'loading_new_veryhigh','loading_new_high', 'loading_new_moderate', 'loading_new_low']\n",
    "    df_loading = generate_df(varis, len(poly))\n",
    "    \n",
    "    df_loading['day'] = poly['12Z Start Day'].values\n",
    "    for var in varis[1:len(varis)]:\n",
    "        df_loading[var] = np.nansum(intersection_xr['weights'].values*grid_sub[var].values)\n",
    "    \n",
    "    return df_loading\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3618c4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fdada0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "\n",
    "d = {'geometry': [Point(1, 2), Point(2, 1)]}\n",
    "\n",
    "gdf = gpd.GeoDataFrame(d)\n",
    "print(gdf.has_sindex)\n",
    "\n",
    "index = gdf.sindex\n",
    "print(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444c1f04",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#load in the merra grid\n",
    "grid = xr.open_dataset('HRRR_GRID.nc')\n",
    "\n",
    "#get the bounds of the buffered polygons\n",
    "poly_latlon =fire_daily.iloc[0:1].to_crs(epsg=4326)\n",
    "print(poly_latlon)\n",
    "bounds = poly_latlon.buffer(1).bounds\n",
    "\n",
    "#first check for rows and cols\n",
    "[rows,cols] = np.where((grid.LAT_CTR.values>bounds['miny'].values)&\n",
    "                (grid.LAT_CTR.values<bounds['maxy'].values)&\n",
    "                (grid.LON_CTR.values>bounds['minx'].values)&\n",
    "                (grid.LON_CTR.values<bounds['maxx'].values))\n",
    "locs = zip(rows,cols)\n",
    "     \n",
    "#make a geodataframe (in paralell of the rows and cols)\n",
    "results = Parallel(n_jobs=8)(delayed(build_one_gridcell)\n",
    "                            (grid['LAT_COR'].values, grid['LON_COR'].values,\n",
    "                            grid['LAT_CTR'].values, grid['LON_CTR'].values,loc) \n",
    "                            for loc in locs)\n",
    "    \n",
    "#format the grid subset into a dataframs\n",
    "df_grid=gpd.GeoDataFrame(results)\n",
    "df_grid.columns = ['lat', 'lon', 'row', 'col', 'geometry']\n",
    "df_grid.set_geometry(col='geometry',inplace=True,crs='EPSG:4326') #need to say it's in lat/lon before transform to LCC\n",
    "#df_grid=df_grid.to_crs(epsg=3347)\n",
    "\n",
    "tic = datetime.datetime.now()\n",
    "index_grid=df_grid.sindex\n",
    "index_poly = poly_latlon.sindex\n",
    "toc=datetime.datetime.now()\n",
    "print(toc-tic)\n",
    "\n",
    "intersection = gpd.overlay(df_grid,poly_latlon,how='intersection',keep_geom_type=False).drop_duplicates()\n",
    "print(intersection)    \n",
    "\n",
    "#make it into boxes\n",
    "#do a contains?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c58ddc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
