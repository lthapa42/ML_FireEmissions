{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aee907ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import path\n",
    "import os\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "np.set_printoptions(threshold=100000)\n",
    "from shapely.geometry import Polygon, Point, MultiPoint\n",
    "from shapely.ops import cascaded_union, unary_union, transform\n",
    "from datetime import datetime, timedelta\n",
    "import datetime\n",
    "import math\n",
    "from scipy.ndimage.interpolation import shift\n",
    "import shapely.wkt\n",
    "from shapely.validation import explain_validity,make_valid\n",
    "import xarray as xr\n",
    "import pygeos as pg\n",
    "import time\n",
    "import seaborn as sns\n",
    "from my_functions import sat_vap_press, vap_press, hot_dry_windy, haines\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "998fcd68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  12Z Start Day  Incident Number Fire Name     UTC Day   Lat Fire    Lon Fire  \\\n",
      "0    2020-08-12       11773470.0      LAKE  2020-08-13  34.678611 -118.451944   \n",
      "1    2020-08-13       11773470.0      LAKE  2020-08-14  34.678611 -118.451944   \n",
      "2    2020-08-14       11773470.0      LAKE  2020-08-15  34.678611 -118.451944   \n",
      "\n",
      "   Number of NEW VIIRS points   NEW FRP  \\\n",
      "0                       132.0    782.77   \n",
      "1                        25.0   1644.50   \n",
      "2                        70.0  11606.51   \n",
      "\n",
      "                                            geometry  fire area (ha)  \n",
      "0  MULTIPOLYGON (((3641986.585 323212.339, 364197...     2056.378485  \n",
      "1  MULTIPOLYGON (((3630083.512 322187.932, 363007...      196.114730  \n",
      "2  MULTIPOLYGON (((3631093.725 323487.926, 363109...      796.663316  \n",
      "0.6925725936889648\n",
      "2020-08-12\n",
      "DatetimeIndex(['2020-08-07 00:00:00', '2020-08-07 01:00:00',\n",
      "               '2020-08-07 02:00:00', '2020-08-07 03:00:00',\n",
      "               '2020-08-07 04:00:00', '2020-08-07 05:00:00',\n",
      "               '2020-08-07 06:00:00', '2020-08-07 07:00:00',\n",
      "               '2020-08-07 08:00:00', '2020-08-07 09:00:00',\n",
      "               ...\n",
      "               '2020-08-12 15:00:00', '2020-08-12 16:00:00',\n",
      "               '2020-08-12 17:00:00', '2020-08-12 18:00:00',\n",
      "               '2020-08-12 19:00:00', '2020-08-12 20:00:00',\n",
      "               '2020-08-12 21:00:00', '2020-08-12 22:00:00',\n",
      "               '2020-08-12 23:00:00', '2020-08-13 00:00:00'],\n",
      "              dtype='datetime64[ns]', length=145, freq='H')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lthapa/anaconda3/envs/ML_py/lib/python3.7/site-packages/numpy/core/_methods.py:151: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "/home/lthapa/anaconda3/envs/ML_py/lib/python3.7/site-packages/numpy/core/_methods.py:151: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "/home/lthapa/anaconda3/envs/ML_py/lib/python3.7/site-packages/numpy/core/_methods.py:151: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[297.10569544]\n",
      "[0.00374961]\n",
      "[4.73482785]\n",
      "[7.46602584]\n",
      "[268.01640709]\n",
      "[0.]\n",
      "[3.39548825]\n",
      "[38.16998767]\n",
      "[0.00703334]\n",
      "[38.16295433]\n",
      "[27.94152084]\n",
      "[4.73482785]\n",
      "[7.46602584]\n",
      "[268.01640709]\n",
      "[0.98780489]\n",
      "[5.12245831]\n",
      "[29.09246185]\n",
      "[0.95384999]\n",
      "[297.10569544]\n",
      "[0.00374961]\n",
      "[4.73482785]\n",
      "[0.98780489]\n",
      "2020-08-13\n",
      "DatetimeIndex(['2020-08-08 00:00:00', '2020-08-08 01:00:00',\n",
      "               '2020-08-08 02:00:00', '2020-08-08 03:00:00',\n",
      "               '2020-08-08 04:00:00', '2020-08-08 05:00:00',\n",
      "               '2020-08-08 06:00:00', '2020-08-08 07:00:00',\n",
      "               '2020-08-08 08:00:00', '2020-08-08 09:00:00',\n",
      "               ...\n",
      "               '2020-08-13 15:00:00', '2020-08-13 16:00:00',\n",
      "               '2020-08-13 17:00:00', '2020-08-13 18:00:00',\n",
      "               '2020-08-13 19:00:00', '2020-08-13 20:00:00',\n",
      "               '2020-08-13 21:00:00', '2020-08-13 22:00:00',\n",
      "               '2020-08-13 23:00:00', '2020-08-14 00:00:00'],\n",
      "              dtype='datetime64[ns]', length=145, freq='H')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lthapa/anaconda3/envs/ML_py/lib/python3.7/site-packages/numpy/core/_methods.py:151: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "/home/lthapa/anaconda3/envs/ML_py/lib/python3.7/site-packages/numpy/core/_methods.py:151: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "/home/lthapa/anaconda3/envs/ML_py/lib/python3.7/site-packages/numpy/core/_methods.py:151: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[256.28388934]\n",
      "[0.00515498]\n",
      "[3.3619282]\n",
      "[6.04931742]\n",
      "[236.16936252]\n",
      "[0.]\n",
      "[4.92803647]\n",
      "[33.84683848]\n",
      "[0.00974692]\n",
      "[33.83709156]\n",
      "[12.08554703]\n",
      "[3.3619282]\n",
      "[6.04931742]\n",
      "[236.16936252]\n",
      "[0.84933891]\n",
      "[3.46210564]\n",
      "[20.11452243]\n",
      "[0.80005856]\n",
      "[256.28388934]\n",
      "[0.00515498]\n",
      "[3.3619282]\n",
      "[0.84933891]\n",
      "2020-08-14\n",
      "DatetimeIndex(['2020-08-09 00:00:00', '2020-08-09 01:00:00',\n",
      "               '2020-08-09 02:00:00', '2020-08-09 03:00:00',\n",
      "               '2020-08-09 04:00:00', '2020-08-09 05:00:00',\n",
      "               '2020-08-09 06:00:00', '2020-08-09 07:00:00',\n",
      "               '2020-08-09 08:00:00', '2020-08-09 09:00:00',\n",
      "               ...\n",
      "               '2020-08-14 15:00:00', '2020-08-14 16:00:00',\n",
      "               '2020-08-14 17:00:00', '2020-08-14 18:00:00',\n",
      "               '2020-08-14 19:00:00', '2020-08-14 20:00:00',\n",
      "               '2020-08-14 21:00:00', '2020-08-14 22:00:00',\n",
      "               '2020-08-14 23:00:00', '2020-08-15 00:00:00'],\n",
      "              dtype='datetime64[ns]', length=145, freq='H')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lthapa/anaconda3/envs/ML_py/lib/python3.7/site-packages/numpy/core/_methods.py:151: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "/home/lthapa/anaconda3/envs/ML_py/lib/python3.7/site-packages/numpy/core/_methods.py:151: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "/home/lthapa/anaconda3/envs/ML_py/lib/python3.7/site-packages/numpy/core/_methods.py:151: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[297.44087913]\n",
      "[0.00672472]\n",
      "[4.8173139]\n",
      "[7.67276933]\n",
      "[274.46825592]\n",
      "[0.]\n",
      "[2.77191869]\n",
      "[41.89677744]\n",
      "[0.01276115]\n",
      "[41.88401628]\n",
      "[23.02115491]\n",
      "[4.8173139]\n",
      "[7.67276933]\n",
      "[274.46825592]\n",
      "[0.98197069]\n",
      "[5.18912904]\n",
      "[22.97264655]\n",
      "[0.95425145]\n",
      "[297.44087913]\n",
      "[0.00672472]\n",
      "[4.8173139]\n",
      "[0.98197069]\n",
      "                   day       hd0w0       hd1w0       hd2w0       hd3w0  \\\n",
      "0  2020-08-12 12:00:00  180.695019  149.177556  140.936607  140.179795   \n",
      "1  2020-08-13 12:00:00  113.757872  102.943724   88.987754   83.541909   \n",
      "2  2020-08-14 12:00:00  201.768454  172.816177  165.752667  147.393035   \n",
      "\n",
      "        hd4w0       hd5w0     temp_2m      q_2m  gust_sfc  ...     soilm  \\\n",
      "0  130.061124  114.790855  297.105695  0.003750  4.734828  ...  3.395488   \n",
      "1   84.032850   78.010898  256.283889  0.005155  3.361928  ...  4.928036   \n",
      "2  143.403234  141.807841  297.440879  0.006725  4.817314  ...  2.771919   \n",
      "\n",
      "     esat_2m      e_2m     vpd_2m        hwp  veg_term  gust_max_term  \\\n",
      "0  38.169988  0.007033  38.162954  27.941521  0.987805       5.122458   \n",
      "1  33.846838  0.009747  33.837092  12.085547  0.849339       3.462106   \n",
      "2  41.896777  0.012761  41.884016  23.021155  0.981971       5.189129   \n",
      "\n",
      "     dd_term  mois_term  snowc_term  \n",
      "0  29.092462   0.953850    0.987805  \n",
      "1  20.114522   0.800059    0.849339  \n",
      "2  22.972647   0.954251    0.981971  \n",
      "\n",
      "[3 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "#all fires\n",
    "#fire_incidents = ['BOBCAT', 'DOLAN', 'HOLIDAY FARM','CREEK', 'LAKE', 'CAMERON PEAK', 'PINE GULCH', 'WILLIAMS FLATS', 'SHADY','PEDRO MOUNTAIN', 'WALKER', '204 COW']\n",
    "\n",
    "#2020 fires\n",
    "#fire_incidents = ['AUGUST COMPLEX','BOBCAT', 'DOLAN', 'HOLIDAY FARM','CREEK', 'LAKE', 'CAMERON PEAK', 'PINE GULCH']\n",
    "fire_incidents = ['CAMERON PEAK']\n",
    "\n",
    "path_poly = '/data2/lthapa/ML_daily/fire_polygons/'\n",
    "start_time=12\n",
    "for jj in range(len(fire_incidents)):\n",
    "    fire_daily = gpd.read_file('./fire_polygons/lake_VIIRS_daily_12Z_day_start.geojson')\n",
    "    fire_daily=fire_daily.drop(columns=['Current Overpass'])\n",
    "    fire_daily = fire_daily.drop(np.where(fire_daily['geometry']==None)[0])\n",
    "    fire_daily['fire area (ha)'] = fire_daily['geometry'].area/10000 #hectares\n",
    "    fire_daily.set_geometry(col='geometry', inplace=True) #designate the geometry column\n",
    "    fire_daily = fire_daily.rename(columns={'Current Day':'UTC Day', 'Local Day': str(start_time)+ 'Z Start Day'})\n",
    "    \n",
    "    fire_daily = fire_daily.iloc[np.array(fire_daily['UTC Day'].values,dtype='datetime64')<=np.datetime64('2020-08-15'),:]\n",
    "\n",
    "    print(fire_daily)\n",
    "    \n",
    "    #merra\n",
    "    #me = merra_timeseries(fire_daily, start_time)\n",
    "    #print(me)\n",
    "    #me.to_csv('./fire_features/'+fire_incidents[jj].lower().replace(' ', '_')+'_Daily_MERRA_Moving_Average_2_'+str(start_time)+'Z.csv') #daily averages\n",
    "\n",
    "    #hrrr\n",
    "    hrrr = hrrr_timeseries(fire_daily,start_time)\n",
    "    print(hrrr)\n",
    "    \n",
    "    #rave\n",
    "    #rave=rave_timeseries(fire_daily,start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d3d36",
   "metadata": {},
   "source": [
    "## Dataset-Dependent Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588ce5a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### MERRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc4ace0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def merra_timeseries(df,day_start_hour):\n",
    "    df_merra = pd.DataFrame({'day': np.zeros(len(df)),'temp':np.zeros(len(df)), 'vpd':np.zeros(len(df)), \n",
    "                             'wind':np.zeros(len(df)),'hd0w0':np.zeros(len(df)), 'hd1w0':np.zeros(len(df)),\n",
    "                             'hd2w0':np.zeros(len(df)),'hd3w0':np.zeros(len(df)), 'hd4w0':np.zeros(len(df)),\n",
    "                             'hd5w0':np.zeros(len(df))}) \n",
    "    \n",
    "    #do the intersection, in parallel\n",
    "    merra_intersections = Parallel(n_jobs=6)(delayed(calculate_intersection)\n",
    "                                 (fire_daily.iloc[ii:ii+1],'MERRA_GRID') \n",
    "                                 for ii in range(len(fire_daily)))\n",
    "    fire_merra_intersection=gpd.GeoDataFrame(pd.concat(merra_intersections, ignore_index=True))\n",
    "    fire_merra_intersection.set_geometry(col='geometry')\n",
    "    \n",
    "    \n",
    "    #loop over all of the days we have intersections\n",
    "    times_intersect = np.unique(fire_merra_intersection[str(day_start_hour)+ 'Z Start Day'].values)\n",
    "    times_utc = np.unique(fire_merra_intersection['UTC Day'].values)\n",
    "    \n",
    "    count = 0\n",
    "    for today in times_intersect:\n",
    "        #get the time\n",
    "        df_sub = fire_merra_intersection.iloc[np.where(fire_merra_intersection[str(day_start_hour)+ 'Z Start Day'].values==today)]\n",
    "        df_sub = df_sub.set_index([str(day_start_hour)+ 'Z Start Day', 'lat', 'lon'])\n",
    "        intersection_sub = df_sub.to_xarray() #polygon and weights for today\n",
    "\n",
    "        times_back = pd.date_range(start=np.datetime64(today)-np.timedelta64(5,'D'), end=np.datetime64(today)+np.timedelta64(1,'D'))\n",
    "        files_back = make_merra_file_namelist(times_back)\n",
    "        \n",
    "        #load in all the merra files associated with this lookback window\n",
    "        dat_merra = xr.open_mfdataset(files_back,concat_dim='time',combine='nested',compat='override', coords='all')\n",
    "    \n",
    "        #add the derived data (svp, vp, vpd)\n",
    "        dat_merra=dat_merra.assign(ESAT=sat_vap_press(dat_merra.TLML))\n",
    "        dat_merra=dat_merra.assign(E=vap_press(dat_merra.QLML, dat_merra.TLML))\n",
    "        dat_merra=dat_merra.assign(VPD=dat_merra.ESAT-dat_merra.E)\n",
    "        \n",
    "        merra_daily_mean = dat_merra.resample(time='24H',base=day_start_hour, label='left').mean(dim='time') #take the daily mean        \n",
    "        merra_daily_mean_region = merra_daily_mean.sel(lat = np.unique(intersection_sub['lat'].values),\n",
    "                                  lon = np.unique(intersection_sub['lon'].values)) #get the location of the overlaps\n",
    "        \n",
    "        hd0 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))\n",
    "        hd1 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(1,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd2 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(2,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd3 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(3,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd4 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(4,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd5 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(5,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        w = np.nansum((merra_daily_mean_region['SPEEDLML'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))\n",
    "        t = np.nansum((merra_daily_mean_region['TLML'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))\n",
    "        \n",
    "        df_merra.iloc[count,:] = [today+ ' '+str(day_start_hour)+':00:00',t,hd0,w,hd0*w,hd1*w,hd2*w,hd3*w,hd4*w,hd5*w]\n",
    "        dat_merra.close()\n",
    "        count =count+1\n",
    "    return df_merra\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd1d028",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_merra_file_namelist(time):\n",
    "    base_filename = '/data2/lthapa/YEAR/MERRA2/WESTUS_MERRA2_400.inst1_2d_lfo_Nx.FULLDATE.nc4'\n",
    "    base_filename_list = np.repeat(base_filename, len(time))\n",
    "\n",
    "    \n",
    "    for jj in range(len(time)):\n",
    "        base_filename_list[jj] = base_filename_list[jj].replace('YEAR',time[jj].strftime('%Y')).\\\n",
    "                                    replace('FULLDATE',time[jj].strftime('%Y%m%d'))\n",
    "        if (time[jj].strftime('%Y%m')=='202009'):\n",
    "            base_filename_list[jj] = base_filename_list[jj].replace('400','401')\n",
    "    return base_filename_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e518f3f0",
   "metadata": {},
   "source": [
    "### HRRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c35357ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hrrr_timeseries(df,day_start_hour):  \n",
    "    df_hrrr_derived = pd.DataFrame({'day': np.zeros(len(df)),'hd0w0':np.zeros(len(df)), 'hd1w0':np.zeros(len(df)),\n",
    "                             'hd2w0':np.zeros(len(df)),'hd3w0':np.zeros(len(df)), 'hd4w0':np.zeros(len(df)),\n",
    "                             'hd5w0':np.zeros(len(df))})\n",
    "    \n",
    "    df_hrrr_raw = pd.DataFrame({'temp_2m':np.zeros(len(df)), 'q_2m':np.zeros(len(df)), \n",
    "                             'gust_sfc':np.zeros(len(df)),'veggie':np.zeros(len(df)), 'dewpt':np.zeros(len(df)),\n",
    "                             'weasd':np.zeros(len(df)),'soilm':np.zeros(len(df)), 'esat_2m':np.zeros(len(df)),\n",
    "                             'e_2m': np.zeros(len(df)),'vpd_2m':np.zeros(len(df)), 'hwp':np.zeros(len(df)), \n",
    "                             'gust_sfc':np.zeros(len(df)),'veggie':np.zeros(len(df)), 'dewpt':np.zeros(len(df)),\n",
    "                             'veg_term':np.zeros(len(df)),'gust_max_term':np.zeros(len(df)), 'dd_term':np.zeros(len(df)),\n",
    "                             'mois_term': np.zeros(len(df)),'temp_2m':np.zeros(len(df)), 'q_2m':np.zeros(len(df)), \n",
    "                             'gust_sfc':np.zeros(len(df)),'snowc_term':np.zeros(len(df))})\n",
    "    \n",
    "    varis = ['temp_2m', 'q_2m','gust_sfc','veggie', 'dewpt','weasd','soilm', 'esat_2m','e_2m',\n",
    "             'vpd_2m', 'hwp','gust_sfc','veggie', 'dewpt','veg_term','gust_max_term', 'dd_term',\n",
    "            'mois_term','temp_2m', 'q_2m','gust_sfc','snowc_term']\n",
    "    \n",
    "    #do the intersection, in parallel\n",
    "    tic=time.time()\n",
    "    hrrr_intersections = Parallel(n_jobs=6)(delayed(calculate_intersection)\n",
    "                                 (fire_daily.iloc[ii:ii+1],'HRRR_GRID') \n",
    "                                 for ii in range(len(fire_daily)))\n",
    "    fire_hrrr_intersection=gpd.GeoDataFrame(pd.concat(hrrr_intersections, ignore_index=True))\n",
    "    fire_hrrr_intersection.set_geometry(col='geometry')\n",
    "    toc = time.time()\n",
    "    print(toc-tic)\n",
    "    \n",
    "    \n",
    "    #loop over all of the days we have intersections\n",
    "    times_intersect = np.unique(fire_hrrr_intersection[str(day_start_hour)+ 'Z Start Day'].values)\n",
    "    times_utc = np.unique(fire_hrrr_intersection['UTC Day'].values)\n",
    "    \n",
    "    count = 0\n",
    "    for today in times_intersect:\n",
    "        print(today)\n",
    "        #get the time\n",
    "        df_sub = fire_hrrr_intersection.iloc[np.where(fire_hrrr_intersection[str(day_start_hour)+ 'Z Start Day'].values==today)]\n",
    "        df_sub = df_sub.set_index([str(day_start_hour)+ 'Z Start Day', 'row', 'col'])\n",
    "        df_sub=df_sub[~df_sub.index.duplicated()]\n",
    "        intersection_sub = df_sub.to_xarray() #polygon and weights for today\n",
    "        \n",
    "        times_back = pd.date_range(start=np.datetime64(today)-np.timedelta64(5,'D'), end=np.datetime64(today)+\n",
    "                                   np.timedelta64(1,'D'),freq='H')\n",
    "        files_back = make_hrrr_file_namelist(times_back)\n",
    "        \n",
    "        #load in all the merra files associated with this lookback window\n",
    "        dat_hrrr = xr.open_mfdataset(files_back,concat_dim='time',combine='nested',compat='override', coords='all')\n",
    "        dat_hrrr = dat_hrrr.assign_coords({'time': times_back})\n",
    "\n",
    "        #add the derived data (svp, vp, vpd)\n",
    "        dat_hrrr=dat_hrrr.assign(esat_2m=sat_vap_press(dat_hrrr.temp_2m))\n",
    "        dat_hrrr=dat_hrrr.assign(e_2m=vap_press(dat_hrrr.q_2m, dat_hrrr.temp_2m))\n",
    "        dat_hrrr=dat_hrrr.assign(VPD=dat_hrrr.esat_2m-dat_hrrr.e_2m)\n",
    "        \n",
    "        hrrr_daily_mean = dat_hrrr.resample(time='24H',base=day_start_hour, label='left').mean(dim='time') #take the daily mean        \n",
    "        \n",
    "        hrrr_daily_mean_region = hrrr_daily_mean.sel(grid_yt = np.unique(intersection_sub['row'].values),\n",
    "                                                    grid_xt = np.unique(intersection_sub['col'].values)) #get the location of the overlaps\n",
    "\n",
    "        hd0 = np.nansum((hrrr_daily_mean_region['vpd_2m'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))\n",
    "        hd1 = np.nansum((hrrr_daily_mean_region['vpd_2m'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(1,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd2 = np.nansum((hrrr_daily_mean_region['vpd_2m'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(2,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd3 = np.nansum((hrrr_daily_mean_region['vpd_2m'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(3,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd4 = np.nansum((hrrr_daily_mean_region['vpd_2m'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(4,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd5 = np.nansum((hrrr_daily_mean_region['vpd_2m'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(5,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        w = np.nansum((hrrr_daily_mean_region['gust_sfc'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))\n",
    "        t = np.nansum((hrrr_daily_mean_region['temp_2m'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))\n",
    "        \n",
    "        df_hrrr_derived.iloc[count,:] = [today+ ' '+str(day_start_hour)+':00:00',hd0*w,hd1*w,hd2*w,hd3*w,hd4*w,hd5*w]\n",
    "        \n",
    "        for var in varis:\n",
    "            df_hrrr_raw[var].iloc[count] = np.nansum(intersection_sub['weights'].values*hrrr_daily_mean_region[var].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values, axis=(1,2))\n",
    "        \n",
    "        df_hrrr = pd.concat([df_hrrr_derived,df_hrrr_raw],axis=1)\n",
    "        dat_hrrr.close()\n",
    "        count =count+1\n",
    "    return df_hrrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0a2b4698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hrrr_file_namelist(time):\n",
    "    base_filename = '/data2/lthapa/ML_daily/pygraf/Processed_HRRR_YYYYMMDDHH.nc'\n",
    "    base_filename_list = np.repeat(base_filename, len(time))\n",
    "\n",
    "    \n",
    "    for jj in range(len(time)):\n",
    "        base_filename_list[jj] = base_filename_list[jj].replace('YYYYMMDDHH',time[jj].strftime('%Y%m%d%H'))\n",
    "    return base_filename_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "388db707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2020-08-01 00:00:00', '2020-08-01 01:00:00',\n",
      "               '2020-08-01 02:00:00', '2020-08-01 03:00:00',\n",
      "               '2020-08-01 04:00:00', '2020-08-01 05:00:00',\n",
      "               '2020-08-01 06:00:00', '2020-08-01 07:00:00',\n",
      "               '2020-08-01 08:00:00', '2020-08-01 09:00:00',\n",
      "               '2020-08-01 10:00:00', '2020-08-01 11:00:00',\n",
      "               '2020-08-01 12:00:00', '2020-08-01 13:00:00',\n",
      "               '2020-08-01 14:00:00', '2020-08-01 15:00:00',\n",
      "               '2020-08-01 16:00:00', '2020-08-01 17:00:00',\n",
      "               '2020-08-01 18:00:00', '2020-08-01 19:00:00',\n",
      "               '2020-08-01 20:00:00', '2020-08-01 21:00:00',\n",
      "               '2020-08-01 22:00:00', '2020-08-01 23:00:00',\n",
      "               '2020-08-02 00:00:00'],\n",
      "              dtype='datetime64[ns]', freq='H')\n",
      "['/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080100.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080101.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080102.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080103.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080104.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080105.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080106.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080107.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080108.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080109.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080110.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080111.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080112.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080113.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080114.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080115.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080116.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080117.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080118.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080119.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080120.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080121.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080122.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080123.nc4'\n",
      " '/data2/lthapa/ML_py/pygraf/Processed_HRRR_2020080200.nc4']\n"
     ]
    }
   ],
   "source": [
    "times_back = pd.date_range(start=np.datetime64('2020-08-01'), end=np.datetime64('2020-08-02'),freq='H')\n",
    "print(times_back)\n",
    "\n",
    "listi = make_hrrr_file_namelist(times_back)\n",
    "print(listi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a15bc8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### RAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd0e9cc0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rave_timeseries(df, day_start_hour):\n",
    "    varis = ['Mean_FRP', 'FRE', 'CO2', 'CO', 'SO2', 'OC', 'BC', 'PM2.5', 'NOx', 'NH3'] #don't need 'area', it's the area of each cell\n",
    "\n",
    "\n",
    "    #do the intersection, in parallel\n",
    "    rave_intersections = Parallel(n_jobs=6)(delayed(calculate_intersection)\n",
    "                                 (fire_daily.iloc[ii:ii+1],'RAVE_GRID') \n",
    "                                 for ii in range(len(fire_daily)))\n",
    "    fire_rave_intersection=gpd.GeoDataFrame(pd.concat(rave_intersections, ignore_index=True))\n",
    "    fire_rave_intersection.set_geometry(col='geometry')\n",
    "    \n",
    "    \n",
    "    fire_rave_intersection = fire_rave_intersection.set_index([str(day_start_hour)+ 'Z Start Day', 'row', 'col'])\n",
    "    print(fire_rave_intersection)\n",
    "    fire_rave_intersection = fire_rave_intersection.to_xarray()\n",
    "    \"\"\"\n",
    "    #load in rave data associated with the fire\n",
    "    times = pd.date_range(np.datetime64(df[str(day_start_hour)+ 'Z Start Day'].iloc[0]),\n",
    "                        np.datetime64(df[str(day_start_hour)+ 'Z Start Day'].iloc[len(df)-1])+\n",
    "                        np.timedelta64(1,'D'))\n",
    "    rave_filenames = make_rave_file_namelist(times)\n",
    "    dat_rave = xr.open_mfdataset(rave_filenames,concat_dim='Time',combine='nested',compat='override', coords='all')\n",
    "    dat_rave = dat_rave.assign_coords({'Time': dat_rave.time}) #assign coords so we can resample along time\n",
    "    dat_rave = dat_rave.resample(Time='24H',base=day_start_hour).sum(dim='Time') #take the daily sum\n",
    "\n",
    "    \n",
    "    #select the locations and times we want\n",
    "    dat_rave_sub = dat_rave.isel(yFRP = fire_rave_intersection_xr['row'].values.astype(int), \n",
    "                    xFRP = fire_rave_intersection_xr['col'].values.astype(int)).sel(\n",
    "                    Time = pd.to_datetime(fire_rave_intersection_xr[str(day_start_hour)+ 'Z Start Day'].values+\n",
    "                                         'T'+str(day_start_hour)+':00:00'))\n",
    "    ndays = len(fire_rave_intersection_xr[str(day_start_hour)+ 'Z Start Day'])\n",
    "    \n",
    "    #preallocate space for the output\n",
    "    df_rave = pd.DataFrame({'day':np.zeros(ndays),'Mean_FRP':np.zeros(ndays),\\\n",
    "                          'FRE':np.zeros(ndays),  'CO2':np.zeros(ndays),  'CO':np.zeros(ndays),\\\n",
    "                          'SO2':np.zeros(ndays),  'OC':np.zeros(ndays),  'BC':np.zeros(ndays),\\\n",
    "                          'PM2.5':np.zeros(ndays),  'NOx':np.zeros(ndays),  'NH3':np.zeros(ndays)})\n",
    "\n",
    "    df_rave['day'].iloc[:] = pd.to_datetime(fire_rave_intersection_xr[str(day_start_hour)+ 'Z Start Day'].values+\n",
    "                                         'T'+str(day_start_hour)+':00:00')\n",
    "    for var in varis:\n",
    "        df_rave[var] = np.nansum(fire_rave_intersection_xr['weights'].values*dat_rave_sub[var].values, axis=(1,2))\n",
    "    \n",
    "    return df_rave\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c0febe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7036f59c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dataset-Independent Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ccf0332",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#makes and saves a geodataframe of a grid given the center and corner points for that grid as 2D matrices\n",
    "def build_one_gridcell(LAT_COR, LON_COR, LAT_CTR, LON_CTR, ii,jj):\n",
    "    #print(ii,jj,count)\n",
    "    #print(LAT_CTR[ii,jj], LON_CTR[ii,jj]) #ctr\n",
    "    sw = (LON_COR[ii, jj],LAT_COR[ii, jj]) #SW\n",
    "    se =(LON_COR[ii, jj+1],LAT_COR[ii, jj+1]) #SE\n",
    "    nw = (LON_COR[ii+1, jj],LAT_COR[ii+1, jj]) #NW\n",
    "    ne = (LON_COR[ii+1, jj+1],LAT_COR[ii+1, jj+1]) #NE\n",
    "            \n",
    "    poly_cell = Polygon([sw,nw,ne,se])\n",
    "    \n",
    "    return LAT_CTR[ii,jj], LON_CTR[ii,jj],ii,jj,poly_cell\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa773ce0",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#poly is the polygon for one timestep (in lcc)\n",
    "#grid is an xarray of a model grid from the nc file\n",
    "#grid_names is a string array [0:'lat_center_name',1:'lon_center_name',2:'lat_corner_name',3:'lon_corner_name']\n",
    "\n",
    "def calculate_intersection(poly,dataset_name):\n",
    "    #load in the merra grid\n",
    "    grid = xr.open_dataset(dataset_name+'.nc')\n",
    "\n",
    "    #for each fire_daily polygon\n",
    "\n",
    "    #get the bounds\n",
    "    poly_latlon =poly.to_crs(epsg=4326)\n",
    "    bounds = poly_latlon.bounds\n",
    "    print(bounds)\n",
    "    \n",
    "    #first check for rows and cols\n",
    "    [rows,cols] = np.where((grid.LAT_CTR.values>bounds['miny'].values)&\n",
    "                    (grid.LAT_CTR.values<bounds['maxy'].values)&\n",
    "                    (grid.LON_CTR.values>bounds['minx'].values)&\n",
    "                    (grid.LON_CTR.values<bounds['maxx'].values))\n",
    "\n",
    "    if rows.size==0: #if the whole polygon is inside one grid cell, find that grid cell\n",
    "        print('empty!')\n",
    "        lat_middle = (bounds['miny'].values+bounds['maxy'].values)/2\n",
    "        lon_middle = (bounds['minx'].values+bounds['maxx'].values)/2\n",
    "\n",
    "        distance = np.sqrt((grid.LAT_CTR-lat_middle)**2+(grid.LON_CTR-lon_middle)**2)\n",
    "        row_min_location,col_min_location = np.where(distance ==np.min(distance))\n",
    "        rows = np.append(rows,row_min_location)\n",
    "        cols = np.append(cols,col_min_location)\n",
    "    \n",
    "    print(rows,cols)\n",
    "    \n",
    "    \n",
    "    #make a geodataframe (in paralell of the rows and cols)\n",
    "    tic = time.time()\n",
    "    results = Parallel(n_jobs=6)(delayed(build_one_gridcell)\n",
    "                                 (grid['LAT_COR'].values, grid['LON_COR'].values,\n",
    "                                  grid['LAT_CTR'].values, grid['LON_CTR'].values,i,j) \n",
    "                                 for i in rows for j in cols)\n",
    "    toc = time.time()\n",
    "    print(toc-tic)\n",
    "    df_grid=gpd.GeoDataFrame(results)\n",
    "    df_grid.columns = ['lat', 'lon', 'row', 'col', 'geometry']\n",
    "    df_grid.set_geometry(col='geometry',inplace=True,crs='EPSG:4326') #need to say it's in lat/lon before transform to LCC\n",
    "    df_grid=df_grid.to_crs(epsg=3347)\n",
    "    #print(df_grid)\n",
    "    \n",
    "    #intersect the polygon with the grid subset\n",
    "    intersection = gpd.overlay(poly, df_grid, how='intersection',keep_geom_type=False)\n",
    "    intersection['grid intersection area (ha)'] =intersection['geometry'].area/10000\n",
    "    intersection['weights'] = intersection['grid intersection area (ha)']/intersection['fire area (ha)'] \n",
    "    \n",
    "    return intersection\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e258e763",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935b694",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb2a8da",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e8487",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36bd8eec",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ecc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "merra_intersections = Parallel(n_jobs=6)(delayed(calculate_intersection)\n",
    "                                 (fire_daily.iloc[ii:ii+1],'MERRA_GRID') \n",
    "                                 for ii in range(len(fire_daily)))\n",
    "toc=time.time()\n",
    "\n",
    "merra_intersections_gdf=gpd.GeoDataFrame(pd.concat(merra_intersections, ignore_index=True))\n",
    "merra_intersections_gdf.set_geometry(col='geometry')\n",
    "print(merra_intersections_gdf)\n",
    "print(toc-tic)\n",
    "tic = time.time()\n",
    "hrrr_intersections = Parallel(n_jobs=6)(delayed(calculate_intersection)\n",
    "                                 (fire_daily.iloc[ii:ii+1],'HRRR_GRID') \n",
    "                                 for ii in range(len(fire_daily)))\n",
    "toc=time.time()\n",
    "\n",
    "hrrr_intersections_gdf=gpd.GeoDataFrame(pd.concat(hrrr_intersections, ignore_index=True))\n",
    "hrrr_intersections_gdf.set_geometry(col='geometry')\n",
    "print(hrrr_intersections_gdf)\n",
    "print(toc-tic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
