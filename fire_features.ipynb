{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a853cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import path\n",
    "import os\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "np.set_printoptions(threshold=100000)\n",
    "from shapely.geometry import Polygon, Point, MultiPoint\n",
    "from shapely.ops import cascaded_union, unary_union, transform\n",
    "from datetime import datetime, timedelta\n",
    "import datetime\n",
    "import math\n",
    "from scipy.ndimage.interpolation import shift\n",
    "import shapely.wkt\n",
    "from shapely.validation import explain_validity,make_valid\n",
    "import xarray as xr\n",
    "import pygeos as pg\n",
    "import time\n",
    "import seaborn as sns\n",
    "from my_functions import sat_vap_press, vap_press, hot_dry_windy, haines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fab38c",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#all fires\n",
    "#fire_incidents = ['BOBCAT', 'DOLAN', 'HOLIDAY FARM','CREEK', 'LAKE', 'CAMERON PEAK', 'PINE GULCH', 'WILLIAMS FLATS', 'SHADY','PEDRO MOUNTAIN', 'WALKER', '204 COW']\n",
    "\n",
    "#2020 fires\n",
    "#fire_incidents = ['AUGUST COMPLEX','BOBCAT', 'DOLAN', 'HOLIDAY FARM','CREEK', 'LAKE', 'CAMERON PEAK', 'PINE GULCH']\n",
    "fire_incidents = ['LAKE']\n",
    "#fire_incidents=['CAMERON PEAK']\n",
    "path_poly = '/data2/lthapa/ML_daily/fire_polygons/'\n",
    "\n",
    "start_time=12\n",
    "for jj in range(len(fire_incidents)):\n",
    "    print(fire_incidents[jj])\n",
    "    fire_daily = gpd.read_file(path_poly+fire_incidents[jj].lower().replace(' ', '_')+'_VIIRS_daily_'+str(start_time)+'Z_day_start.geojson') #polygons and attributes\n",
    "    \n",
    "    \n",
    "    #get rid of rows/cols we don't need\n",
    "    fire_daily=fire_daily.drop(columns=['Current Overpass'])\n",
    "    fire_daily = fire_daily.drop(np.where(fire_daily['geometry']==None)[0])\n",
    "    fire_daily['fire area (ha)'] = fire_daily['geometry'].area/10000 #hectares\n",
    "    fire_daily.set_geometry(col='geometry', inplace=True) #designate the geometry column\n",
    "    fire_daily = fire_daily.rename(columns={'Current Day':'UTC Day', 'Local Day': str(start_time)+ 'Z Start Day'})\n",
    "    \n",
    "       \n",
    "    fire_daily = fire_daily.iloc[np.array(fire_daily['UTC Day'].values,dtype='datetime64')<=np.datetime64('2020-08-19'),:]\n",
    "    #print(fire_daily)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #merra\n",
    "    #me = merra_timeseries(fire_daily, start_time)\n",
    "    #print(me)\n",
    "    #me.to_csv('./fire_features/'+fire_incidents[jj].lower().replace(' ', '_')+'_Daily_MERRA_Moving_Average_2_'+str(start_time)+'Z.csv') #daily averages\n",
    "\n",
    "    #rave\n",
    "    #rave = rave_timeseries(fire_daily, times,start_time)\n",
    "    #print(rave)\n",
    "    #rave.to_csv('./fire_features/'+fire_incidents[jj].lower().replace(' ', '_')+'_Daily_RAVE_'+str(start_time)+'Z.csv')\n",
    "    \n",
    "    #esi\n",
    "    #esi = esi_timeseries(fire_daily, times)\n",
    "    #print(esi)\n",
    "    #esi.to_csv('./fire_features/'+fire_incidents[jj].lower().replace(' ', '_')+'_Daily_ESI.csv')\n",
    "    \n",
    "    #pws\n",
    "    #pws = pws_timeseries(fire_daily, times)\n",
    "    #print(pws)\n",
    "    #pws.to_csv('./fire_features/'+fire_incidents[jj].lower().replace(' ', '_')+'_Daily_PWS.csv')\n",
    "    \n",
    "    #imerg\n",
    "    #imerg = imerg_fwi_timeseries(fire_daily, times)\n",
    "    #print(imerg)\n",
    "    #imerg.to_csv('./fire_features/'+fire_incidents[jj].lower().replace(' ', '_')+'_Daily_FWI_IMERG.csv')\n",
    "    \n",
    "    #cpc\n",
    "    #cpc = cpc_fwi_timeseries(fire_daily,times)\n",
    "    #print(cpc)\n",
    "    #cpc.to_csv('./fire_features/'+fire_incidents[jj].lower().replace(' ', '_')+'_Daily_FWI_CPC.csv')\n",
    "    \n",
    "    #hrrr\n",
    "    #hrrr = hrrr_timeseries(fire_daily,start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07791560",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_end = np.minimum(np.datetime64(fire_daily['UTC Day'].iloc[len(fire_daily)-1])+np.timedelta64(1,'D'), \n",
    "                      np.datetime64('2020-10-31'))\n",
    "    times = pd.date_range(np.datetime64(fire_daily['UTC Day'].iloc[0]),\\\n",
    "                         times_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_daily = gpd.read_file('./fire_polygons/august_complex_VIIRS_daily_12Z_day_start.geojson') #polygons and attributes\n",
    "tic=time.time()\n",
    "fire_daily = fire_daily.to_crs(epsg=4326)\n",
    "toc=time.time()\n",
    "print(fire_daily)\n",
    "print(toc-tic)\n",
    "\n",
    "#tic=time.time()\n",
    "#fire_daily_total = fire_daily.dissolve(by='Fire Name')\n",
    "#toc=time.time()\n",
    "#print(fire_daily_total)\n",
    "#print(toc-tic)\n",
    "\n",
    "#print(fire_daily['geometry'].iloc[0])\n",
    "#hrrr_grid = gpd.read_file('HRRR_GRID.geojson',rows=10)\n",
    "#print(hrrr_grid)\n",
    "#tic = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce8c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#:\n",
    "tic= time.time()\n",
    "for ii in range(1):#len(fire_daily)):\n",
    "    hrrr_grid = gpd.read_file('HRRR_GRID.geojson',mask=fire_daily['geometry'].iloc[ii])\n",
    "    \n",
    "toc=time.time()\n",
    "#print(hrrr_grid)\n",
    "print(toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4941a4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-118.57547009085407, 34.63046041345746, -118.45018028129323, 34.71835637672822)\n",
      "          lat         lon  row  col  \\\n",
      "0   34.619087 -118.490150  456  261   \n",
      "1   34.625168 -118.458282  456  262   \n",
      "2   34.633114 -118.561287  457  259   \n",
      "3   34.639214 -118.529419  457  260   \n",
      "4   34.645306 -118.497551  457  261   \n",
      "5   34.651390 -118.465675  457  262   \n",
      "6   34.657463 -118.433792  457  263   \n",
      "7   34.659328 -118.568710  458  259   \n",
      "8   34.665432 -118.536835  458  260   \n",
      "9   34.671528 -118.504951  458  261   \n",
      "10  34.677612 -118.473068  458  262   \n",
      "11  34.683689 -118.441177  458  263   \n",
      "12  34.685547 -118.576134  459  259   \n",
      "13  34.691654 -118.544250  459  260   \n",
      "14  34.697750 -118.512360  459  261   \n",
      "15  34.703838 -118.480461  459  262   \n",
      "16  34.709915 -118.448563  459  263   \n",
      "17  34.711765 -118.583572  460  259   \n",
      "18  34.717873 -118.551674  460  260   \n",
      "19  34.723972 -118.519775  460  261   \n",
      "20  34.730061 -118.487869  460  262   \n",
      "\n",
      "                                             geometry  \n",
      "0   POLYGON ((-118.50238 34.60293, -118.50978 34.6...  \n",
      "1   POLYGON ((-118.47052 34.60902, -118.47791 34.6...  \n",
      "2   POLYGON ((-118.57350 34.61695, -118.58092 34.6...  \n",
      "3   POLYGON ((-118.54164 34.62305, -118.54906 34.6...  \n",
      "4   POLYGON ((-118.50978 34.62915, -118.51718 34.6...  \n",
      "5   POLYGON ((-118.47791 34.63524, -118.48531 34.6...  \n",
      "6   POLYGON ((-118.44604 34.64132, -118.45343 34.6...  \n",
      "7   POLYGON ((-118.58092 34.64317, -118.58836 34.6...  \n",
      "8   POLYGON ((-118.54906 34.64927, -118.55648 34.6...  \n",
      "9   POLYGON ((-118.51718 34.65537, -118.52460 34.6...  \n",
      "10  POLYGON ((-118.48531 34.66146, -118.49271 34.6...  \n",
      "11  POLYGON ((-118.45343 34.66754, -118.46082 34.6...  \n",
      "12  POLYGON ((-118.58836 34.66938, -118.59579 34.6...  \n",
      "13  POLYGON ((-118.55648 34.67549, -118.56390 34.7...  \n",
      "14  POLYGON ((-118.52460 34.68159, -118.53201 34.7...  \n",
      "15  POLYGON ((-118.49271 34.68768, -118.50011 34.7...  \n",
      "16  POLYGON ((-118.46082 34.69376, -118.46821 34.7...  \n",
      "17  POLYGON ((-118.59579 34.69560, -118.60324 34.7...  \n",
      "18  POLYGON ((-118.56390 34.70171, -118.57133 34.7...  \n",
      "19  POLYGON ((-118.53201 34.70781, -118.53943 34.7...  \n",
      "20  POLYGON ((-118.50011 34.71391, -118.50752 34.7...  \n",
      "48.63452434539795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lthapa/anaconda3/envs/ML_py/lib/python3.7/site-packages/geopandas/geodataframe.py:577: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  for feature in features_lst:\n"
     ]
    }
   ],
   "source": [
    "fire_daily = gpd.read_file('./fire_polygons/lake_VIIRS_daily_12Z_day_start.geojson') #polygons and attributes\n",
    "fire_daily = fire_daily.to_crs(epsg=4326)\n",
    "print(fire_daily['geometry'].iloc[0].bounds)\n",
    "\n",
    "tic= time.time()\n",
    "hrrr_grid = gpd.read_file('HRRR_GRID.geojson',bbox=fire_daily['geometry'].iloc[0].bounds)\n",
    "toc=time.time()\n",
    "print(hrrr_grid)\n",
    "print(toc-tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6495f8b1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## MERRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f939e3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def merra_timeseries(df,day_start_hour):\n",
    "    df_merra = pd.DataFrame({'day': np.zeros(len(df)),'temp':np.zeros(len(df)), 'vpd':np.zeros(len(df)), \n",
    "                             'wind':np.zeros(len(df)),'hd0w0':np.zeros(len(df)), 'hd1w0':np.zeros(len(df)),\n",
    "                             'hd2w0':np.zeros(len(df)),'hd3w0':np.zeros(len(df)), 'hd4w0':np.zeros(len(df)),\n",
    "                             'hd5w0':np.zeros(len(df))})\n",
    "    #load in the grid\n",
    "    merra_grid = gpd.read_file('MERRA_GRID.geojson')\n",
    "    merra_grid = merra_grid.to_crs(epsg=3347) #put into lambert conformal conic \n",
    "    \n",
    "    #do the intersection, not with a for loop!\n",
    "    fire_merra_intersection = gpd.overlay(df, merra_grid, how='intersection',keep_geom_type=False)\n",
    "    fire_merra_intersection['grid intersection area (ha)'] =fire_merra_intersection['geometry'].area/10000\n",
    "    fire_merra_intersection['weights'] = fire_merra_intersection['grid intersection area (ha)']/fire_merra_intersection['fire area (ha)'] \n",
    "    \n",
    "    #loop over all of the days we have intersections\n",
    "    times_intersect = np.unique(fire_merra_intersection[str(day_start_hour)+ 'Z Start Day'].values)\n",
    "    times_utc = np.unique(fire_merra_intersection['UTC Day'].values)\n",
    "    \n",
    "    count = 0\n",
    "    for today in times_intersect:\n",
    "        #get the time\n",
    "        df_sub = fire_merra_intersection.iloc[np.where(fire_merra_intersection[str(day_start_hour)+ 'Z Start Day'].values==today)]\n",
    "        df_sub = df_sub.set_index([str(day_start_hour)+ 'Z Start Day', 'lat', 'lon'])\n",
    "        intersection_sub = df_sub.to_xarray() #polygon and weights for today\n",
    "\n",
    "        times_back = pd.date_range(start=np.datetime64(today)-np.timedelta64(5,'D'), end=np.datetime64(today)+np.timedelta64(1,'D'))\n",
    "        files_back = make_merra_file_namelist(times_back)\n",
    "        \n",
    "        #load in all the merra files associated with this lookback window\n",
    "        dat_merra = xr.open_mfdataset(files_back,concat_dim='time',combine='nested',compat='override', coords='all')\n",
    "    \n",
    "        #add the derived data (svp, vp, vpd)\n",
    "        dat_merra=dat_merra.assign(ESAT=sat_vap_press(dat_merra.TLML))\n",
    "        dat_merra=dat_merra.assign(E=vap_press(dat_merra.QLML, dat_merra.TLML))\n",
    "        dat_merra=dat_merra.assign(VPD=dat_merra.ESAT-dat_merra.E)\n",
    "        \n",
    "        merra_daily_mean = dat_merra.resample(time='24H',base=day_start_hour, label='left').mean(dim='time') #take the daily mean        \n",
    "        merra_daily_mean_region = merra_daily_mean.sel(lat = np.unique(intersection_sub['lat'].values),\n",
    "                                  lon = np.unique(intersection_sub['lon'].values)) #get the location of the overlaps\n",
    "        \n",
    "        hd0 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))\n",
    "        hd1 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(1,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd2 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(2,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd3 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(3,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd4 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(4,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd5 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(5,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        w = np.nansum((merra_daily_mean_region['SPEEDLML'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))\n",
    "        t = np.nansum((merra_daily_mean_region['TLML'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))\n",
    "        \n",
    "        df_merra.iloc[count,:] = [today+ ' '+str(day_start_hour)+':00:00',t,hd0,w,hd0*w,hd1*w,hd2*w,hd3*w,hd4*w,hd5*w]\n",
    "        dat_merra.close()\n",
    "        count =count+1\n",
    "    return df_merra\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47cb31",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_merra_file_namelist(time):\n",
    "    base_filename = '/data2/lthapa/YEAR/MERRA2/WESTUS_MERRA2_400.inst1_2d_lfo_Nx.FULLDATE.nc4'\n",
    "    base_filename_list = np.repeat(base_filename, len(time))\n",
    "\n",
    "    \n",
    "    for jj in range(len(time)):\n",
    "        base_filename_list[jj] = base_filename_list[jj].replace('YEAR',time[jj].strftime('%Y')).\\\n",
    "                                    replace('FULLDATE',time[jj].strftime('%Y%m%d'))\n",
    "        if (time[jj].strftime('%Y%m')=='202009'):\n",
    "            base_filename_list[jj] = base_filename_list[jj].replace('400','401')\n",
    "    return base_filename_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a010eb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## RAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a8c023",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#only need to do this once\n",
    "times_rave_coverage = pd.date_range(np.datetime64('2020-04-07'), np.datetime64('2020-10-31'))\n",
    "print(times_rave_coverage)\n",
    "base_filename_nc = '/data2/lthapa/YEAR/AprYEAR_to_OctYEAR/Hourly_Emissions_FV3_13km_FULLDATE0000_FULLDATE2300.nc'\n",
    "base_filename_xr = '/data2/lthapa/YEAR/AprYEAR_to_OctYEAR/Hourly_Emissions_FV3_13km_FULLDATE0000_FULLDATE2300xr.nc'\n",
    "for jj in range(len(times_rave_coverage)):\n",
    "    filename_nc = base_filename_nc.replace('YEAR',times_rave_coverage[jj].strftime('%Y')).\\\n",
    "                                    replace('FULLDATE',times_rave_coverage[jj].strftime('%Y%m%d'))\n",
    "    filename_xr = base_filename_xr.replace('YEAR',times_rave_coverage[jj].strftime('%Y')).\\\n",
    "                                    replace('FULLDATE',times_rave_coverage[jj].strftime('%Y%m%d'))\n",
    "    #print(filename_nc, filename_xr)\n",
    "    \n",
    "    cmd = 'ncrename -v Latitude,lat -v Longitude,lon ' + filename_nc + ' '+ filename_xr\n",
    "    print(cmd)\n",
    "    os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2cb9ec",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rave_timeseries(df,time, day_start_hour):\n",
    "    varis = ['Mean_FRP', 'FRE', 'CO2', 'CO', 'SO2', 'OC', 'BC', 'PM2.5', 'NOx', 'NH3'] #don't need 'area', it's the area of each cell\n",
    "    \n",
    "    \n",
    "    #load in the grid\n",
    "    rave_grid = gpd.read_file('RAVE_GRID.geojson')\n",
    "    rave_grid = rave_grid.to_crs(epsg=3347)\n",
    "\n",
    "    #do the intersection, not with a for loop!\n",
    "    fire_rave_intersection = gpd.overlay(fire_daily, rave_grid, how='intersection',keep_geom_type=False)\n",
    "    fire_rave_intersection['grid intersection area (ha)'] =fire_rave_intersection['geometry'].area/10000\n",
    "    fire_rave_intersection['weights'] = fire_rave_intersection['grid intersection area (ha)']/fire_rave_intersection['fire area (ha)'] \n",
    "    \n",
    "    \n",
    "    fire_rave_intersection = fire_rave_intersection.set_index([str(day_start_hour)+ 'Z Start Day', 'row', 'col'])\n",
    "    fire_rave_intersection_xr = fire_rave_intersection.to_xarray()\n",
    "    \n",
    "    #load in rave data associated with the fire\n",
    "    time = pd.date_range(np.datetime64(df[str(day_start_hour)+ 'Z Start Day'].iloc[0]),\n",
    "                        np.datetime64(df[str(day_start_hour)+ 'Z Start Day'].iloc[len(df)-1])+\n",
    "                        np.timedelta64(1,'D'))\n",
    "    rave_filenames = make_rave_file_namelist(time)\n",
    "    dat_rave = xr.open_mfdataset(rave_filenames,concat_dim='Time',combine='nested',compat='override', coords='all')\n",
    "    dat_rave = dat_rave.assign_coords({'Time': dat_rave.time}) #assign coords so we can resample along time\n",
    "    dat_rave = dat_rave.resample(Time='24H',base=day_start_hour).sum(dim='Time') #take the daily sum\n",
    "\n",
    "    \n",
    "    #select the locations and times we want\n",
    "    dat_rave_sub = dat_rave.isel(yFRP = fire_rave_intersection_xr['row'].values.astype(int), \n",
    "                    xFRP = fire_rave_intersection_xr['col'].values.astype(int)).sel(\n",
    "                    Time = pd.to_datetime(fire_rave_intersection_xr[str(day_start_hour)+ 'Z Start Day'].values+\n",
    "                                         'T'+str(day_start_hour)+':00:00'))\n",
    "    ndays = len(fire_rave_intersection_xr[str(day_start_hour)+ 'Z Start Day'])\n",
    "    \n",
    "    #preallocate space for the output\n",
    "    df_rave = pd.DataFrame({'day':np.zeros(ndays),'Mean_FRP':np.zeros(ndays),\\\n",
    "                          'FRE':np.zeros(ndays),  'CO2':np.zeros(ndays),  'CO':np.zeros(ndays),\\\n",
    "                          'SO2':np.zeros(ndays),  'OC':np.zeros(ndays),  'BC':np.zeros(ndays),\\\n",
    "                          'PM2.5':np.zeros(ndays),  'NOx':np.zeros(ndays),  'NH3':np.zeros(ndays)})\n",
    "\n",
    "    df_rave['day'].iloc[:] = pd.to_datetime(fire_rave_intersection_xr[str(day_start_hour)+ 'Z Start Day'].values+\n",
    "                                         'T'+str(day_start_hour)+':00:00')\n",
    "    for var in varis:\n",
    "        df_rave[var] = np.nansum(fire_rave_intersection_xr['weights'].values*dat_rave_sub[var].values, axis=(1,2))\n",
    "    \n",
    "    return df_rave\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4301c3bc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_rave_file_namelist(time):\n",
    "    base_filename = '/data2/lthapa/YEAR/AprYEAR_to_OctYEAR/Hourly_Emissions_FV3_13km_FULLDATE0000_FULLDATE2300xr.nc'\n",
    "    base_filename_list = np.repeat(base_filename, len(time))\n",
    "\n",
    "    \n",
    "    for jj in range(len(time)):\n",
    "        base_filename_list[jj] = base_filename_list[jj].replace('YEAR',time[jj].strftime('%Y')).\\\n",
    "                                    replace('FULLDATE',time[jj].strftime('%Y%m%d'))\n",
    "    return base_filename_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2e7f85",
   "metadata": {},
   "source": [
    "## ESI (Evaporative Stress Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4d31c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def esi_timeseries(df, time):\n",
    "    #load in the grid\n",
    "    esi_grid = gpd.read_file('ESI_GRID.geojson')\n",
    "    esi_grid = esi_grid.to_crs(epsg=3347)\n",
    "\n",
    "    #do the intersection, not with a for loop!\n",
    "    fire_esi_intersection = gpd.overlay(fire_daily, esi_grid, how='intersection',keep_geom_type=False)\n",
    "    fire_esi_intersection['grid intersection area (ha)'] =fire_esi_intersection['geometry'].area/10000\n",
    "    fire_esi_intersection['weights'] = fire_esi_intersection['grid intersection area (ha)']/fire_esi_intersection['fire area (ha)'] \n",
    "    \n",
    "    inds_esi_range = np.where(pd.to_datetime(fire_esi_intersection['Current Day'].values)<=np.datetime64('2020-10-31'))\n",
    "    \n",
    "    fire_esi_intersection = fire_esi_intersection.iloc[inds_esi_range]\n",
    "    fire_esi_intersection = fire_esi_intersection.set_index(['Current Day', 'lat', 'lon'])\n",
    "    fire_esi_intersection_xr = fire_esi_intersection.to_xarray()\n",
    "    \n",
    "    #load in esi data associated with the fire\n",
    "    esi_filenames, esi_times = make_esi_file_namelist(times)\n",
    "    \n",
    "    #open the esi files\n",
    "    dat_esi = xr.open_mfdataset(esi_filenames,concat_dim='time',combine='nested',compat='override', coords='all')\n",
    "    dat_esi = dat_esi.assign_coords({'time': esi_times}) #assign coords so we can resample along time\n",
    "\n",
    "    dat_esi_daily = dat_esi.reindex(time=times,method='nearest')\n",
    "    dat_esi_daily_sub = dat_esi_daily.sel(lat = fire_esi_intersection_xr['lat'].values, \n",
    "                                          lon = fire_esi_intersection_xr['lon'].values,\n",
    "                      time = pd.to_datetime(fire_esi_intersection_xr['Current Day'].values), method='nearest')\n",
    "                                          \n",
    "\n",
    "    ndays = len(fire_esi_intersection_xr['Current Day'])\n",
    "    \n",
    "    #preallocate space for the output\n",
    "    df_esi = pd.DataFrame({'day':np.zeros(ndays),'ESI':np.zeros(ndays)})\n",
    "\n",
    "    df_esi['day'].iloc[:] = pd.to_datetime(fire_esi_intersection_xr['Current Day'].values)\n",
    "    varis=['ESI']\n",
    "    for var in varis:\n",
    "        df_esi[var] = np.nansum(fire_esi_intersection_xr['weights'].values*dat_esi_daily_sub['Band1'].values, axis=(1,2))\n",
    "    \n",
    "    return df_esi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8a2ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_esi_file_namelist(time):\n",
    "    base_filename = '/data2/lthapa/DFPPM_4WK_YEARJDAY.nc'\n",
    "    \n",
    "    dates_weekly = pd.date_range(np.datetime64('2020-01-08'), np.datetime64('2020-12-31'),freq='7D')\n",
    "    fire_start=time[0]\n",
    "    fire_end=time[len(time)-1]\n",
    "    \n",
    "    fire_times_esi = dates_weekly[np.where((dates_weekly>=fire_start)&(dates_weekly<=fire_end))] #weeks in the ESI record where we had fire\n",
    "    \n",
    "    base_filename_list = np.repeat(base_filename, len(fire_times_esi))\n",
    "    \n",
    "    for jj in range(len(fire_times_esi)):\n",
    "        base_filename_list[jj] = base_filename_list[jj].replace('YEAR',fire_times_esi[jj].strftime('%Y')).\\\n",
    "                                    replace('JDAY',fire_times_esi[jj].strftime('%j'))\n",
    "    return base_filename_list, fire_times_esi\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e69a921",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## PWS (Plant Water Sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c77c73a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pws_timeseries(df, time_fire):\n",
    "    #load in the grid\n",
    "    pws_grid = gpd.read_file('PWS_GRID.geojson')\n",
    "    pws_grid = pws_grid.to_crs(epsg=3347)\n",
    "\n",
    "    #do the intersection, not with a for loop!\n",
    "    fire_pws_intersection = gpd.overlay(fire_daily, pws_grid, how='intersection',keep_geom_type=False)\n",
    "    fire_pws_intersection['grid intersection area (ha)'] =fire_pws_intersection['geometry'].area/10000\n",
    "    fire_pws_intersection['weights'] = fire_pws_intersection['grid intersection area (ha)']/fire_pws_intersection['fire area (ha)'] \n",
    "    \n",
    "    inds_pws_range = np.where(pd.to_datetime(fire_pws_intersection['Current Day'].values)<=np.datetime64('2020-10-31'))\n",
    "    \n",
    "    fire_pws_intersection = fire_pws_intersection.iloc[inds_pws_range]\n",
    "    fire_pws_intersection = fire_pws_intersection.set_index(['Current Day', 'lat', 'lon'])\n",
    "    fire_pws_intersection_xr = fire_pws_intersection.to_xarray()\n",
    "    \n",
    "    #load in PWS data associated with the fire (it's only one dataset)\n",
    "    \n",
    "    #open the PWS files\n",
    "    path_pws = '/data2/lthapa/PWS_6_jan_2021.nc'\n",
    "    dat_pws = xr.open_dataset(path_pws) #map is fixed in time\n",
    "    dat_pws = dat_pws.assign_coords({'time': time_fire})\n",
    "    dat_pws_daily = dat_pws['Band1'].expand_dims({'time': time_fire}) #the PWS expanded over all the days\n",
    "    \n",
    "    dat_pws_daily_sub = dat_pws_daily.sel(lat = fire_pws_intersection_xr['lat'].values, \n",
    "                                          lon = fire_pws_intersection_xr['lon'].values,\n",
    "                      time = pd.to_datetime(fire_pws_intersection_xr['Current Day'].values), method='nearest')\n",
    "                                          \n",
    "    ndays = len(fire_pws_intersection_xr['Current Day'])\n",
    "    \n",
    "    #preallocate space for the output\n",
    "    df_pws = pd.DataFrame({'day':np.zeros(ndays),'PWS':np.zeros(ndays)})\n",
    "\n",
    "    df_pws['day'].iloc[:] = pd.to_datetime(fire_pws_intersection_xr['Current Day'].values)\n",
    "    varis=['PWS']\n",
    "    for var in varis:\n",
    "        df_pws[var] = np.nansum(fire_pws_intersection_xr['weights'].values*dat_pws_daily_sub.values, axis=(1,2))\n",
    "    \n",
    "    return df_pws\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39ce9c8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b29a981",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calculate_grid_cell_corners(LAT, LON):\n",
    "    #we will assume the very edges of the polygons don't touch the boundary of the domain\n",
    "    lat_corners = (LAT[0:(LAT.shape[0]-1),  0:(LAT.shape[1])-1] + LAT[1:(LAT.shape[0]), 1:(LAT.shape[1])])/2\n",
    "    lon_corners = (LON[0:(LAT.shape[0]-1),  0:(LAT.shape[1])-1] + LON[1:(LAT.shape[0]), 1:(LAT.shape[1])])/2\n",
    "    return lat_corners, lon_corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db53c1f5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fd9803",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643e252b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "esi_filenames, esi_times = make_esi_file_namelist(times)\n",
    "#open the esi files\n",
    "dat_esi = xr.open_mfdataset(esi_filenames,concat_dim='time',combine='nested',compat='override', coords='all')\n",
    "dat_esi = dat_esi.assign_coords({'time': esi_times}) #assign coords so we can resample along time\n",
    "dat_esi = dat_esi.reindex(time=times,method='nearest')\n",
    "print(dat_esi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c2a303",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lat_lake = np.array([34.625, 34.675, 34.725])\n",
    "print(dat_esi['lat'].values)\n",
    "print(dat_esi.sel(lat=lat_lake, method='nearest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb59bc3f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## IMERG FWI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77dbace",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def imerg_fwi_timeseries(df, time):\n",
    "    varis = ['IMERG.FINAL.v6_DC','IMERG.FINAL.v6_DMC','IMERG.FINAL.v6_FFMC',\n",
    "             'IMERG.FINAL.v6_ISI','IMERG.FINAL.v6_BUI','IMERG.FINAL.v6_FWI',\n",
    "             'IMERG.FINAL.v6_DSR'] \n",
    "\n",
    "    #load in the grid\n",
    "    imerg_grid = gpd.read_file('IMERG_FWI_GRID.geojson')\n",
    "    imerg_grid = imerg_grid.to_crs(epsg=3347)\n",
    "\n",
    "    #do the intersection, not with a for loop!\n",
    "    fire_imerg_intersection = gpd.overlay(fire_daily, imerg_grid, how='intersection',keep_geom_type=False)\n",
    "    fire_imerg_intersection['grid intersection area (ha)'] =fire_imerg_intersection['geometry'].area/10000\n",
    "    fire_imerg_intersection['weights'] = fire_imerg_intersection['grid intersection area (ha)']/fire_imerg_intersection['fire area (ha)'] \n",
    "    \n",
    "    inds_imerg_range = np.where(pd.to_datetime(fire_imerg_intersection['Current Day'].values)<=np.datetime64('2020-10-31'))\n",
    "    \n",
    "    fire_imerg_intersection = fire_imerg_intersection.iloc[inds_imerg_range]\n",
    "    fire_imerg_intersection = fire_imerg_intersection.set_index(['Current Day', 'lat', 'lon'])\n",
    "    fire_imerg_intersection_xr = fire_imerg_intersection.to_xarray()\n",
    "    \n",
    "    #load in rave data associated with the fire\n",
    "    imerg_filenames = make_imerg_file_namelist(times)\n",
    "    dat_imerg = xr.open_mfdataset(imerg_filenames,concat_dim='time',combine='nested',compat='override', coords='all')\n",
    "    dat_imerg = dat_imerg.assign_coords({'time':time}) #assign coords to make it clearer to work with\n",
    "    \n",
    "    print(fire_imerg_intersection_xr['lat'].values)\n",
    "    #select the locations and times we want\n",
    "    dat_imerg_sub = dat_imerg.sel(lat = fire_imerg_intersection_xr['lat'].values, \n",
    "                                lon = fire_imerg_intersection_xr['lon'].values,method='nearest',\n",
    "                                time = pd.to_datetime(fire_imerg_intersection_xr['Current Day'].values))\n",
    "        \n",
    "    \n",
    "    ndays = len(fire_imerg_intersection_xr['Current Day'])\n",
    "    \n",
    "    #preallocate space for the output\n",
    "    df_imerg = pd.DataFrame({'day':np.zeros(ndays),'IMERG.FINAL.v6_DC':np.zeros(ndays),\\\n",
    "                          'IMERG.FINAL.v6_DMC':np.zeros(ndays),  'IMERG.FINAL.v6_FFMC':np.zeros(ndays),\n",
    "                        'IMERG.FINAL.v6_ISI':np.zeros(ndays),'IMERG.FINAL.v6_BUI':np.zeros(ndays),\n",
    "                            'IMERG.FINAL.v6_FWI':np.zeros(ndays),  'IMERG.FINAL.v6_DSR':np.zeros(ndays)})\n",
    "\n",
    "    df_imerg['day'].iloc[:] = pd.to_datetime(fire_imerg_intersection_xr['Current Day'].values)\n",
    "    for var in varis:\n",
    "        df_imerg[var] = np.nansum(fire_imerg_intersection_xr['weights'].values*dat_imerg_sub[var].values, axis=(1,2))\n",
    "    \n",
    "    return df_imerg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5e8bcb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_imerg_file_namelist(time):\n",
    "    base_filename = '/data2/lthapa/YEAR/FWI_IMERG/WESTUS_FWI.IMERG.FINAL.v6.Daily.Default.FULLDATE.nc'\n",
    "    base_filename_list = np.repeat(base_filename, len(time))\n",
    "\n",
    "    \n",
    "    for jj in range(len(time)):\n",
    "        base_filename_list[jj] = base_filename_list[jj].replace('YEAR',time[jj].strftime('%Y')).\\\n",
    "                                    replace('FULLDATE',time[jj].strftime('%Y%m%d'))\n",
    "        \n",
    "    return base_filename_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f547f17b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## CPC FWI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3f610f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cpc_fwi_timeseries(df, time):\n",
    "    varis = ['CPC_DC','CPC_DMC','CPC_FFMC',\n",
    "             'CPC_ISI','CPC_BUI','CPC_FWI',\n",
    "             'CPC_DSR'] \n",
    "\n",
    "    #load in the grid\n",
    "    cpc_grid = gpd.read_file('CPC_FWI_GRID.geojson')\n",
    "    cpc_grid = cpc_grid.to_crs(epsg=3347)\n",
    "\n",
    "    #do the intersection, not with a for loop!\n",
    "    fire_cpc_intersection = gpd.overlay(fire_daily, cpc_grid, how='intersection',keep_geom_type=False)\n",
    "    fire_cpc_intersection['grid intersection area (ha)'] =fire_cpc_intersection['geometry'].area/10000\n",
    "    fire_cpc_intersection['weights'] = fire_cpc_intersection['grid intersection area (ha)']/fire_cpc_intersection['fire area (ha)'] \n",
    "    \n",
    "    inds_cpc_range = np.where(pd.to_datetime(fire_cpc_intersection['Current Day'].values)<=np.datetime64('2020-10-31'))\n",
    "    \n",
    "    fire_cpc_intersection = fire_cpc_intersection.iloc[inds_cpc_range]\n",
    "    fire_cpc_intersection = fire_cpc_intersection.set_index(['Current Day', 'lat', 'lon'])\n",
    "    fire_cpc_intersection_xr = fire_cpc_intersection.to_xarray()\n",
    "    \n",
    "    #load in rave data associated with the fire\n",
    "    cpc_filenames = make_cpc_file_namelist(time)\n",
    "    dat_cpc = xr.open_mfdataset(cpc_filenames,concat_dim='time',combine='nested',compat='override', coords='all')\n",
    "    dat_cpc = dat_cpc.assign_coords({'time':time}) #assign coords to make it clearer to work with\n",
    "    \n",
    "    print(fire_cpc_intersection_xr['lat'].values)\n",
    "    #select the locations and times we want\n",
    "    dat_cpc_sub = dat_cpc.sel(lat = fire_cpc_intersection_xr['lat'].values, \n",
    "                                lon = fire_cpc_intersection_xr['lon'].values,method='nearest',\n",
    "                                time = pd.to_datetime(fire_cpc_intersection_xr['Current Day'].values))\n",
    "        \n",
    "    \n",
    "    ndays = len(fire_cpc_intersection_xr['Current Day'])\n",
    "    \n",
    "    #preallocate space for the output\n",
    "    df_cpc = pd.DataFrame({'day':np.zeros(ndays),'CPC_DC':np.zeros(ndays),\\\n",
    "                          'CPC_DMC':np.zeros(ndays),  'CPC_FFMC':np.zeros(ndays),\n",
    "                        'CPC_ISI':np.zeros(ndays),'CPC_BUI':np.zeros(ndays),\n",
    "                            'CPC_FWI':np.zeros(ndays),  'CPC_DSR':np.zeros(ndays)})\n",
    "\n",
    "    df_cpc['day'].iloc[:] = pd.to_datetime(fire_cpc_intersection_xr['Current Day'].values)\n",
    "    for var in varis:\n",
    "        df_cpc[var] = np.nansum(fire_cpc_intersection_xr['weights'].values*dat_cpc_sub[var].values, axis=(1,2))\n",
    "    \n",
    "    return df_cpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6208b945",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_cpc_file_namelist(time):\n",
    "    base_filename = '/data2/lthapa/YEAR/FWI_CPC/WESTUS_FWI.CPC.Daily.Default.FULLDATE.nc'\n",
    "    base_filename_list = np.repeat(base_filename, len(time))\n",
    "\n",
    "    \n",
    "    for jj in range(len(time)):\n",
    "        base_filename_list[jj] = base_filename_list[jj].replace('YEAR',time[jj].strftime('%Y')).\\\n",
    "                                    replace('FULLDATE',time[jj].strftime('%Y%m%d'))\n",
    "        \n",
    "    return base_filename_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564fd947",
   "metadata": {},
   "source": [
    "## HRRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4800ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hrrr_timeseries(df,day_start_hour):  \n",
    "    df_hrrr = pd.DataFrame({'day': np.zeros(len(df)),'temp_2m':np.zeros(len(df)), 'q_2m':np.zeros(len(df)), \n",
    "                             'gust_sfc':np.zeros(len(df)),'veggie':np.zeros(len(df)), 'dewpt':np.zeros(len(df)),\n",
    "                             'weasd':np.zeros(len(df)),'soilm':np.zeros(len(df)), 'esat_2m':np.zeros(len(df)),\n",
    "                             'e_2m': np.zeros(len(df)),'vpd_2m':np.zeros(len(df)), 'hwp':np.zeros(len(df)), \n",
    "                             'gust_sfc':np.zeros(len(df)),'veggie':np.zeros(len(df)), 'dewpt':np.zeros(len(df)),\n",
    "                             'veg_term':np.zeros(len(df)),'gust_max_term':np.zeros(len(df)), 'dd_term':np.zeros(len(df)),\n",
    "                             'mois_term': np.zeros(len(df)),'temp_2m':np.zeros(len(df)), 'q_2m':np.zeros(len(df)), \n",
    "                             'gust_sfc':np.zeros(len(df)),'snowc_term':np.zeros(len(df)),'hd0w0':np.zeros(len(df)), 'hd1w0':np.zeros(len(df)),\n",
    "                             'hd2w0':np.zeros(len(df)),'hd3w0':np.zeros(len(df)), 'hd4w0':np.zeros(len(df)),\n",
    "                             'hd5w0':np.zeros(len(df))})\n",
    "    #load in the grid\n",
    "    hrrr_grid = gpd.read_file('HRRR_GRID.geojson')\n",
    "    hrrr_grid = hrrr_grid.to_crs(epsg=3347) #put into lambert conformal conic \n",
    "    \n",
    "    #do the intersection, not with a for loop!\n",
    "    for geom in dg['geometry']:\n",
    "        print(geom)\n",
    "    \n",
    "    #fire_hrrr_intersection = gpd.overlay(df, hrrr_grid, how='intersection',keep_geom_type=False)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    fire_hrrr_intersection['grid intersection area (ha)'] =fire_hrrr_intersection['geometry'].area/10000\n",
    "    fire_hrrr_intersection['weights'] = fire_hrrr_intersection['grid intersection area (ha)']/fire_hrrr_intersection['fire area (ha)'] \n",
    "    \n",
    "    #loop over all of the days we have intersections\n",
    "    times_intersect = np.unique(fire_hrrr_intersection[str(day_start_hour)+ 'Z Start Day'].values)\n",
    "    times_utc = np.unique(fire_hrrr_intersection['UTC Day'].values)\n",
    "    \n",
    "    count = 0\n",
    "    for today in times_intersect:\n",
    "        #get the time\n",
    "        df_sub = fire_hrrr_intersection.iloc[np.where(fire_hrrr_intersection[str(day_start_hour)+ 'Z Start Day'].values==today)]\n",
    "        df_sub = df_sub.set_index([str(day_start_hour)+ 'Z Start Day', 'lat', 'lon'])\n",
    "        intersection_sub = df_sub.to_xarray() #polygon and weights for today\n",
    "\n",
    "        times_back = pd.date_range(start=np.datetime64(today)-np.timedelta64(5,'D'), end=np.datetime64(today)+np.timedelta64(1,'D'))\n",
    "        files_back = make_hrrr_file_namelist(times_back)\n",
    "        print(files_back)\n",
    "        \n",
    "        #load in all the merra files associated with this lookback window\n",
    "        dat_merra = xr.open_mfdataset(files_back,concat_dim='time',combine='nested',compat='override', coords='all')\n",
    "    \n",
    "        #add the derived data (svp, vp, vpd)\n",
    "        dat_merra=dat_merra.assign(ESAT=sat_vap_press(dat_merra.TLML))\n",
    "        dat_merra=dat_merra.assign(E=vap_press(dat_merra.QLML, dat_merra.TLML))\n",
    "        dat_merra=dat_merra.assign(VPD=dat_merra.ESAT-dat_merra.E)\n",
    "        \n",
    "        merra_daily_mean = dat_merra.resample(time='24H',base=day_start_hour, label='left').mean(dim='time') #take the daily mean        \n",
    "        merra_daily_mean_region = merra_daily_mean.sel(lat = np.unique(intersection_sub['lat'].values),\n",
    "                                  lon = np.unique(intersection_sub['lon'].values)) #get the location of the overlaps\n",
    "        \n",
    "        hd0 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))\n",
    "        hd1 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(1,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd2 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(2,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd3 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(3,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd4 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(4,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd5 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(5,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        w = np.nansum((merra_daily_mean_region['SPEEDLML'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))\n",
    "        t = np.nansum((merra_daily_mean_region['TLML'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))\n",
    "        \n",
    "        df_merra.iloc[count,:] = [today+ ' '+str(day_start_hour)+':00:00',t,hd0,w,hd0*w,hd1*w,hd2*w,hd3*w,hd4*w,hd5*w]\n",
    "        dat_merra.close()\n",
    "        count =count+1\n",
    "    return df_merra\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f169d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hrrr_file_namelist(time):\n",
    "    base_filename = '/data2/lthapa/ML_py/pygraf/Processed_HRRR_FULLDATE.nc4'\n",
    "    base_filename_list = np.repeat(base_filename, len(time))\n",
    "\n",
    "    \n",
    "    for jj in range(len(time)):\n",
    "        base_filename_list[jj] = base_filename_list[jj].replace('FULLDATE',time[jj].strftime('%Y%m%d%H'))\n",
    "    return base_filename_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44ee9c5",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0914201",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(times)\n",
    "names = make_merra_file_namelist(times)\n",
    "print(names)\n",
    "dat_cpc = xr.open_mfdataset(names,concat_dim='time',combine='nested',compat='override', coords='all')\n",
    "print(dat_cpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac2ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_daily = gpd.read_file('./fire_polygons/lake_VIIRS_daily.geojson') #polygons and attributes\n",
    "merra_grid = gpd.read_file('MERRA_GRID.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59286f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_total = fire_daily.dissolve()\n",
    "print(fire_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc5cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_total = fire_total.to_crs(epsg=4326) #put into wsg84\n",
    "\n",
    "fire_total['geometry'].iloc[0].bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53afaaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox2ij(lon,lat,bbox):\n",
    "    bbox=np.array(bbox)\n",
    "    mypath=np.array([bbox[[0,1,1,0]],bbox[[2,2,3,3]]]).T\n",
    "    p = path.Path(mypath)\n",
    "    points = np.vstack((lon.flatten(),lat.flatten())).T\n",
    "    n,m = np.shape(lon)\n",
    "    inside = p.contains_points(points).reshape((n,m))\n",
    "    ii,jj = np.meshgrid(range(m),range(n)) #ii is the columns, jj is the rows\n",
    "    return min(ii[inside]),max(ii[inside]),min(jj[inside]),max(jj[inside])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b1a644",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# hdw_type= 'max' (use/report daily maxes), 'ave' (use/report averages)\n",
    "def merra_timeseries_old(df, time, hdw_type):\n",
    "    \n",
    "    df_merra = pd.DataFrame({'day': np.zeros(len(df)),'temp':np.zeros(len(df)), 'vpd':np.zeros(len(df)), \n",
    "                             'wind':np.zeros(len(df)),'hd0w0':np.zeros(len(df)), 'hd1w0':np.zeros(len(df)),\n",
    "                             'hd2w0':np.zeros(len(df)),'hd3w0':np.zeros(len(df)), 'hd4w0':np.zeros(len(df)),\n",
    "                             'hd5w0':np.zeros(len(df))})\n",
    "    #load in the grid\n",
    "    merra_grid = gpd.read_file('MERRA_GRID.geojson')\n",
    "    merra_grid = merra_grid.to_crs(epsg=3347) #put into lambert conformal conic \n",
    "    \n",
    "    #do the intersection, not with a for loop!\n",
    "    fire_merra_intersection = gpd.overlay(df, merra_grid, how='intersection',keep_geom_type=False)\n",
    "    fire_merra_intersection['grid intersection area (ha)'] =fire_merra_intersection['geometry'].area/10000\n",
    "    fire_merra_intersection['weights'] = fire_merra_intersection['grid intersection area (ha)']/fire_merra_intersection['fire area (ha)'] \n",
    "    \n",
    "    \n",
    "    #loop over all of the days we have intersections\n",
    "    times_intersect = np.unique(fire_merra_intersection['Current Day'].values)\n",
    "    \n",
    "    count = 0\n",
    "    for today in times_intersect:        \n",
    "        #get the time\n",
    "        df_sub = fire_merra_intersection.iloc[np.where(fire_merra_intersection['Current Day'].values==today)]\n",
    "        df_sub = df_sub.set_index(['Current Day', 'lat', 'lon'])\n",
    "        intersection_sub = df_sub.to_xarray() #polygon and weights for today\n",
    "\n",
    "        times_back = pd.date_range(start=np.datetime64(today)-np.timedelta64(5,'D'), end=np.datetime64(today))\n",
    "        files_back = make_merra_file_namelist(times_back)\n",
    "\n",
    "        #load in all the merra files associated with this lookback window\n",
    "        dat_merra = xr.open_mfdataset(files_back,concat_dim='time',combine='nested',compat='override', coords='all')\n",
    "\n",
    "        #add the derived data (svp, vp, vpd)\n",
    "        dat_merra=dat_merra.assign(ESAT=sat_vap_press(dat_merra.TLML))\n",
    "        dat_merra=dat_merra.assign(E=vap_press(dat_merra.QLML, dat_merra.TLML))\n",
    "        dat_merra=dat_merra.assign(VPD=dat_merra.ESAT-dat_merra.E)\n",
    "    \n",
    "        merra_daily_mean = dat_merra.resample(time='D').mean(dim='time') #take the daily mean\n",
    "        \n",
    "        merra_daily_mean_region = merra_daily_mean.sel(lat = np.unique(intersection_sub['lat'].values),\n",
    "                                  lon = np.unique(intersection_sub['lon'].values)) #get the location of the overlaps\n",
    "\n",
    "        hd0 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today)).values)*(intersection_sub['weights'].values))\n",
    "        hd1 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today)-np.timedelta64(1,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd2 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today)-np.timedelta64(2,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd3 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today)-np.timedelta64(3,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd4 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today)-np.timedelta64(4,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd5 = np.nansum((merra_daily_mean_region['VPD'].sel(time=np.datetime64(today)-np.timedelta64(5,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        w = np.nansum((merra_daily_mean_region['SPEEDLML'].sel(time=np.datetime64(today)).values)*(intersection_sub['weights'].values))\n",
    "        t = np.nansum((merra_daily_mean_region['TLML'].sel(time=np.datetime64(today)).values)*(intersection_sub['weights'].values))\n",
    "        \n",
    "        df_merra.iloc[count,:] = [today,t,hd0,w,hd0*w,hd1*w,hd2*w,hd3*w,hd4*w,hd5*w]\n",
    "        dat_merra.close()\n",
    "        count =count+1\n",
    "    return df_merra\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
