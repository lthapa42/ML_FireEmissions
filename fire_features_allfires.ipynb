{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dd26306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import path\n",
    "import os\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "np.set_printoptions(threshold=100000)\n",
    "from shapely.geometry import Polygon, Point, MultiPoint, LineString, LinearRing\n",
    "from shapely.ops import cascaded_union, unary_union, transform\n",
    "import datetime\n",
    "import math\n",
    "from scipy.ndimage.interpolation import shift\n",
    "import scipy.interpolate as si\n",
    "import shapely.wkt\n",
    "from shapely.validation import explain_validity\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "from my_functions import sat_vap_press, vap_press, hot_dry_windy, haines\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from os.path import exists\n",
    "import rasterio\n",
    "from rasterio.windows import get_data_window,Window, from_bounds\n",
    "from rasterio.plot import show\n",
    "from itertools import product\n",
    "\n",
    "from timezonefinder import TimezoneFinder\n",
    "import pytz\n",
    "\n",
    "from helper_functions import build_one_gridcell, calculate_intersection, calculate_grid_cell_corners, make_file_namelist, generate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2592f4c8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Unweighted Fire Features\n",
    "We are testing the effect of taking the non-area weighted mean, ie the mean over the intersecting grid cells, for the April 17th 2023 research update slideshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f661e30e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/lthapa/ML_daily/fire_polygons/lake_VIIRS_daily_12Z_day_start.geojson\n",
      "epsg:3347\n",
      "   12Z Start Day  Incident Number Fire Name     UTC Day   Lat Fire  \\\n",
      "0     2020-08-12       11773470.0      LAKE  2020-08-13  34.678611   \n",
      "1     2020-08-13       11773470.0      LAKE  2020-08-14  34.678611   \n",
      "2     2020-08-14       11773470.0      LAKE  2020-08-15  34.678611   \n",
      "3     2020-08-15       11773470.0      LAKE  2020-08-16  34.678611   \n",
      "4     2020-08-16       11773470.0      LAKE  2020-08-17  34.678611   \n",
      "5     2020-08-17       11773470.0      LAKE  2020-08-18  34.678611   \n",
      "6     2020-08-18       11773470.0      LAKE  2020-08-19  34.678611   \n",
      "7     2020-08-19       11773470.0      LAKE  2020-08-20  34.678611   \n",
      "8     2020-08-20       11773470.0      LAKE  2020-08-21  34.678611   \n",
      "9     2020-08-21       11773470.0      LAKE  2020-08-22  34.678611   \n",
      "10    2020-08-22       11773470.0      LAKE  2020-08-23  34.678611   \n",
      "11    2020-08-29       11773470.0      LAKE  2020-08-30  34.678611   \n",
      "\n",
      "      Lon Fire  Number of NEW VIIRS points   NEW FRP  \\\n",
      "0  -118.451944                       132.0    782.77   \n",
      "1  -118.451944                        25.0   1644.50   \n",
      "2  -118.451944                        70.0  11606.51   \n",
      "3  -118.451944                        34.0   7911.41   \n",
      "4  -118.451944                        28.0   1806.03   \n",
      "5  -118.451944                       133.0   2547.34   \n",
      "6  -118.451944                        32.0   1936.13   \n",
      "7  -118.451944                        55.0   3272.50   \n",
      "8  -118.451944                        63.0   1469.15   \n",
      "9  -118.451944                        54.0   2983.50   \n",
      "10 -118.451944                         4.0     34.50   \n",
      "11 -118.451944                         1.0      1.74   \n",
      "\n",
      "                                             geometry  fire area (ha)  \n",
      "0   MULTIPOLYGON (((3641986.585 323212.339, 364197...     2056.378485  \n",
      "1   MULTIPOLYGON (((3630083.512 322187.932, 363007...      196.114730  \n",
      "2   MULTIPOLYGON (((3631093.725 323487.926, 363109...      796.663316  \n",
      "3   MULTIPOLYGON (((3629744.627 321900.201, 362973...      918.180196  \n",
      "4   MULTIPOLYGON (((3627559.548 324615.168, 362755...      445.839030  \n",
      "5   MULTIPOLYGON (((3633424.529 326832.891, 363342...     2461.323352  \n",
      "6   MULTIPOLYGON (((3627744.130 324262.296, 362773...      431.101382  \n",
      "7   MULTIPOLYGON (((3628041.297 324222.835, 362803...      796.799837  \n",
      "8   MULTIPOLYGON (((3626573.771 331082.596, 362656...     1017.305845  \n",
      "9   MULTIPOLYGON (((3626799.236 325038.628, 362679...      632.931011  \n",
      "10  MULTIPOLYGON (((3625516.379 324486.136, 362550...       27.908175  \n",
      "11  POLYGON ((3633778.944 329023.363, 3633772.010 ...        0.094198  \n"
     ]
    }
   ],
   "source": [
    "fire_incidents = ['204 COW', 'GRANITE GULCH', 'SHADY', 'WILLIAMS FLATS', 'PEDRO MOUNTAIN', 'WALKER',\n",
    "                  'AUGUST COMPLEX', 'BOBCAT', 'CAMERON PEAK', 'CREEK', 'DOLAN', 'EAST TROUBLESOME',\n",
    "                  'HOLIDAY FARM', 'LAKE','RIVERSIDE', 'PINE GULCH', 'CZU', 'SCU']\n",
    "\"\"\"fire_incidents = ['AUGUST COMPLEX', 'BOBCAT', 'CAMERON PEAK', 'CREEK', 'DOLAN', 'EAST TROUBLESOME',\n",
    "                  'HOLIDAY FARM', 'LAKE','RIVERSIDE', 'PINE GULCH', 'CZU', 'SCU']\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"fire_incidents=['BOBCAT', 'CAMERON PEAK', 'CREEK','DOLAN', 'EAST TROUBLESOME',\n",
    "                  'HOLIDAY FARM', 'LAKE','RIVERSIDE', 'PINE GULCH', 'CZU', 'SCU']\"\"\"\n",
    "\n",
    "#fire_incidents=['AUGUST COMPLEX']\n",
    "#fire_incidents=['BOBCAT']\n",
    "fire_incidents=['LAKE']\n",
    "#fire_incidents = ['GRANITE GULCH', 'SHADY', 'WILLIAMS FLATS', 'PEDRO MOUNTAIN', 'WALKER']\n",
    "\n",
    "path_poly = '/data2/lthapa/ML_daily/fire_polygons/'\n",
    "suffix_poly = 'Z_day_start.geojson'\n",
    "start_time=12\n",
    "\n",
    "for jj in range(len(fire_incidents)):\n",
    "    fire_name = fire_incidents[jj].lower().replace(' ','_')\n",
    "    print(path_poly+fire_name+'_VIIRS_daily_'+str(start_time)+suffix_poly)\n",
    "    fire_daily = gpd.read_file(path_poly+fire_name+'_VIIRS_daily_'+str(start_time)+suffix_poly)\n",
    "    print(fire_daily.crs)\n",
    "    fire_daily=fire_daily.drop(columns=['Current Overpass'])\n",
    "    fire_daily = fire_daily.drop(np.where(fire_daily['geometry']==None)[0])\n",
    "    fire_daily['fire area (ha)'] = fire_daily['geometry'].area/10000 #hectares. from m2\n",
    "    fire_daily.set_geometry(col='geometry', inplace=True) #designate the geometry column\n",
    "    fire_daily = fire_daily.rename(columns={'Current Day':'UTC Day', 'Local Day': str(start_time)+ 'Z Start Day'})\n",
    "    print(fire_daily)\n",
    "    \n",
    "    #THE TOP FEATURES ARE HWP, PWS, ESI, LOW FUEL, DROUGHT CODE, BLENDED_SM\n",
    "    \n",
    "    \"\"\"#HRRR\n",
    "    hrrr_unweighted = hrrr_timeseries(fire_daily, start_time, False)\n",
    "    print(hrrr_unweighted)\n",
    "    hrrr_unweighted.to_csv('./fire_features_3/'+fire_name+'_Daily_HRRR_Unweighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    \n",
    "    #pws\n",
    "    pws_unweighted=pws_timeseries(fire_daily,start_time, False)\n",
    "    print(pws_unweighted)\n",
    "    pws_unweighted.to_csv('./fire_features_3/'+fire_name+'_Daily_PWS_Unweighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    \n",
    "    #esi\n",
    "    esi_unweighted = esi_timeseries(fire_daily, start_time, False)\n",
    "    print(esi_unweighted)\n",
    "    esi_unweighted.to_csv('./fire_features_3/'+fire_name+'_Daily_ESI_Unweighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    \n",
    "    #fuel loading (1km)\n",
    "    fuel_unweighted = fuel_loading_timeseries(fire_daily, start_time, False)\n",
    "    print(fuel_unweighted)\n",
    "    fuel_unweighted.to_csv('./fire_features_3/'+fire_name+'_Daily_FUEL_LOADING_Unweighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    \n",
    "    #imerg \n",
    "    imerg_unweighted = imerg_fwi_timeseries(fire_daily, start_time, False)\n",
    "    print(imerg_unweighted)\n",
    "    imerg_unweighted.to_csv('./fire_features_3/'+fire_name+'_Daily_FWI_IMERG_Unweighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    \n",
    "    \n",
    "    #smops\n",
    "    smops_unweighted=smops_timeseries(fire_daily,start_time, False)\n",
    "    print(smops_unweighted)\n",
    "    smops_unweighted.to_csv('./fire_features_3/'+fire_name+'_Daily_SMOPS_Unweighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    \"\"\"\n",
    "   \n",
    "    \"\"\"hdw = hdw_timeseries(fire_daily, start_time, False)\n",
    "    print(hdw)\n",
    "    hdw.to_csv('./fire_features_3/'+fire_name+'_Daily_Lagged_HDW_Unweighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b821d2ba",
   "metadata": {},
   "source": [
    "## Weighted Fire Features\n",
    "We have been weighting the features by the area of the polygon contained in each intersecting grid cell, and this was used exclusively in the machine learning and index building until the April 10th, 2023 research Update slideshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3f99fd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/lthapa/ML_daily/fire_polygons/ClippedFires2020_VIIRS_daily_12Z_Day_Start.geojson\n",
      "epsg:3347\n",
      "We are processing 273 unique fires for 2020\n",
      "   INC209R_IDENTIFIER     REPORT_FROM_DATE       REPORT_TO_DATE  \\\n",
      "0            11683732  01/09/2020 12:30:00  01/10/2020 08:15:00   \n",
      "1            11683744  01/10/2020 08:00:00  01/12/2020 08:00:00   \n",
      "2            11683755  01/10/2020 09:00:00  01/11/2020 09:00:00   \n",
      "3            11683718  01/08/2020 12:30:00  01/08/2020 21:15:00   \n",
      "4            11683871  01/09/2020 17:00:00  01/10/2020 14:00:00   \n",
      "\n",
      "   PCT_CONTAINED_COMPLETED  INCIDENT_NAME  \\\n",
      "0                     96.0  Dogwood Trail   \n",
      "1                    100.0  Dogwood Trail   \n",
      "2                    100.0  Dogwood Trail   \n",
      "3                     90.0  Dogwood Trail   \n",
      "4                    100.0       WEST 380   \n",
      "\n",
      "                       IRWIN_IDENTIFIER  \n",
      "0  AA302C33-2DC5-44D0-9A55-19A133EF04B3  \n",
      "1  AA302C33-2DC5-44D0-9A55-19A133EF04B3  \n",
      "2  AA302C33-2DC5-44D0-9A55-19A133EF04B3  \n",
      "3  AA302C33-2DC5-44D0-9A55-19A133EF04B3  \n",
      "4  66AC9FFB-E40F-4C65-8D23-78AB32B85D70  \n",
      "0\n",
      "00B3EDB2-87A4-4D0A-A834-4E1012BF9693\n",
      "[0.9999999999999998, 1.000000000000001, 1.0000000000000004, 0.9999999999999998, 0.9999999999999998, 1.0000000000000004, 1.000000000000003, 0.9999999999999982, 1.0000000000000009, 1.0000000000000024, 0.9999999999999998, 1.0, 1.0]\n",
      "<xarray.Dataset>\n",
      "Dimensions:   (Time: 15, south_north: 3179, west_east: 5399)\n",
      "Coordinates:\n",
      "  * Time      (Time) datetime64[ns] 2020-09-07 2020-09-08 ... 2020-09-21\n",
      "Dimensions without coordinates: south_north, west_east\n",
      "Data variables:\n",
      "    XLAT_M    (Time, south_north, west_east) float32 dask.array<chunksize=(1, 3179, 5399), meta=np.ndarray>\n",
      "    XLONG_M   (Time, south_north, west_east) float32 dask.array<chunksize=(1, 3179, 5399), meta=np.ndarray>\n",
      "    FMCG2D    (Time, south_north, west_east) float32 dask.array<chunksize=(1, 3179, 5399), meta=np.ndarray>\n",
      "    FMCGLH2D  (Time, south_north, west_east) float32 dask.array<chunksize=(1, 3179, 5399), meta=np.ndarray>\n",
      "Attributes:\n",
      "    history:                    Thu Apr  4 13:20:35 2019: ncks -A -v XLAT_M,X...\n",
      "    history_of_appended_files:  Thu Apr  4 13:20:35 2019: Appended file geo_e...\n",
      "    NCO:                        netCDF Operators version 4.7.5 (Homepage = ht...\n",
      "    time:                       2020-09-07_20:00:00\n",
      "    DX:                         1000.0\n",
      "    DY:                         1000.0\n",
      "    TRUELAT1:                   38.5\n",
      "    TRUELAT2:                   38.5\n",
      "    STAND_LON:                  -97.5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-3e53d847488b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m                               (days<=np.datetime64(str(years[jj]+1)+'-01-01'))].reset_index(drop=True)\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mncar_weighted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncar_unweighted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mncar_timeseries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_fire\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \"\"\" df_fire.to_crs('epsg:4326').plot(column='12Z Start Day')\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "path_poly = '/data2/lthapa/ML_daily/fire_polygons/'\n",
    "suffix_poly = 'Z_Day_Start.geojson'\n",
    "start_time=12\n",
    "resources_file  = '../YYYY_PCT_CONT.xlsx'\n",
    "\n",
    "#years =[2019,2020]\n",
    "years = [2020]\n",
    "\n",
    "\n",
    "\n",
    "for jj in range(len(years)):\n",
    "    print(path_poly+'ClippedFires'+str(years[jj])+'_VIIRS_daily_'+str(start_time)+suffix_poly)\n",
    "    \n",
    "    fire_daily = gpd.read_file(path_poly+'ClippedFires'+str(years[jj])+'_VIIRS_daily_'+str(start_time)+suffix_poly)\n",
    "    print(fire_daily.crs)\n",
    "    \n",
    "    fire_daily=fire_daily.drop(columns=['Current Overpass'])\n",
    "    fire_daily = fire_daily.drop(np.where(fire_daily['geometry']==None)[0])\n",
    "    fire_daily['fire area (ha)'] = fire_daily['geometry'].area/10000 #hectares. from m2\n",
    "    fire_daily.set_geometry(col='geometry', inplace=True) #designate the geometry column\n",
    "    fire_daily = fire_daily.rename(columns={'Current Day':'UTC Day', 'Local Day': str(start_time)+ 'Z Start Day'})\n",
    "    \n",
    "    irwinIDs = np.unique(fire_daily['irwinID'].values)\n",
    "    print('We are processing ' +str(len(irwinIDs)) + ' unique fires for '+ str(years[jj]))\n",
    "\n",
    " \n",
    "    \n",
    "    for ii in range(len(irwinIDs)):\n",
    "        print(ii)\n",
    "        fireID=irwinIDs[ii]\n",
    "        print(fireID)\n",
    "        df_fire = fire_daily[fire_daily['irwinID']==fireID] #this is what gets fed to the feature selection code\n",
    "\n",
    "        #maybe we filter 12Z Start dates to where we have data?\n",
    "        days=np.array(df_fire['12Z Start Day'].values, dtype='datetime64')\n",
    "        df_fire = df_fire[(days>=np.datetime64(str(years[jj])+'-07-01'))&\n",
    "                              (days<=np.datetime64(str(years[jj]+1)+'-01-01'))].reset_index(drop=True)\n",
    "        \n",
    "        ncar_weighted, ncar_unweighted = ncar_timeseries(df_fire, start_time)\n",
    "        ncar_weighted = pd.concat([esi_weighted, pd.DataFrame({'irwinID':[fireID]*len(esi_weighted)})], axis=1)\n",
    "        ncar_unweighted = pd.concat([esi_unweighted, pd.DataFrame({'irwinID':[fireID]*len(esi_unweighted)})], axis=1)\n",
    "        print(ncar_weighted)\n",
    "        print(ncar_unweighted)\n",
    "        \n",
    "        ncar_weighted.to_csv('./fire_features_ncar/'+fireID+str(years[jj])+'_Daily_ESI_Weighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "        ncar_unweighted.to_csv('./fire_features_ncar/'+fireID+str(years[jj])+'_Daily_ESI_Unweighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "        q\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a9dd435c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   12Z Start Day                               irwinID  Incident Number  \\\n",
      "0     2020-09-07  00B3EDB2-87A4-4D0A-A834-4E1012BF9693       11943503.0   \n",
      "1     2020-09-08  00B3EDB2-87A4-4D0A-A834-4E1012BF9693       11943503.0   \n",
      "2     2020-09-09  00B3EDB2-87A4-4D0A-A834-4E1012BF9693       11943503.0   \n",
      "3     2020-09-10  00B3EDB2-87A4-4D0A-A834-4E1012BF9693       11943503.0   \n",
      "4     2020-09-11  00B3EDB2-87A4-4D0A-A834-4E1012BF9693       11943503.0   \n",
      "5     2020-09-12  00B3EDB2-87A4-4D0A-A834-4E1012BF9693       11943503.0   \n",
      "6     2020-09-13  00B3EDB2-87A4-4D0A-A834-4E1012BF9693       11943503.0   \n",
      "7     2020-09-14  00B3EDB2-87A4-4D0A-A834-4E1012BF9693       11943503.0   \n",
      "8     2020-09-15  00B3EDB2-87A4-4D0A-A834-4E1012BF9693       11943503.0   \n",
      "9     2020-09-16  00B3EDB2-87A4-4D0A-A834-4E1012BF9693       11943503.0   \n",
      "10    2020-09-17  00B3EDB2-87A4-4D0A-A834-4E1012BF9693       11943503.0   \n",
      "11    2020-09-19  00B3EDB2-87A4-4D0A-A834-4E1012BF9693       11943503.0   \n",
      "12    2020-09-20  00B3EDB2-87A4-4D0A-A834-4E1012BF9693       11943503.0   \n",
      "\n",
      "     Fire Name     UTC Day  Lat Fire  Lon Fire  Number of NEW VIIRS points  \\\n",
      "0   BIG HOLLOW  2020-09-08  45.92707 -121.9799                        12.0   \n",
      "1   BIG HOLLOW  2020-09-09  45.92707 -121.9799                       296.0   \n",
      "2   BIG HOLLOW  2020-09-10  45.92707 -121.9799                       272.0   \n",
      "3   BIG HOLLOW  2020-09-11  45.92707 -121.9799                        89.0   \n",
      "4   BIG HOLLOW  2020-09-12  45.92707 -121.9799                        76.0   \n",
      "5   BIG HOLLOW  2020-09-13  45.92707 -121.9799                        41.0   \n",
      "6   BIG HOLLOW  2020-09-13  45.92707 -121.9799                         4.0   \n",
      "7   BIG HOLLOW  2020-09-15  45.92707 -121.9799                        48.0   \n",
      "8   BIG HOLLOW  2020-09-16  45.92707 -121.9799                        42.0   \n",
      "9   BIG HOLLOW  2020-09-17  45.92707 -121.9799                        41.0   \n",
      "10  BIG HOLLOW  2020-09-18  45.92707 -121.9799                         8.0   \n",
      "11  BIG HOLLOW  2020-09-20  45.92707 -121.9799                         2.0   \n",
      "12  BIG HOLLOW  2020-09-21  45.92707 -121.9799                         3.0   \n",
      "\n",
      "    NEW FRP   daily_area                                           geometry  \\\n",
      "0    268.54   211.574663  MULTIPOLYGON (((3919568.378 1635426.400, 39195...   \n",
      "1   3257.75  2475.259442  MULTIPOLYGON (((3920185.284 1634911.471, 39201...   \n",
      "2   2334.30  1885.054054  MULTIPOLYGON (((3920000.230 1633013.462, 39199...   \n",
      "3    835.20   736.680530  MULTIPOLYGON (((3920648.358 1633085.585, 39206...   \n",
      "4    214.17   406.680814  MULTIPOLYGON (((3911383.050 1643153.142, 39113...   \n",
      "5    303.68   167.081336  MULTIPOLYGON (((3919980.922 1632657.416, 39199...   \n",
      "6     24.75    31.968238  MULTIPOLYGON (((3919065.991 1632169.666, 39190...   \n",
      "7    389.40   584.760256  MULTIPOLYGON (((3919351.413 1632338.820, 39193...   \n",
      "8    106.30   525.254140  MULTIPOLYGON (((3918938.379 1631855.951, 39189...   \n",
      "9     58.60   224.857505  MULTIPOLYGON (((3919346.754 1631873.918, 39193...   \n",
      "10    36.36    64.260547  MULTIPOLYGON (((3919358.094 1631780.165, 39193...   \n",
      "11     2.09     0.287891  POLYGON ((3922420.882 1637017.336, 3922432.844...   \n",
      "12     2.27    34.988528  MULTIPOLYGON (((3919238.295 1631665.005, 39192...   \n",
      "\n",
      "    fire area (ha)  \n",
      "0       161.602295  \n",
      "1      3773.651926  \n",
      "2      1582.910615  \n",
      "3      1167.869442  \n",
      "4       273.393461  \n",
      "5       235.484215  \n",
      "6        12.937546  \n",
      "7       342.545376  \n",
      "8       250.415429  \n",
      "9       118.551686  \n",
      "10       40.405469  \n",
      "11        0.116509  \n",
      "12        8.863281  \n",
      "[1.0000000000000027, 1.0000000000000004, 0.9999999999999984, 1.0000000000000004, 0.9999999999999947, 1.0000000000000042, 1.0000000000000235, 1.0000000000000007, 0.9999999999999993, 0.9999999999999941, 0.9999999999999922, 1.0000000000002542, 1.0000000000000147]\n",
      "<xarray.Dataset>\n",
      "Dimensions:                      (12Z Start Day: 13, col: 18, row: 17)\n",
      "Coordinates:\n",
      "  * 12Z Start Day                (12Z Start Day) object '2020-09-07' ... '202...\n",
      "  * row                          (row) int64 2665 2666 2667 ... 2679 2680 2681\n",
      "  * col                          (col) int64 796 797 798 799 ... 810 811 812 813\n",
      "Data variables: (12/16)\n",
      "    lat                          (12Z Start Day, row, col) float64 nan ... nan\n",
      "    lon                          (12Z Start Day, row, col) float64 nan ... nan\n",
      "    irwinID                      (12Z Start Day, row, col) object nan ... nan\n",
      "    Incident Number              (12Z Start Day, row, col) float64 nan ... nan\n",
      "    Fire Name                    (12Z Start Day, row, col) object nan ... nan\n",
      "    UTC Day                      (12Z Start Day, row, col) object nan ... nan\n",
      "    ...                           ...\n",
      "    daily_area                   (12Z Start Day, row, col) float64 nan ... nan\n",
      "    fire area (ha)               (12Z Start Day, row, col) float64 nan ... nan\n",
      "    geometry                     (12Z Start Day, row, col) object nan ... nan\n",
      "    grid intersection area (ha)  (12Z Start Day, row, col) float64 nan ... nan\n",
      "    weights                      (12Z Start Day, row, col) float64 nan ... nan\n",
      "    weights_mask                 (12Z Start Day, row, col) float64 nan ... nan\n",
      "<xarray.Dataset>\n",
      "Dimensions:   (Time: 15, south_north: 3179, west_east: 5399)\n",
      "Coordinates:\n",
      "  * Time      (Time) datetime64[ns] 2020-09-07 2020-09-08 ... 2020-09-21\n",
      "Dimensions without coordinates: south_north, west_east\n",
      "Data variables:\n",
      "    XLAT_M    (Time, south_north, west_east) float32 dask.array<chunksize=(1, 3179, 5399), meta=np.ndarray>\n",
      "    XLONG_M   (Time, south_north, west_east) float32 dask.array<chunksize=(1, 3179, 5399), meta=np.ndarray>\n",
      "    FMCG2D    (Time, south_north, west_east) float32 dask.array<chunksize=(1, 3179, 5399), meta=np.ndarray>\n",
      "    FMCGLH2D  (Time, south_north, west_east) float32 dask.array<chunksize=(1, 3179, 5399), meta=np.ndarray>\n",
      "Attributes:\n",
      "    history:                    Thu Apr  4 13:20:35 2019: ncks -A -v XLAT_M,X...\n",
      "    history_of_appended_files:  Thu Apr  4 13:20:35 2019: Appended file geo_e...\n",
      "    NCO:                        netCDF Operators version 4.7.5 (Homepage = ht...\n",
      "    time:                       2020-09-07_20:00:00\n",
      "    DX:                         1000.0\n",
      "    DY:                         1000.0\n",
      "    TRUELAT1:                   38.5\n",
      "    TRUELAT2:                   38.5\n",
      "    STAND_LON:                  -97.5\n",
      "<xarray.DataArray 'FMCG2D' (Time: 13, south_north: 17, west_east: 18)>\n",
      "dask.array<getitem, shape=(13, 17, 18), dtype=float32, chunksize=(1, 17, 18), chunktype=numpy.ndarray>\n",
      "Coordinates:\n",
      "  * Time     (Time) datetime64[ns] 2020-09-07 2020-09-08 ... 2020-09-20\n",
      "Dimensions without coordinates: south_north, west_east\n",
      "Attributes:\n",
      "    FieldType:    104\n",
      "    MemoryOrder:  XY \n",
      "    units:        decimals\n",
      "    description:  Dead Fuel Moisture Content\n",
      "    stagger:      M\n",
      "    sr_x:         1\n",
      "    sr_y:         1\n",
      "          day    FMCG2D  FMCGLH2D\n",
      "0  2020-09-07  0.092936  0.807539\n",
      "1  2020-09-08  0.090713  0.744046\n",
      "2  2020-09-09       NaN       NaN\n",
      "3  2020-09-10  0.125350  0.666944\n",
      "4  2020-09-11  0.095486  0.797660\n",
      "5  2020-09-12  0.082663  0.885906\n",
      "6  2020-09-13  0.111290  0.874955\n",
      "7  2020-09-14  0.108483  0.872289\n",
      "8  2020-09-15  0.110794  0.942089\n",
      "9  2020-09-16  0.097826  0.888965\n",
      "10 2020-09-17  0.083988  0.873723\n",
      "11 2020-09-19  0.086229  0.681515\n",
      "12 2020-09-20  0.123261  0.820758\n"
     ]
    }
   ],
   "source": [
    "print(df_fire)\n",
    "ncar_weighted, ncar_unweighted = ncar_timeseries(df_fire, start_time)\n",
    "print(ncar_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "122eae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ncar grid was 1000m = 1km = grid resolution\n",
    "# fwi grid was 10000m=10km = grid resolution\n",
    "def ncar_timeseries(df, day_start_hour):\n",
    "    varis_ncar = ['day','FMCG2D','FMCGLH2D']\n",
    "    df_ncar_weighted = generate_df(varis_ncar, len(df))\n",
    "    df_ncar_unweighted = generate_df(varis_ncar, len(df))\n",
    "\n",
    "    #do the intersection, in parallel\n",
    "    ncar_intersections = Parallel(n_jobs=8)(delayed(calculate_intersection)\n",
    "                                 (df.iloc[ii:ii+1],'NCAR_MOISTURE_GRID',1000) \n",
    "                                 for ii in range(len(df)))\n",
    "    print([ncar_intersections[jj]['weights'].sum() for jj in range(len(ncar_intersections))])\n",
    "\n",
    "    \n",
    "    fire_ncar_intersection=gpd.GeoDataFrame(pd.concat(ncar_intersections, ignore_index=True))\n",
    "    fire_ncar_intersection.set_geometry(col='geometry')  \n",
    "    fire_ncar_intersection = fire_ncar_intersection.set_index([str(day_start_hour)+'Z Start Day', 'row', 'col'])\n",
    "    fire_ncar_intersection=fire_ncar_intersection[~fire_ncar_intersection.index.duplicated()]\n",
    "\n",
    "    fire_ncar_intersection_xr = fire_ncar_intersection.to_xarray()\n",
    "    fire_ncar_intersection_xr['weights_mask'] = xr.where(fire_ncar_intersection_xr['weights']>0,1, np.nan)\n",
    "    print(fire_ncar_intersection_xr)\n",
    "    #print(fire_ncar_intersection)\n",
    "    #load in rave data associated with the fire\n",
    "    times = pd.date_range(np.datetime64(df[str(day_start_hour)+ 'Z Start Day'].iloc[0]),\n",
    "                        np.datetime64(df[str(day_start_hour)+ 'Z Start Day'].iloc[len(df)-1])+\n",
    "                        np.timedelta64(1,'D'))\n",
    "    ncar_filenames,times_back_used = make_file_namelist(times,'/data2/lthapa/YYYY/FMC/fmc_YYYYMMDD_20Z.nc')\n",
    "\n",
    "    \n",
    "    dat_ncar = xr.open_mfdataset(ncar_filenames,concat_dim='Time',combine='nested',compat='override', coords='all')\n",
    "    #dat_ncar = dat_ncar.assign_coords({'Time': times_back_used}) #assign coords so we can select in time\n",
    "    dat_ncar = dat_ncar.reindex(Time=times_back_used,method='nearest') #makes the weekly data daily\n",
    "    print(dat_ncar)\n",
    "    \n",
    "    \n",
    "    #select the locations and times we want\n",
    "    dat_ncar_sub = dat_ncar.isel(south_north = fire_ncar_intersection_xr['row'].values.astype(int), \n",
    "                                 west_east = fire_ncar_intersection_xr['col'].values.astype(int)).sel(\n",
    "                                 Time = pd.to_datetime(fire_ncar_intersection_xr[str(day_start_hour)+'Z Start Day'].values))#these should be lined up correctly\n",
    "    print(dat_ncar_sub['FMCG2D'])\n",
    "    df_ncar_weighted['day'].iloc[:] = pd.to_datetime(fire_ncar_intersection_xr[str(day_start_hour)+'Z Start Day'].values)\n",
    "    df_ncar_unweighted['day'].iloc[:] = pd.to_datetime(fire_ncar_intersection_xr[str(day_start_hour)+'Z Start Day'].values)\n",
    "\n",
    "    for var in varis_ncar[1:]:\n",
    "        df_ncar_weighted[var] = np.nansum(fire_ncar_intersection_xr['weights'].values*dat_ncar_sub[var].values, axis=(1,2))\n",
    "        df_ncar_unweighted[var] = np.nansum(fire_ncar_intersection_xr['weights'].values*dat_ncar_sub[var].values, axis=(1,2))\n",
    "\n",
    "           \n",
    "    #this day is messed up, fill it in with NANS\n",
    "    df_ncar_weighted.iloc[df_ncar_weighted['day']=='2020-09-09'] = [pd.date_range(np.datetime64('2020-09-09'),np.datetime64('2020-09-09')+np.timedelta64(0,'D')),\n",
    "                                                           np.nan,np.nan]\n",
    "    df_ncar_unweighted.iloc[df_ncar_unweighted['day']=='2020-09-09'] = [pd.date_range(np.datetime64('2020-09-09'),np.datetime64('2020-09-09')+np.timedelta64(0,'D')),\n",
    "                                                           np.nan,np.nan]\n",
    "    return df_ncar_weighted, df_ncar_unweighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db42b75",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## test out other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b700f7fb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "      print(ii)\n",
    "        fireID=irwinIDs[ii]\n",
    "        print(fireID)\n",
    "        df_fire = fire_daily[fire_daily['irwinID']==fireID] #this is what gets fed to the feature selection code\n",
    "        \n",
    "        #maybe we filter 12Z Start dates to where we have data?\n",
    "        days=np.array(df_fire['12Z Start Day'].values, dtype='datetime64')\n",
    "        df_fire = df_fire[(days>=np.datetime64(str(years[jj])+'-07-01'))&\n",
    "                              (days<=np.datetime64(str(years[jj]+1)+'-01-01'))].reset_index(drop=True)\n",
    "\n",
    "        #PWS\n",
    "        esi_weighted, esi_unweighted = esi_timeseries(df_fire, start_time)\n",
    "        esi_weighted = pd.concat([esi_weighted, pd.DataFrame({'irwinID':[fireID]*len(esi_weighted)})], axis=1)\n",
    "        esi_unweighted = pd.concat([esi_unweighted, pd.DataFrame({'irwinID':[fireID]*len(esi_unweighted)})], axis=1)\n",
    "        print(esi_weighted)\n",
    "        print(esi_unweighted) loading_weighted_all = pd.DataFrame() \n",
    "    loading_unweighted_all = pd.DataFrame()\n",
    "    \n",
    "    for ii in range(0,1):#len(irwinIDs)):\n",
    "        print(ii)\n",
    "        fireID=irwinIDs[ii]\n",
    "        print(fireID)\n",
    "        df_fire = fire_daily[fire_daily['irwinID']==fireID] #this is what gets fed to the feature selection code\n",
    "        \n",
    "        #maybe we filter 12Z Start dates to where we have data?\n",
    "        days=np.array(df_fire['12Z Start Day'].values, dtype='datetime64')\n",
    "        df_fire = df_fire[(days>=np.datetime64(str(years[jj])+'-07-01'))&\n",
    "                              (days<=np.datetime64(str(years[jj]+1)+'-01-01'))].reset_index(drop=True)\n",
    "\n",
    "        #PWS\n",
    "        loading_weighted, loading_unweighted = fuel_loading_timeseries(df_fire, start_time)\n",
    "        loading_weighted = pd.concat([loading_weighted, pd.DataFrame({'irwinID':[fireID]*len(loading_weighted)})], axis=1)\n",
    "        loading_unweighted = pd.concat([loading_unweighted, pd.DataFrame({'irwinID':[fireID]*len(loading_unweighted)})], axis=1)\n",
    "        print(loading_weighted)\n",
    "        print(loading_unweighted)\n",
    "        \n",
    "        loading_weighted_all = pd.concat([loading_weighted_all, loading_weighted], axis=0).reset_index(drop=True)\n",
    "        loading_unweighted_all = pd.concat([loading_unweighted_all, loading_unweighted], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    loading_weighted_all.to_csv('./fire_features_3/'+'ClippedFires'+str(years[jj])+'_Daily_LOADING_Weighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    loading_unweighted_all.to_csv('./fire_features_3/'+'ClippedFires'+str(years[jj])+'_Daily_LOADING_Unweighted_'+str(start_time)+'Z_day_start.csv') #daily averages\"\"\"#NOAA Soil Moisture\n",
    "        #smops = smops_timeseries(df_fire,start_time, False) #UNWEIGHTED\n",
    "        #smops = smops_timeseries(df_fire,start_time, True) #WEIGHTED\n",
    "        #smops = pd.concat([smops, pd.DataFrame({'irwinID':[fireID]*len(smops)})], axis=1)\n",
    "        #print(smops_weighted)\n",
    "        #smops_all = pd.concat([smops_all, smops], axis=0).reset_index(drop=True)\n",
    "        \n",
    "        #RAVE (always unweighted SUM)\n",
    "        \"\"\"rave = rave_timeseries(df_fire, start_time, 24, True)\n",
    "        rave = pd.concat([rave, pd.DataFrame({'irwinID':[fireID]*len(rave)})], axis=1)\n",
    "        print(rave)\n",
    "        rave_all = pd.concat([rave_all, rave], axis=0).reset_index(drop=True)\"\"\"\n",
    "        \n",
    "        #HRRR HWP\n",
    "        \"\"\"hwp_weighted, hwp_unweighted = hwp_timeseries(df_fire, start_time)\n",
    "        hwp_weighted = pd.concat([hwp_weighted, pd.DataFrame({'irwinID':[fireID]*len(hwp_weighted)})], axis=1)\n",
    "        hwp_unweighted = pd.concat([hwp_unweighted, pd.DataFrame({'irwinID':[fireID]*len(hwp_unweighted)})], axis=1)\n",
    "        print(hwp_weighted)\n",
    "        print(hwp_unweighted)\n",
    "        hwp_weighted_all = pd.concat([hwp_weighted_all, hwp_weighted], axis=0).reset_index(drop=True)\n",
    "        hwp_unweighted_all = pd.concat([hwp_unweighted_all, hwp_unweighted], axis=0).reset_index(drop=True)\"\"\"\n",
    "        \n",
    "        #HRRR HDW\n",
    "        hdw_weighted, hdw_unweighted = hdw_lagged_timeseries(df_fire, start_time)\n",
    "        hdw_weighted = pd.concat([hdw_weighted, pd.DataFrame({'irwinID':[fireID]*len(hdw_weighted)})], axis=1)\n",
    "        hdw_unweighted = pd.concat([hdw_unweighted, pd.DataFrame({'irwinID':[fireID]*len(hdw_unweighted)})], axis=1)\n",
    "        print(hdw_weighted)\n",
    "        print(hdw_unweighted)\n",
    "        \n",
    "        hdw_weighted_all = pd.concat([hdw_weighted_all, hdw_weighted], axis=0).reset_index(drop=True)\n",
    "        hdw_unweighted_all = pd.concat([hdw_unweighted_all, hdw_unweighted], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    hdw_weighted_all.to_csv('./fire_features_3/'+'ClippedFires'+str(years[jj])+'_Daily_HDW_Weighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    hdw_unweighted_all.to_csv('./fire_features_3/'+'ClippedFires'+str(years[jj])+'_Daily_HDW_Unweighted_'+str(start_time)+'Z_day_start.csv') #daily average        \n",
    "    \n",
    "    print(ii)\n",
    "        fireID=irwinIDs[ii]\n",
    "        print(fireID)\n",
    "        df_fire = fire_daily[fire_daily['irwinID']==fireID] #this is what gets fed to the feature selection code\n",
    "        \n",
    "        #maybe we filter 12Z Start dates to where we have data?\n",
    "        days=np.array(df_fire['12Z Start Day'].values, dtype='datetime64')\n",
    "        df_fire = df_fire[(days>=np.datetime64(str(years[jj])+'-07-01'))&\n",
    "                              (days<=np.datetime64(str(years[jj]+1)+'-01-01'))].reset_index(drop=True)\n",
    "        #df_fire.to_crs('EPSG:4326').plot()\n",
    "        #HRRR HDW\n",
    "        imerg_weighted, imerg_unweighted = imerg_fwi_timeseries(df_fire, start_time)\n",
    "        imerg_weighted = pd.concat([imerg_weighted, pd.DataFrame({'irwinID':[fireID]*len(imerg_weighted)})], axis=1)\n",
    "        imerg_unweighted = pd.concat([imerg_unweighted, pd.DataFrame({'irwinID':[fireID]*len(imerg_unweighted)})], axis=1)\n",
    "        print(imerg_weighted)\n",
    "        print(imerg_unweighted)\n",
    "        \n",
    "        imerg_weighted_all = pd.concat([imerg_weighted_all, imerg_weighted], axis=0).reset_index(drop=True)\n",
    "        imerg_unweighted_all = pd.concat([imerg_unweighted_all, imerg_unweighted], axis=0).reset_index(drop=True)\n",
    "        #PWS\n",
    "        pws_weighted, pws_unweighted = pws_timeseries(df_fire, start_time)\n",
    "        pws_weighted = pd.concat([pws_weighted, pd.DataFrame({'irwinID':[fireID]*len(pws_weighted)})], axis=1)\n",
    "        pws_unweighted = pd.concat([pws_unweighted, pd.DataFrame({'irwinID':[fireID]*len(pws_unweighted)})], axis=1)\n",
    "        print(pws_weighted)\n",
    "        print(pws_unweighted)\n",
    "        \n",
    "        pws_weighted_all = pd.concat([pws_weighted_all, pws_weighted], axis=0).reset_index(drop=True)\n",
    "        pws_unweighted_all = pd.concat([pws_unweighted_all, pws_unweighted], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    pws_weighted_all.to_csv('./fire_features_3/'+'ClippedFires'+str(years[jj])+'_Daily_PWS_Weighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    pws_unweighted_all.to_csv('./fire_features_3/'+'ClippedFires'+str(years[jj])+'_Daily_PWS_Unweighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    \n",
    "    #print(imerg_weighted_all)\n",
    "    #imerg_weighted_all.to_csv('./fire_features_3/'+'ClippedFires'+str(years[jj])+'_Daily_IMERG_FWI_Weighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    #imerg_unweighted_all.to_csv('./fire_features_3/'+'ClippedFires'+str(years[jj])+'_Daily_IMERG_FWI_Unweighted_'+str(start_time)+'Z_day_start.csv') #daily average        \n",
    "    #print(rave_all)\n",
    "    #rave_all.to_csv('./fire_features_3/'+'ClippedFires'+str(years[jj])+'_Daily_RAVE_Unweighted_'+str(start_time)+'Z_day_start.csv') #daily sums\n",
    "    #smops_all.to_csv('./fire_features_3/'+'ClippedFires'+str(years[jj])+'_Daily_SMOPS_Unweighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    #hwp_weighted_all.to_csv('./fire_features_3/'+'ClippedFires'+str(years[jj])+'_Daily_HWP_Weighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    #hwp_unweighted_all.to_csv('./fire_features_3/'+'ClippedFires'+str(years[jj])+'_Daily_HWP_Unweighted_'+str(start_time)+'Z_day_start.csv') #daily averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395f0cab",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Call other feature extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6117759",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    " #rave\n",
    "    \"\"\"rave_weighted = rave_timeseries(fire_daily, start_time, 24, True)\n",
    "    print(rave_weighted)\n",
    "    rave_weighted.to_csv('./fire_features_3/'+fire_name+'_Daily_RAVE_Weighted_'+str(start_time)+'Z_day_start.csv')\n",
    "    \n",
    "    rave_unweighted = rave_timeseries(fire_daily, start_time, 24, False)\n",
    "    print(rave_unweighted)\n",
    "    rave_unweighted.to_csv('./fire_features_3/'+fire_name+'_Daily_RAVE_Unweighted_'+str(start_time)+'Z_day_start.csv')\n",
    "    \"\"\"\n",
    "    \n",
    "    #smops\n",
    "    \"\"\"smops_unweighted=smops_timeseries(fire_daily,start_time, False)\n",
    "    print(smops_unweighted)\n",
    "    smops_unweighted.to_csv('./fire_features_3/'+fire_name+'_Daily_SMOPS_Unweighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #pws\n",
    "    \"\"\"pws_weighted=pws_timeseries(fire_daily,start_time, True)\n",
    "    print(pws_weighted)\n",
    "    pws_weighted.to_csv('./fire_features_3/'+fire_name+'_Daily_PWS_Weighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    \n",
    "    pws_unweighted=pws_timeseries(fire_daily,start_time, False)\n",
    "    print(pws_unweighted)\n",
    "    pws_unweighted.to_csv('./fire_features_3/'+fire_name+'_Daily_PWS_Unweighted_'+str(start_time)+'Z_day_start.csv') #daily averages\"\"\"\n",
    "    \n",
    "    \n",
    "    #imerg \n",
    "    \"\"\"imerg_weighted = imerg_fwi_timeseries(fire_daily, start_time, True)\n",
    "    print(imerg_weighted)\n",
    "    imerg_weighted.to_csv('./fire_features_3/'+fire_name+'_Daily_FWI_IMERG_Weighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    \n",
    "    imerg_unweighted = imerg_fwi_timeseries(fire_daily, start_time, False)\n",
    "    print(imerg_unweighted)\n",
    "    imerg_unweighted.to_csv('./fire_features_3/'+fire_name+'_Daily_FWI_IMERG_Unweighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    \"\"\"\n",
    "    \n",
    "    #fuel loading (1km)\n",
    "    \"\"\"fuel_weighted = fuel_loading_timeseries(fire_daily, start_time, True)\n",
    "    print(fuel_weighted)\n",
    "    fuel_weighted.to_csv('./fire_features_3/'+fire_name+'_Daily_FUEL_LOADING_Weighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    \n",
    "    fuel_unweighted = fuel_loading_timeseries(fire_daily, start_time, False)\n",
    "    print(fuel_unweighted)\n",
    "    fuel_unweighted.to_csv('./fire_features_3/'+fire_name+'_Daily_FUEL_LOADING_Unweighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    \"\"\"\n",
    "    #esi\n",
    "    \"\"\"esi_weighted = esi_timeseries(fire_daily, start_time, True)\n",
    "    print(esi_weighted)\n",
    "    esi_weighted.to_csv('./fire_features_3/'+fire_name+'_Daily_ESI_Weighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    \n",
    "    esi_unweighted = esi_timeseries(fire_daily, start_time, False)\n",
    "    print(esi_unweighted)\n",
    "    esi_unweighted.to_csv('./fire_features_3/'+fire_name+'_Daily_ESI_Unweighted_'+str(start_time)+'Z_day_start.csv') #daily averages\"\"\"\n",
    "    \n",
    "    #HRRR\n",
    "    #hrrr_weighted = hrrr_timeseries(fire_daily, start_time, True)\n",
    "    #print(hrrr_weighted)\n",
    "    #hrrr_weighted.to_csv('./fire_features_3/'+fire_name+'_Daily_HRRR_Weighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    \n",
    "    #HDW with lags\n",
    "    \"\"\"hdw = hdw_timeseries(fire_daily, start_time, True)\n",
    "    print(hdw)\n",
    "    hdw.to_csv('./fire_features_3/'+fire_name+'_Daily_Lagged_HDW_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    hrrr_unweighted = hrrr_timeseries(fire_daily, start_time, False)\n",
    "    print(hrrr_unweighted)\n",
    "    hrrr_unweighted.to_csv('./fire_features_3/'+fire_name+'_Daily_HRRR_Unweighted_'+str(start_time)+'Z_day_start.csv') #daily averages\n",
    "    \"\"\"\n",
    "    \n",
    "    #sit_209\n",
    "    \n",
    "    \"\"\" sit209 = resources_timeseries(fire_daily, start_time)\n",
    "    print(sit209)\n",
    "    sit209.to_csv('./fire_features_3/'+fire_name+'_Daily_RESOURCES_'+str(start_time)+'Z_day_start.csv') #daily averages\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b334beef",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Fill in Missing Data\n",
    "1. 11/24 in the Rave Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a63e1b9",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = xr.open_dataset('/data2/lthapa/2020/RAVE/11/RAVE-HrlyEmiss-3km-CONUS_v1r1_blend_s20201125.nc')\n",
    "\n",
    "times = data.time.values\n",
    "times_shifted = times-np.timedelta64(1,'D')\n",
    "data['time'] = times_shifted\n",
    "data.assign_coords({'time':times_shifted})\n",
    "print(data.time.values)\n",
    "#print(data.variables)\n",
    "varis = ['FRP_MEAN', 'FRP_SD', 'FRE', 'CO2', 'CO', 'SO2', 'OC','BC', 'PM25', 'NOx', 'NH3','TPM', 'VOCs', 'CH4']\n",
    "for var in varis:\n",
    "    data[var].where(data[var] != np.nan)\n",
    "    \n",
    "#print(data.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7174caf3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad872cfe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.to_netcdf('/data2/lthapa/2020/RAVE/11/RAVE-HrlyEmiss-3km-CONUS_v1r1_blend_s20201124.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba5e60",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "da = xr.DataArray(np.arange(16).reshape(4, 4), dims=[\"x\", \"y\"])\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6afb6d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "da.where(da.x + da.y < 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba67e65",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,15)) \n",
    "rave.plot(x='day', y=['FRP_MEAN'],\n",
    "                                figsize=(20,6),style='ro-',ax=ax )\n",
    "h1, l1 = ax.get_legend_handles_labels()\n",
    "ax.legend(h1, l1,bbox_to_anchor=(1.1, 1), loc='upper left')\n",
    "plt.title('LAKE FRP')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d238b584",
   "metadata": {},
   "source": [
    "## Dataset-Dependent Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfd17b9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### SIT-209 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d81db2b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def resources_timeseries(df,day_start_hour, sit209_data):\n",
    "    #sit209_data = pd.read_csv('../merged_sit.csv')\n",
    "    \n",
    "    \n",
    "    #get the fire incident number, lat, and lon\n",
    "    incident_number = df['irwinID'].iloc[0]\n",
    "    fire_lat = df['Lat Fire'].iloc[0]\n",
    "    fire_lon = df['Lon Fire'].iloc[0]\n",
    "    #print(incident_number, fire_lat, fire_lon)\n",
    "    \n",
    "    sit209_data_fire = sit209_data[sit209_data['IRWIN_IDENTIFIER']==incident_number]\n",
    "    print(sit209_data_fire)\n",
    "    #print(sit209_data_fire.columns.values)\n",
    "    #do the time zone conversion\n",
    "    obj=TimezoneFinder() #initialize the timezone finder\n",
    "    tz = obj.timezone_at(lng=fire_lon, lat=fire_lat) #get the timezone\n",
    "    local = pytz.timezone(tz)\n",
    "    utc = pytz.utc\n",
    "    \n",
    "    #put the start and end times in local time\n",
    "    loc_dt_start = [local.localize(datetime.datetime.strptime(date, '%m/%d/%Y %H:%M:%S')) for date in sit209_data_fire['REPORT_FROM_DATE'].values]\n",
    "    loc_dt_end = [local.localize(datetime.datetime.strptime(date, '%m/%d/%Y %H:%M:%S')) for date in sit209_data_fire['REPORT_TO_DATE'].values]\n",
    "    \n",
    "    #put them in UTC time\n",
    "    utc_dt_start = [time_start.astimezone(utc) for time_start in loc_dt_start]\n",
    "    utc_dt_end = [time_end.astimezone(utc) for time_end in loc_dt_end]\n",
    "    \n",
    "    start_day = pd.to_datetime(utc_dt_start[0]).strftime('%Y-%m-%d')+' '+str(day_start_hour)+':00'\n",
    "    \n",
    "    \n",
    "    #reassign to UTC time, this DOES keep track of daylight savings (eg +7 is used for PDT, +8 is used for PST)\n",
    "    sit209_data_fire['Report Start UTC'] = pd.to_datetime(utc_dt_start)\n",
    "    sit209_data_fire['Report End UTC'] = pd.to_datetime(utc_dt_end)\n",
    "    sit209_data_fire['Timezone']= tz\n",
    "    \n",
    "    #localise the index\n",
    "    sit209_data_fire = sit209_data_fire.set_index(['Report Start UTC']).tz_localize(None)\n",
    "    #print(sit209_data_fire.iloc[0:4])\n",
    "    \n",
    "    \n",
    "    ## do the 12z-12z day grouping, based on the UTC times\n",
    "    #start_day_utc = str(utc_dt_start[0])\n",
    "    start_day_utc=str(df[str(day_start_hour)+'Z Start Day'][0])\n",
    "    start_datetime_utc = np.datetime64(start_day_utc[0:10]+'T'+str(day_start_hour).zfill(2)+':00')\n",
    "    print(start_datetime_utc)\n",
    "    #sit209_data_fire = sit209_data_fire.resample('24H',origin=start_datetime_utc)\n",
    "\n",
    "    #personnel = sit209_data_fire['RESOURCE_PERSONNEL'].resample('24H',origin=start_datetime_utc).sum().reset_index()\n",
    "    percent_contained = sit209_data_fire['PCT_CONTAINED_COMPLETED'].resample('24H',origin=start_datetime_utc).mean().reset_index()\n",
    "    df_sit209 = pd.concat([percent_contained],axis=1)\n",
    "    df_sit209.columns=['day', 'percent_contained']\n",
    "    df_sit209['day'] = pd.to_datetime(df_sit209['day'].values).strftime('%Y-%m-%d')\n",
    "    df_sit209=df_sit209.fillna(method='ffill')\n",
    "    #df_sit209['day'].iloc[:] = pd.to_datetime(df[str(day_start_hour)+ 'Z Start Day'].values)\n",
    "\n",
    "    #inds = df_sit209['day'].isin(df[str(day_start_hour)+'Z Start Day']).values\n",
    "    \n",
    "    return df_sit209\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41439e5d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### HRRR HWP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "246c50c1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#HRRR_WS formulation from, take time mean, then take weighted average. For HDW, multiply the weighted means of VPD and WIND\n",
    "def hwp_timeseries(df,day_start_hour):  #with the wind speed\n",
    "    varis_hrrr_derived = ['day','hwp'] #'hd1w0','hd2w0', 'hd3w0', 'hd4w0', 'hd5w0',\n",
    "    \n",
    "    #return both!\n",
    "    df_hwp_weighted = generate_df(varis_hrrr_derived, len(df))\n",
    "    df_hwp_unweighted = generate_df(varis_hrrr_derived, len(df))\n",
    "\n",
    "    #do the intersection, in parallel\n",
    "    #print(tic)\n",
    "    hrrr_intersections = Parallel(n_jobs=8)(delayed(calculate_intersection)\n",
    "                                 (df.iloc[ii:ii+1],'HRRR_GRID',3000) \n",
    "                                 for ii in range(len(df)))\n",
    "    print([hrrr_intersections[jj]['weights'].sum() for jj in range(len(hrrr_intersections))])\n",
    "\n",
    "    fire_hrrr_intersection=gpd.GeoDataFrame(pd.concat(hrrr_intersections, ignore_index=True))\n",
    "    fire_hrrr_intersection.set_geometry(col='geometry')\n",
    "    \n",
    "    \n",
    "    #loop over all of the days we have intersections\n",
    "    times_intersect = np.unique(fire_hrrr_intersection[str(day_start_hour)+ 'Z Start Day'].values)\n",
    "    times_utc = np.unique(fire_hrrr_intersection['UTC Day'].values)\n",
    "    \n",
    "    count = 0\n",
    "    for today in times_intersect:\n",
    "        print(today)\n",
    "        #get the time\n",
    "        df_sub = fire_hrrr_intersection.iloc[np.where(fire_hrrr_intersection[str(day_start_hour)+ 'Z Start Day'].values==today)]\n",
    "        df_sub = df_sub.set_index([str(day_start_hour)+ 'Z Start Day', 'row', 'col'])\n",
    "        df_sub=df_sub[~df_sub.index.duplicated()]\n",
    "        intersection_sub = df_sub.to_xarray() #polygon and weights for today\n",
    "        intersection_sub['weights_mask'] =xr.where(intersection_sub['weights']>0,1, np.nan)\n",
    "        \n",
    "        times_back = pd.date_range(start=np.datetime64(today)-np.timedelta64(1,'D'), end=np.datetime64(today)+\n",
    "                                   np.timedelta64(1,'D'),freq='H')\n",
    "        #print(times_back)\n",
    "        files_back,times_back_used = make_file_namelist(times_back,'/data2/lthapa/ML_daily/pygraf/processed_hrrr_hdw_hwp/Processed_HRRR_YYYYMMDDHH_HDW_HWP.nc')\n",
    "        #load in all the merra files associated with this lookback window\n",
    "        dat_hrrr = xr.open_mfdataset(files_back,concat_dim='time',combine='nested',compat='override', coords='all')\n",
    "        dat_hrrr = dat_hrrr.assign_coords({'time': times_back_used})\n",
    "        \n",
    "        dat_hrrr['hd0w0'] = dat_hrrr['wind_speed']*dat_hrrr['vpd_2m']\n",
    "        #print(dat_hrrr)\n",
    "        \n",
    "        hrrr_daily_mean = dat_hrrr.resample(time='24H',base=day_start_hour, label='left').mean(dim='time') #take the daily mean        \n",
    "        \n",
    "        hrrr_daily_mean_region = hrrr_daily_mean.sel(grid_yt = np.unique(intersection_sub['row'].values),\n",
    "                                                    grid_xt = np.unique(intersection_sub['col'].values)) #get the location of the overlaps\n",
    "    \n",
    "        df_hwp_weighted['day'].iloc[count] =today# pd.to_datetime([str(day_start_hour)+ 'Z Start Day'].values[count])            df_hrrr_derived.loc[count, ('hd0w0')] = np.nansum((hrrr_daily_mean_region['hd0w0'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))\n",
    "        df_hwp_unweighted['day'].iloc[count] =today# pd.to_datetime([str(day_start_hour)+ 'Z Start Day'].values[count])            df_hrrr_derived.loc[count, ('hd0w0')] = np.nansum((hrrr_daily_mean_region['hd0w0'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))\n",
    "\n",
    "        df_hwp_weighted.loc[count, ('hwp')] =np.nansum((hrrr_daily_mean_region['hwp'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))   \n",
    "        df_hwp_unweighted.loc[count, ('hwp')] = np.nanmean((hrrr_daily_mean_region['hwp'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights_mask'].values))\n",
    "\n",
    "        dat_hrrr.close()\n",
    "        count =count+1\n",
    "        #print(df_hrrr_derived)\n",
    "    return df_hwp_weighted, df_hwp_unweighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6455ac",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### HRRR HDW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a2496d01",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#HRRR_WS formulation from, take time mean, then take weighted average. For HDW, multiply the weighted means of VPD and WIND\n",
    "def hdw_lagged_timeseries(df,day_start_hour):  #with the wind speed\n",
    "    varis_hrrr_derived = ['day','hd0w0', 'hd1w0','hd2w0', 'hd3w0']#, 'hd4w0', 'hd5w0',\n",
    "    df_hdw_weighted = generate_df(varis_hrrr_derived, len(df))\n",
    "    df_hdw_unweighted = generate_df(varis_hrrr_derived, len(df))\n",
    "    \n",
    "    #do the intersection, in parallel\n",
    "    #print(tic)\n",
    "    hrrr_intersections = Parallel(n_jobs=8)(delayed(calculate_intersection)\n",
    "                                 (df.iloc[ii:ii+1],'HRRR_GRID',3000) \n",
    "                                 for ii in range(len(df)))\n",
    "    \n",
    "    fire_hrrr_intersection=gpd.GeoDataFrame(pd.concat(hrrr_intersections, ignore_index=True))\n",
    "    fire_hrrr_intersection.set_geometry(col='geometry')\n",
    "    \n",
    "    \n",
    "    #loop over all of the days we have intersections\n",
    "    times_intersect = np.unique(fire_hrrr_intersection[str(day_start_hour)+ 'Z Start Day'].values)\n",
    "    times_utc = np.unique(fire_hrrr_intersection['UTC Day'].values)\n",
    "    \n",
    "    count = 0\n",
    "    for today in times_intersect:\n",
    "        print(today)\n",
    "        #get the time\n",
    "        df_sub = fire_hrrr_intersection.iloc[np.where(fire_hrrr_intersection[str(day_start_hour)+ 'Z Start Day'].values==today)]\n",
    "        df_sub = df_sub.set_index([str(day_start_hour)+ 'Z Start Day', 'row', 'col'])\n",
    "        df_sub=df_sub[~df_sub.index.duplicated()]\n",
    "        intersection_sub = df_sub.to_xarray() #polygon and weights for today\n",
    "        intersection_sub['weights_mask'] =xr.where(intersection_sub['weights']>0,1, np.nan)\n",
    "        \n",
    "        times_back = pd.date_range(start=np.datetime64(today)-np.timedelta64(3,'D'), end=np.datetime64(today)+\n",
    "                                   np.timedelta64(36,'h'),freq='H')\n",
    "        files_back,times_back_used = make_file_namelist(times_back,'/data2/lthapa/ML_daily/pygraf/processed_hrrr_hdw_hwp/Processed_HRRR_YYYYMMDDHH_HDW_HWP.nc')\n",
    "        #load in all the merra files associated with this lookback window\n",
    "        dat_hrrr = xr.open_mfdataset(files_back,concat_dim='time',combine='nested',compat='override', coords='all')\n",
    "        dat_hrrr = dat_hrrr.assign_coords({'time': times_back_used})\n",
    "        dat_hrrr = dat_hrrr.resample(time='h').asfreq()\n",
    "        #print(dat_hrrr['time'].values)\n",
    "        #define the days\n",
    "        day0= np.datetime64(today)+np.timedelta64(12,'h')\n",
    "        day1 =day0-np.timedelta64(1,'D')\n",
    "        day2 =day0-np.timedelta64(2,'D')\n",
    "        day3 =day0-np.timedelta64(3,'D')\n",
    "\n",
    "        #define the times we will select for VPD\n",
    "        times_0 = pd.date_range(start=day0, end=day0+np.timedelta64(23,'h'),freq='H')\n",
    "        times_1 = pd.date_range(start=day1, end=day1+np.timedelta64(23,'h'),freq='H')\n",
    "        times_2 = pd.date_range(start=day2, end=day2+np.timedelta64(23,'h'),freq='H')\n",
    "        times_3 = pd.date_range(start=day3, end=day3+np.timedelta64(23,'h'),freq='H')\n",
    "        \n",
    "        w0 = dat_hrrr['wind_speed'].sel(time=times_0, grid_yt = np.unique(intersection_sub['row'].values),grid_xt = np.unique(intersection_sub['col'].values))\n",
    "        hd0 = dat_hrrr['vpd_2m'].sel(time=times_0, grid_yt = np.unique(intersection_sub['row'].values),grid_xt = np.unique(intersection_sub['col'].values))\n",
    "        hd1 = dat_hrrr['vpd_2m'].sel(time=times_1, grid_yt = np.unique(intersection_sub['row'].values),grid_xt = np.unique(intersection_sub['col'].values))\n",
    "        hd1=hd1.assign_coords({'time':w0['time'].values})\n",
    "        hd2 = dat_hrrr['vpd_2m'].sel(time=times_2, grid_yt = np.unique(intersection_sub['row'].values),grid_xt = np.unique(intersection_sub['col'].values))\n",
    "        hd2=hd2.assign_coords({'time':w0['time'].values})\n",
    "        hd3 = dat_hrrr['vpd_2m'].sel(time=times_3, grid_yt = np.unique(intersection_sub['row'].values),grid_xt = np.unique(intersection_sub['col'].values))\n",
    "        hd3=hd3.assign_coords({'time':w0['time'].values})\n",
    "            \n",
    "        hd0w0 = hd0*w0\n",
    "        hd1w0 = hd1*w0\n",
    "        hd2w0 = hd2*w0\n",
    "        hd3w0 = hd3*w0\n",
    "\n",
    "        hd0w0_daily_mean = hd0w0.resample(time='24H',base=day_start_hour, label='left').mean(dim='time') #take the daily mean        \n",
    "        hd1w0_daily_mean = hd1w0.resample(time='24H',base=day_start_hour, label='left').mean(dim='time') #take the daily mean        \n",
    "        hd2w0_daily_mean = hd2w0.resample(time='24H',base=day_start_hour, label='left').mean(dim='time') #take the daily mean        \n",
    "        hd3w0_daily_mean = hd3w0.resample(time='24H',base=day_start_hour, label='left').mean(dim='time') #take the daily mean        \n",
    "\n",
    "        \n",
    "        df_hdw_weighted['day'].iloc[count] =today# pd.to_datetime([str(day_start_hour)+ 'Z Start Day'].values[count])\n",
    "        df_hdw_unweighted['day'].iloc[count] =today# pd.to_datetime([str(day_start_hour)+ 'Z Start Day'].values[count])\n",
    "\n",
    "        #WEIGHTED\n",
    "        df_hdw_weighted.loc[count, ('hd0w0')] = np.nansum((hd0w0_daily_mean.values)*(intersection_sub['weights'].values))\n",
    "        df_hdw_weighted.loc[count, ('hd1w0')] = np.nansum((hd1w0_daily_mean.values)*(intersection_sub['weights'].values))\n",
    "        df_hdw_weighted.loc[count, ('hd2w0')] = np.nansum((hd2w0_daily_mean.values)*(intersection_sub['weights'].values))\n",
    "        df_hdw_weighted.loc[count, ('hd3w0')] = np.nansum((hd3w0_daily_mean.values)*(intersection_sub['weights'].values))\n",
    "\n",
    "        #UNWEIGHTED\n",
    "        df_hdw_unweighted.loc[count, ('hd0w0')] = np.nanmean((hd0w0_daily_mean.values))\n",
    "        df_hdw_unweighted.loc[count, ('hd1w0')] = np.nanmean((hd1w0_daily_mean.values))\n",
    "        df_hdw_unweighted.loc[count, ('hd2w0')] = np.nanmean((hd2w0_daily_mean.values))\n",
    "        df_hdw_unweighted.loc[count, ('hd3w0')] = np.nanmean((hd3w0_daily_mean.values))\n",
    "        \n",
    "        count=count+1\n",
    "    return df_hdw_weighted, df_hdw_unweighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d59b1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### HRRR HDW and HWP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4a93ddd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#HRRR_WS formulation from, take time mean, then take weighted average. For HDW, multiply the weighted means of VPD and WIND\n",
    "def hdw_timeseries(df,day_start_hour, use_weights):  #with the wind speed\n",
    "    varis_hrrr_derived = ['day','hd0w0', 'hd1w0','hd2w0', 'hd3w0']#, 'hd4w0', 'hd5w0',\n",
    "    df_hrrr_derived = generate_df(varis_hrrr_derived, len(df))\n",
    "    \n",
    "    #do the intersection, in parallel\n",
    "    #print(tic)\n",
    "    hrrr_intersections = Parallel(n_jobs=8)(delayed(calculate_intersection)\n",
    "                                 (df.iloc[ii:ii+1],'HRRR_GRID',3000) \n",
    "                                 for ii in range(len(df)))\n",
    "    \n",
    "    fire_hrrr_intersection=gpd.GeoDataFrame(pd.concat(hrrr_intersections, ignore_index=True))\n",
    "    fire_hrrr_intersection.set_geometry(col='geometry')\n",
    "    \n",
    "    \n",
    "    #loop over all of the days we have intersections\n",
    "    times_intersect = np.unique(fire_hrrr_intersection[str(day_start_hour)+ 'Z Start Day'].values)\n",
    "    times_utc = np.unique(fire_hrrr_intersection['UTC Day'].values)\n",
    "    \n",
    "    count = 0\n",
    "    for today in times_intersect:\n",
    "        print(today)\n",
    "        #get the time\n",
    "        df_sub = fire_hrrr_intersection.iloc[np.where(fire_hrrr_intersection[str(day_start_hour)+ 'Z Start Day'].values==today)]\n",
    "        df_sub = df_sub.set_index([str(day_start_hour)+ 'Z Start Day', 'row', 'col'])\n",
    "        df_sub=df_sub[~df_sub.index.duplicated()]\n",
    "        intersection_sub = df_sub.to_xarray() #polygon and weights for today\n",
    "        intersection_sub['weights_mask'] =xr.where(intersection_sub['weights']>0,1, np.nan)\n",
    "        \n",
    "        times_back = pd.date_range(start=np.datetime64(today)-np.timedelta64(3,'D'), end=np.datetime64(today)+\n",
    "                                   np.timedelta64(36,'h'),freq='H')\n",
    "        files_back,times_back_used = make_file_namelist(times_back,'/data2/lthapa/ML_daily/pygraf/processed_hrrr_hdw_hwp/Processed_HRRR_YYYYMMDDHH_HDW_HWP.nc')\n",
    "        #load in all the merra files associated with this lookback window\n",
    "        dat_hrrr = xr.open_mfdataset(files_back,concat_dim='time',combine='nested',compat='override', coords='all')\n",
    "        dat_hrrr = dat_hrrr.assign_coords({'time': times_back_used})\n",
    "        dat_hrrr = dat_hrrr.resample(time='h').asfreq()\n",
    "        print(dat_hrrr['time'].values)\n",
    "        #define the days\n",
    "        day0= np.datetime64(today)+np.timedelta64(12,'h')\n",
    "        day1 =day0-np.timedelta64(1,'D')\n",
    "        day2 =day0-np.timedelta64(2,'D')\n",
    "        day3 =day0-np.timedelta64(3,'D')\n",
    "\n",
    "        #define the times we will select for VPD\n",
    "        times_0 = pd.date_range(start=day0, end=day0+np.timedelta64(23,'h'),freq='H')\n",
    "        print(times_0)\n",
    "        times_1 = pd.date_range(start=day1, end=day1+np.timedelta64(23,'h'),freq='H')\n",
    "        times_2 = pd.date_range(start=day2, end=day2+np.timedelta64(23,'h'),freq='H')\n",
    "        times_3 = pd.date_range(start=day3, end=day3+np.timedelta64(23,'h'),freq='H')\n",
    "        \n",
    "        w0 = dat_hrrr['wind_speed'].sel(time=times_0, grid_yt = np.unique(intersection_sub['row'].values),grid_xt = np.unique(intersection_sub['col'].values))\n",
    "        hd0 = dat_hrrr['vpd_2m'].sel(time=times_0, grid_yt = np.unique(intersection_sub['row'].values),grid_xt = np.unique(intersection_sub['col'].values))\n",
    "        hd1 = dat_hrrr['vpd_2m'].sel(time=times_1, grid_yt = np.unique(intersection_sub['row'].values),grid_xt = np.unique(intersection_sub['col'].values))\n",
    "        hd1=hd1.assign_coords({'time':w0['time'].values})\n",
    "        hd2 = dat_hrrr['vpd_2m'].sel(time=times_2, grid_yt = np.unique(intersection_sub['row'].values),grid_xt = np.unique(intersection_sub['col'].values))\n",
    "        hd2=hd2.assign_coords({'time':w0['time'].values})\n",
    "        hd3 = dat_hrrr['vpd_2m'].sel(time=times_3, grid_yt = np.unique(intersection_sub['row'].values),grid_xt = np.unique(intersection_sub['col'].values))\n",
    "        hd3=hd3.assign_coords({'time':w0['time'].values})\n",
    "            \n",
    "        hd0w0 = hd0*w0\n",
    "        hd1w0 = hd1*w0\n",
    "        hd2w0 = hd2*w0\n",
    "        hd3w0 = hd3*w0\n",
    "\n",
    "        hd0w0_daily_mean = hd0w0.resample(time='24H',base=day_start_hour, label='left').mean(dim='time') #take the daily mean        \n",
    "        hd1w0_daily_mean = hd1w0.resample(time='24H',base=day_start_hour, label='left').mean(dim='time') #take the daily mean        \n",
    "        hd2w0_daily_mean = hd2w0.resample(time='24H',base=day_start_hour, label='left').mean(dim='time') #take the daily mean        \n",
    "        hd3w0_daily_mean = hd3w0.resample(time='24H',base=day_start_hour, label='left').mean(dim='time') #take the daily mean        \n",
    "\n",
    "        \n",
    "        df_hrrr_derived['day'].iloc[count] =today# pd.to_datetime([str(day_start_hour)+ 'Z Start Day'].values[count])\n",
    "        if use_weights==True:\n",
    "            df_hrrr_derived.loc[count, ('hd0w0')] = np.nansum((hd0w0_daily_mean.values)*(intersection_sub['weights'].values))\n",
    "            df_hrrr_derived.loc[count, ('hd1w0')] = np.nansum((hd1w0_daily_mean.values)*(intersection_sub['weights'].values))\n",
    "            df_hrrr_derived.loc[count, ('hd2w0')] = np.nansum((hd2w0_daily_mean.values)*(intersection_sub['weights'].values))\n",
    "            df_hrrr_derived.loc[count, ('hd3w0')] = np.nansum((hd3w0_daily_mean.values)*(intersection_sub['weights'].values))\n",
    "\n",
    "        else:\n",
    "            df_hrrr_derived.loc[count, ('hd0w0')] = np.nanmean((hd0w0_daily_mean.values))\n",
    "            df_hrrr_derived.loc[count, ('hd1w0')] = np.nanmean((hd1w0_daily_mean.values))\n",
    "            df_hrrr_derived.loc[count, ('hd2w0')] = np.nanmean((hd2w0_daily_mean.values))\n",
    "            df_hrrr_derived.loc[count, ('hd3w0')] = np.nanmean((hd3w0_daily_mean.values))        \n",
    "        print(df_hrrr_derived)\n",
    "        count=count+1\n",
    "    return df_hrrr_derived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6795641b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" hd0w0 = dat_hrrr['wind_speed'].sel(time=times_0)*dat_hrrr['vpd_2m'].sel(time=times_0)\\n\\n        hd0w0_daily_mean= hd0w0.sel(grid_yt = np.unique(intersection_sub['row'].values),\\n                                           grid_xt = np.unique(intersection_sub['col'].values)).resample(time='24H',\\n                                                                                      base=day_start_hour, \\n                                                                                      label='left').mean(dim='time')\\n\\n        \\n        #print(dat_hrrr.sel(time=times_0))\\n        \\n        \\n        dat_hrrr['hd0w0'] = dat_hrrr['wind_speed']*dat_hrrr['vpd_2m']\\n        #print(dat_hrrr)\\n        \\n        hrrr_daily_mean = dat_hrrr.resample(time='24H',base=day_start_hour, label='left').mean(dim='time') #take the daily mean        \\n        \\n        hrrr_daily_mean_region = hrrr_daily_mean.sel(grid_yt = np.unique(intersection_sub['row'].values),\\n                                                    grid_xt = np.unique(intersection_sub['col'].values)) #get the location of the overlaps\\n    \\n        df_hrrr_derived['day'].iloc[count] =today# pd.to_datetime([str(day_start_hour)+ 'Z Start Day'].values[count])\\n        #TO DO: ADD IF STATEMENT FOR WEIGHTED OR UNWEIGHTED\\n        if use_weights==True:\\n            df_hrrr_derived.loc[count, ('hd0w0')] = np.nansum((hrrr_daily_mean_region['hd0w0'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))\\n            df_hrrr_derived.loc[count, ('hwp')] =np.nansum((hrrr_daily_mean_region['hwp'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))   \\n        else:\\n            df_hrrr_derived['hd0w0'].iloc[count] = np.nanmean((hrrr_daily_mean_region['hd0w0'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights_mask'].values))\\n            df_hrrr_derived['hwp'].iloc[count] = np.nanmean((hrrr_daily_mean_region['hwp'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights_mask'].values))\\n\\n        dat_hrrr.close()\\n        count =count+1\\n        #print(df_hrrr_derived)\\n    return df_hrrr_derived\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" hd0w0 = dat_hrrr['wind_speed'].sel(time=times_0)*dat_hrrr['vpd_2m'].sel(time=times_0)\n",
    "\n",
    "        hd0w0_daily_mean= hd0w0.sel(grid_yt = np.unique(intersection_sub['row'].values),\n",
    "                                           grid_xt = np.unique(intersection_sub['col'].values)).resample(time='24H',\n",
    "                                                                                      base=day_start_hour, \n",
    "                                                                                      label='left').mean(dim='time')\n",
    "\n",
    "        \n",
    "        #print(dat_hrrr.sel(time=times_0))\n",
    "        \n",
    "        \n",
    "        dat_hrrr['hd0w0'] = dat_hrrr['wind_speed']*dat_hrrr['vpd_2m']\n",
    "        #print(dat_hrrr)\n",
    "        \n",
    "        hrrr_daily_mean = dat_hrrr.resample(time='24H',base=day_start_hour, label='left').mean(dim='time') #take the daily mean        \n",
    "        \n",
    "        hrrr_daily_mean_region = hrrr_daily_mean.sel(grid_yt = np.unique(intersection_sub['row'].values),\n",
    "                                                    grid_xt = np.unique(intersection_sub['col'].values)) #get the location of the overlaps\n",
    "    \n",
    "        #TO DO: ADD IF STATEMENT FOR WEIGHTED OR UNWEIGHTED\n",
    "        if use_weights==True:\n",
    "            df_hrrr_derived.loc[count, ('hd0w0')] = np.nansum((hrrr_daily_mean_region['hd0w0'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))\n",
    "            df_hrrr_derived.loc[count, ('hwp')] =np.nansum((hrrr_daily_mean_region['hwp'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))   \n",
    "        else:\n",
    "            df_hrrr_derived['hd0w0'].iloc[count] = np.nanmean((hrrr_daily_mean_region['hd0w0'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights_mask'].values))\n",
    "            df_hrrr_derived['hwp'].iloc[count] = np.nanmean((hrrr_daily_mean_region['hwp'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights_mask'].values))\n",
    "\n",
    "        dat_hrrr.close()\n",
    "        count =count+1\n",
    "        #print(df_hrrr_derived)\n",
    "    return df_hrrr_derived\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e561c37",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#HRRR_WS formulation from, take time mean, then take weighted average. For HDW, multiply the weighted means of VPD and WIND\n",
    "def hrrr_timeseries(df,day_start_hour, use_weights):  #with the wind speed\n",
    "    varis_hrrr_derived = ['day','hwp'] #'hd1w0','hd2w0', 'hd3w0', 'hd4w0', 'hd5w0',\n",
    "    df_hrrr_derived = generate_df(varis_hrrr_derived, len(df))\n",
    "    \n",
    "    #do the intersection, in parallel\n",
    "    #print(tic)\n",
    "    hrrr_intersections = Parallel(n_jobs=8)(delayed(calculate_intersection)\n",
    "                                 (df.iloc[ii:ii+1],'HRRR_GRID',3000) \n",
    "                                 for ii in range(len(df)))\n",
    "    \n",
    "    fire_hrrr_intersection=gpd.GeoDataFrame(pd.concat(hrrr_intersections, ignore_index=True))\n",
    "    fire_hrrr_intersection.set_geometry(col='geometry')\n",
    "    \n",
    "    \n",
    "    #loop over all of the days we have intersections\n",
    "    times_intersect = np.unique(fire_hrrr_intersection[str(day_start_hour)+ 'Z Start Day'].values)\n",
    "    times_utc = np.unique(fire_hrrr_intersection['UTC Day'].values)\n",
    "    \n",
    "    count = 0\n",
    "    for today in times_intersect:\n",
    "        print(today)\n",
    "        #get the time\n",
    "        df_sub = fire_hrrr_intersection.iloc[np.where(fire_hrrr_intersection[str(day_start_hour)+ 'Z Start Day'].values==today)]\n",
    "        df_sub = df_sub.set_index([str(day_start_hour)+ 'Z Start Day', 'row', 'col'])\n",
    "        df_sub=df_sub[~df_sub.index.duplicated()]\n",
    "        intersection_sub = df_sub.to_xarray() #polygon and weights for today\n",
    "        intersection_sub['weights_mask'] =xr.where(intersection_sub['weights']>0,1, np.nan)\n",
    "        \n",
    "        times_back = pd.date_range(start=np.datetime64(today)-np.timedelta64(1,'D'), end=np.datetime64(today)+\n",
    "                                   np.timedelta64(1,'D'),freq='H')\n",
    "        print(times_back)\n",
    "        files_back,times_back_used = make_file_namelist(times_back,'/data2/lthapa/ML_daily/pygraf/processed_hrrr_hdw_hwp/Processed_HRRR_YYYYMMDDHH_HDW_HWP.nc')\n",
    "        #load in all the merra files associated with this lookback window\n",
    "        dat_hrrr = xr.open_mfdataset(files_back,concat_dim='time',combine='nested',compat='override', coords='all')\n",
    "        dat_hrrr = dat_hrrr.assign_coords({'time': times_back_used})\n",
    "        \n",
    "        dat_hrrr['hd0w0'] = dat_hrrr['wind_speed']*dat_hrrr['vpd_2m']\n",
    "        #print(dat_hrrr)\n",
    "        \n",
    "        hrrr_daily_mean = dat_hrrr.resample(time='24H',base=day_start_hour, label='left').mean(dim='time') #take the daily mean        \n",
    "        \n",
    "        hrrr_daily_mean_region = hrrr_daily_mean.sel(grid_yt = np.unique(intersection_sub['row'].values),\n",
    "                                                    grid_xt = np.unique(intersection_sub['col'].values)) #get the location of the overlaps\n",
    "    \n",
    "        df_hrrr_derived['day'].iloc[count] =today# pd.to_datetime([str(day_start_hour)+ 'Z Start Day'].values[count])\n",
    "        #TO DO: ADD IF STATEMENT FOR WEIGHTED OR UNWEIGHTED\n",
    "        if use_weights==True:\n",
    "            df_hrrr_derived.loc[count, ('hd0w0')] = np.nansum((hrrr_daily_mean_region['hd0w0'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))\n",
    "            df_hrrr_derived.loc[count, ('hwp')] =np.nansum((hrrr_daily_mean_region['hwp'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))   \n",
    "        else:\n",
    "            df_hrrr_derived['hd0w0'].iloc[count] = np.nanmean((hrrr_daily_mean_region['hd0w0'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights_mask'].values))\n",
    "            df_hrrr_derived['hwp'].iloc[count] = np.nanmean((hrrr_daily_mean_region['hwp'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights_mask'].values))\n",
    "\n",
    "        dat_hrrr.close()\n",
    "        count =count+1\n",
    "        #print(df_hrrr_derived)\n",
    "    return df_hrrr_derived"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6dce32",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Code for calculating LAGGED HDW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daddd4c7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "        \"\"\"#print(hrrr_daily_mean_region['time'].values)\n",
    "        hd0 = np.nansum((hrrr_daily_mean_region['vpd_2m'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))\n",
    "        hd1 = np.nansum((hrrr_daily_mean_region['vpd_2m'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(1,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd2 = np.nansum((hrrr_daily_mean_region['vpd_2m'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(2,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd3 = np.nansum((hrrr_daily_mean_region['vpd_2m'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(3,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd4 = np.nansum((hrrr_daily_mean_region['vpd_2m'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(4,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        hd5 = np.nansum((hrrr_daily_mean_region['vpd_2m'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')-np.timedelta64(5,'D')).values)*\n",
    "                     (intersection_sub['weights'].values))\n",
    "        w = np.nansum((hrrr_daily_mean_region['wind_speed'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))\n",
    "        hwp = np.nansum((hrrr_daily_mean_region['hwp'].sel(time=np.datetime64(today+ ' '+str(day_start_hour)+':00:00')).values)*(intersection_sub['weights'].values))\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60fe49",
   "metadata": {},
   "source": [
    "### ESI (choose weighted or unweighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4573291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def esi_timeseries(df, day_start_hour):\n",
    "    #preallocate space for the output\n",
    "    df_esi_weighted = pd.DataFrame({'day':np.zeros(len(df)),'ESI':np.zeros(len(df))})\n",
    "    df_esi_unweighted = pd.DataFrame({'day':np.zeros(len(df)),'ESI':np.zeros(len(df))})\n",
    "    \n",
    "    \n",
    "    #do the intersection, in parallel\n",
    "    esi_intersections = Parallel(n_jobs=8)(delayed(calculate_intersection)\n",
    "                                 (df_fire.iloc[ii:ii+1],'ESI_GRID',5000) \n",
    "                                 for ii in range(len(df_fire)))\n",
    "    print([esi_intersections[jj]['weights'].sum() for jj in range(len(esi_intersections))])\n",
    "\n",
    "    \n",
    "    fire_esi_intersection=gpd.GeoDataFrame(pd.concat(esi_intersections, ignore_index=True))\n",
    "    fire_esi_intersection.set_geometry(col='geometry')\n",
    "    \n",
    "    \n",
    "    fire_esi_intersection = fire_esi_intersection.set_index([str(day_start_hour)+ 'Z Start Day', 'lat', 'lon'])\n",
    "    \n",
    "    fire_esi_intersection_xr = fire_esi_intersection.to_xarray()\n",
    "    fire_esi_intersection_xr['weights_mask'] = xr.where(fire_esi_intersection_xr['weights']>0,1, np.nan)\n",
    "    print(fire_esi_intersection_xr)\n",
    "    #load in esi data associated with the fire\n",
    "    times = pd.date_range(np.datetime64(df[str(day_start_hour)+ 'Z Start Day'].iloc[0])-np.timedelta64(1,'W'),\n",
    "                        np.datetime64(df[str(day_start_hour)+ 'Z Start Day'].iloc[len(df)-1])+np.timedelta64(1,'W')+\n",
    "                        np.timedelta64(1,'D')) #buffer by a week on each side in case the weekly data timestep isn't a fire day\n",
    "    esi_filenames, esi_times = make_file_namelist(times,'/data2/lthapa/YYYY/ESI/DFPPM_4WK_YYYYJJJ.nc')\n",
    "    \n",
    "    print(esi_filenames)\n",
    "    print(esi_times)\n",
    "    \n",
    "    \n",
    "    #open the esi files\n",
    "    dat_esi = xr.open_mfdataset(esi_filenames,concat_dim='time',combine='nested',compat='override', coords='all')\n",
    "    dat_esi = dat_esi.assign_coords({'time': esi_times}) #assign coords so we can resample along time\n",
    "    dat_esi = dat_esi.where(dat_esi['Band1']!=-9999) #gets rid of the missing values!\n",
    "    dat_esi_daily = dat_esi.reindex(time=times,method='nearest') #makes the weekly data daily\n",
    "    dat_esi_daily_sub = dat_esi_daily.sel(lat = fire_esi_intersection_xr['lat'].values, \n",
    "                                          lon = fire_esi_intersection_xr['lon'].values,\n",
    "                      time = pd.to_datetime(fire_esi_intersection_xr[str(day_start_hour)+ 'Z Start Day'].values), method='nearest')\n",
    "                                          \n",
    "\n",
    "    print(df_esi_unweighted)\n",
    "    print(fire_esi_intersection_xr[str(day_start_hour)+ 'Z Start Day'].values)\n",
    "    \n",
    "    df_esi_weighted['day'].iloc[:] = pd.to_datetime(fire_esi_intersection_xr[str(day_start_hour)+ 'Z Start Day'].values)\n",
    "    df_esi_unweighted['day'].iloc[:] = pd.to_datetime(fire_esi_intersection_xr[str(day_start_hour)+ 'Z Start Day'].values)\n",
    "\n",
    "    varis=['ESI']\n",
    "    for var in varis:\n",
    "        df_esi_weighted[var] = np.nansum(fire_esi_intersection_xr['weights'].values*dat_esi_daily_sub['Band1'].values, axis=(1,2))\n",
    "        df_esi_unweighted[var] = np.nanmean(fire_esi_intersection_xr['weights_mask'].values*dat_esi_daily_sub['Band1'].values, axis=(1,2))\n",
    "    \n",
    "    return df_esi_weighted, df_esi_unweighted\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5a5384",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Fuel Loading (990m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "960de6cc",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fuel_loading_timeseries(df, day_start_hour):\n",
    "    #do the intersection, in parallel\n",
    "    tic = datetime.datetime.now()\n",
    "    fuel_fwi_intersections = Parallel(n_jobs=8)(delayed(calculate_intersection)\n",
    "                                 (df.iloc[ii:ii+1],'FUEL_FWI_GRID_990M',2000) \n",
    "                                 for ii in range(len(df)))\n",
    "    toc = datetime.datetime.now()\n",
    "    print(toc-tic)\n",
    "    print([fuel_fwi_intersections[jj]['weights'].sum() for jj in range(len(fuel_fwi_intersections))])\n",
    "    \n",
    "    fire_fuel_fwi_intersection=gpd.GeoDataFrame(pd.concat(fuel_fwi_intersections, ignore_index=True))\n",
    "    fire_fuel_fwi_intersection = fire_fuel_fwi_intersection.drop(columns='geometry')\n",
    "    fire_fuel_fwi_intersection = fire_fuel_fwi_intersection.set_index(['12Z Start Day','row', 'col'])\n",
    "    fire_fuel_fwi_intersection_xr = fire_fuel_fwi_intersection.to_xarray()\n",
    "    fire_fuel_fwi_intersection_xr['weights_mask'] = xr.where(fire_fuel_fwi_intersection_xr['weights']>0,1, np.nan)\n",
    "\n",
    "    path_fuel_fwi = '/data2/lthapa/ML_daily/fuel_fwi_990m.nc'\n",
    "    dat_fuel_fwi = xr.open_dataset(path_fuel_fwi) #map is fixed in time\n",
    "    dat_fuel_fwi_daily = dat_fuel_fwi.expand_dims({'time': pd.to_datetime(fire_fuel_fwi_intersection_xr['12Z Start Day'].values)}) #the PWS expanded over all the days\n",
    "\n",
    "    dat_fuel_fwi_sub_daily = dat_fuel_fwi_daily.sel(row = fire_fuel_fwi_intersection_xr['row'].values, \n",
    "                                        col = fire_fuel_fwi_intersection_xr['col'].values, method='nearest')\n",
    "    print(dat_fuel_fwi_sub_daily)\n",
    "    #preallocate space for the output\n",
    "    varis = ['day','Extreme_N', 'VeryHigh_N','High_N', 'Moderate_N', 'Low_N']\n",
    "    df_loading_weighted = generate_df(varis, len(df))\n",
    "    df_loading_unweighted = generate_df(varis, len(df))\n",
    "\n",
    "    df_loading_weighted['day'] = df['12Z Start Day'].values\n",
    "    df_loading_unweighted['day'] = df['12Z Start Day'].values\n",
    "\n",
    "    for var in varis[1:len(varis)]:\n",
    "        df_loading_weighted[var] = np.nansum(fire_fuel_fwi_intersection_xr['weights'].values*dat_fuel_fwi_sub_daily[var].values, axis=(1,2))\n",
    "        df_loading_unweighted[var] = np.nanmean(fire_fuel_fwi_intersection_xr['weights_mask'].values*dat_fuel_fwi_sub_daily[var].values, axis=(1,2))\n",
    "\n",
    "    return df_loading_weighted, df_loading_unweighted\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14107d1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### FWI IMERG (WEIGHTED VS UNWEIGHTED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f43e869d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def imerg_fwi_timeseries(df, day_start_hour):\n",
    "    varis = ['day','IMERG.FINAL.v6_DC','IMERG.FINAL.v6_DMC','IMERG.FINAL.v6_FFMC',\n",
    "             'IMERG.FINAL.v6_ISI','IMERG.FINAL.v6_BUI','IMERG.FINAL.v6_FWI',\n",
    "             'IMERG.FINAL.v6_DSR'] \n",
    "    df_imerg_weighted = generate_df(varis, len(df))\n",
    "    df_imerg_unweighted = generate_df(varis, len(df))\n",
    "    #do the intersection, in parallel\n",
    "    fwi_intersections = Parallel(n_jobs=1)(delayed(calculate_intersection)\n",
    "                                 (df.iloc[ii:ii+1],'IMERG_FWI_GRID',10000) \n",
    "                                 for ii in range(len(df)))\n",
    "    \n",
    "    print([fwi_intersections[jj]['weights'].sum() for jj in range(len(fwi_intersections))])\n",
    "\n",
    "    fire_fwi_intersection=gpd.GeoDataFrame(pd.concat(fwi_intersections, ignore_index=True))\n",
    "    fire_fwi_intersection.set_geometry(col='geometry')    \n",
    "    fire_fwi_intersection = fire_fwi_intersection.set_index([str(day_start_hour)+ 'Z Start Day', 'lat', 'lon'])\n",
    "\n",
    "    fire_fwi_intersection=fire_fwi_intersection[~fire_fwi_intersection.index.duplicated()]\n",
    "    \n",
    "    fire_fwi_intersection_xr = fire_fwi_intersection.to_xarray()\n",
    "    fire_fwi_intersection_xr['weights_mask'] = xr.where(fire_fwi_intersection_xr['weights']>0,1, np.nan)\n",
    "    print(fire_fwi_intersection_xr)\n",
    "    \n",
    "    #load in FWI data associated with the fire\n",
    "    times = pd.date_range(np.datetime64(df['UTC Day'].iloc[0]),\n",
    "                        np.datetime64(df['UTC Day'].iloc[len(df)-1])+\n",
    "                        np.timedelta64(1,'D'))\n",
    "    fwi_filenames,times_back_used = make_file_namelist(times,'/data2/lthapa/YYYY/FWI_IMERG/FWI.IMERG.FINAL.v6.Daily.Default.YYYYMMDD.nc')\n",
    "    \n",
    "    dat_fwi = xr.open_mfdataset(fwi_filenames,concat_dim='time',combine='nested',compat='override', coords='all')\n",
    "    dat_fwi = dat_fwi.assign_coords({'time': times_back_used}) #assign coords so we can resample along time\n",
    "    dat_fwi = dat_fwi.resample(time='1H').pad() #make the data hourly, so we can define the day as 12z-12z instead of 0z-0z\n",
    "    dat_fwi_mean = dat_fwi.resample(time='24H',base=day_start_hour ,label='left').mean(dim='time') #take the daily mean         \n",
    "    \n",
    "    #select the locations and times we want\n",
    "    dat_fwi_sub = dat_fwi_mean.sel(lat = fire_fwi_intersection_xr['lat'].values, \n",
    "                                    lon = fire_fwi_intersection_xr['lon'].values).sel(\n",
    "                            time = pd.to_datetime(fire_fwi_intersection_xr[str(day_start_hour)+ 'Z Start Day'].values+ ' '+\n",
    "                                         str(day_start_hour)+':00:00'), method='nearest')#these should be lined up correctly\n",
    "\n",
    "    print(dat_fwi_sub)\n",
    "    df_imerg_weighted['day'].iloc[:] = pd.to_datetime(fire_fwi_intersection_xr['12Z Start Day'].values)\n",
    "    df_imerg_unweighted['day'].iloc[:] = pd.to_datetime(fire_fwi_intersection_xr['12Z Start Day'].values)\n",
    "\n",
    "    for var in varis[1:len(varis)]:\n",
    "        df_imerg_weighted[var] = np.nansum(fire_fwi_intersection_xr['weights'].values*dat_fwi_sub[var].values, axis=(1,2)) #weighted average\n",
    "        df_imerg_unweighted[var] = np.nanmean(fire_fwi_intersection_xr['weights_mask'].values*dat_fwi_sub[var].values,axis=(1,2)) #mask and average\n",
    "    return df_imerg_weighted, df_imerg_unweighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019d721b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### PWS (NEED TO FIX ONCE WE DECIDE WEIGHTED VS UNWEIGHTED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff5bb9ee",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pws_timeseries(df, day_start_hour):\n",
    "    #do the intersection, in parallel\n",
    "    tic = datetime.datetime.now()\n",
    "    pws_intersections = Parallel(n_jobs=1)(delayed(calculate_intersection)\n",
    "                                 (df.iloc[ii:ii+1],'PWS_GRID',4000) \n",
    "                                 for ii in range(len(df)))\n",
    "    toc = datetime.datetime.now()\n",
    "    print(toc-tic)\n",
    "    print([pws_intersections[jj]['weights'].sum() for jj in range(len(pws_intersections))])\n",
    "\n",
    "    \n",
    "    fire_pws_intersection=gpd.GeoDataFrame(pd.concat(pws_intersections, ignore_index=True))\n",
    "    fire_pws_intersection.set_geometry(col='geometry')\n",
    "    fire_pws_intersection = fire_pws_intersection.set_index([str(day_start_hour)+ 'Z Start Day', 'lat', 'lon'])\n",
    "    \n",
    "    fire_pws_intersection_xr = fire_pws_intersection.to_xarray()\n",
    "    \n",
    "    #nc[\"cdd_hdd\"] = xr.where(nc[\"tavg\"] > 65, nc[\"tavg\"] - 65, 65 - nc[\"tavg\"])\n",
    "    fire_pws_intersection_xr['weights_mask'] = xr.where(fire_pws_intersection_xr['weights']>0,1, np.nan)\n",
    "    \n",
    "    #load in PWS data associated with the fire (it's only one dataset)  \n",
    "    #open the PWS files\n",
    "    path_pws = '/data2/lthapa/PWS_6_jan_2021.nc'\n",
    "    dat_pws = xr.open_dataset(path_pws) #map is fixed in time\n",
    "    #print(dat_pws)\n",
    "    \n",
    "    dat_pws = dat_pws.assign_coords({'time': pd.to_datetime(fire_pws_intersection_xr[str(day_start_hour)+ 'Z Start Day'].values)})\n",
    "    dat_pws_daily = dat_pws['Band1'].expand_dims({'time': pd.to_datetime(fire_pws_intersection_xr[str(day_start_hour)+ 'Z Start Day'].values)}) #the PWS expanded over all the days\n",
    "    \n",
    "    dat_pws_daily_sub = dat_pws_daily.sel(lat = fire_pws_intersection_xr['lat'].values, \n",
    "                                          lon = fire_pws_intersection_xr['lon'].values,\n",
    "                      time = pd.to_datetime(fire_pws_intersection_xr[str(day_start_hour)+ 'Z Start Day'].values), method='nearest')\n",
    "    ndays = len(fire_pws_intersection_xr[str(day_start_hour)+ 'Z Start Day'])\n",
    "    \n",
    "    #preallocate space for the output\n",
    "    df_pws_weighted = pd.DataFrame({'day':np.zeros(ndays),'PWS':np.zeros(ndays)})\n",
    "    df_pws_unweighted = pd.DataFrame({'day':np.zeros(ndays),'PWS':np.zeros(ndays)})\n",
    "\n",
    "\n",
    "    df_pws_weighted['day'].iloc[:] = pd.to_datetime(fire_pws_intersection_xr[str(day_start_hour)+ 'Z Start Day'].values)\n",
    "    df_pws_unweighted['day'].iloc[:] = pd.to_datetime(fire_pws_intersection_xr[str(day_start_hour)+ 'Z Start Day'].values)\n",
    "\n",
    "    varis=['PWS']\n",
    "    for var in varis:\n",
    "        df_pws_weighted[var] = np.nansum(fire_pws_intersection_xr['weights'].values*dat_pws_daily_sub.values, axis=(1,2)) #WEIGHTED AVERAGE\n",
    "        df_pws_unweighted[var] = np.nanmean(fire_pws_intersection_xr['weights_mask'].values*dat_pws_daily_sub.values, axis=(1,2)) #MASK AND AVERAGE\n",
    "        #print(np.nanmean(dat_pws_daily_sub.values, axis=(1,2)))\n",
    "        #df_pws[var] = dat_pws_daily_sub.mean(dim=['lat','lon'], skipna=True)\n",
    "    return df_pws_weighted, df_pws_unweighted\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dca6f5",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### SMOPS Data (need to fix once we decide on weighted vs unweighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a470c6cb",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def smops_timeseries(df, day_start_hour, use_weights):\n",
    "    varis_smops = ['day','Blended_SM']\n",
    "    df_smops = generate_df(varis_smops, len(df))\n",
    "    #do the intersection, in parallel\n",
    "    smops_intersections = Parallel(n_jobs=8)(delayed(calculate_intersection)\n",
    "                                 (df.iloc[ii:ii+1],'SMOPS_GRID',25000) \n",
    "                                 for ii in range(len(df)))\n",
    "    print([smops_intersections[jj]['weights'].sum() for jj in range(len(smops_intersections))])\n",
    "\n",
    "    fire_smops_intersection=gpd.GeoDataFrame(pd.concat(smops_intersections, ignore_index=True))\n",
    "    fire_smops_intersection.set_geometry(col='geometry')  \n",
    "    fire_smops_intersection = fire_smops_intersection.set_index([str(day_start_hour)+'Z Start Day', 'row', 'col'])\n",
    "    fire_smops_intersection=fire_smops_intersection[~fire_smops_intersection.index.duplicated()]\n",
    "\n",
    "    fire_smops_intersection_xr = fire_smops_intersection.to_xarray()\n",
    "    fire_smops_intersection_xr['weights_mask'] = xr.where(fire_smops_intersection_xr['weights']>0,1, np.nan)\n",
    "    #print(fire_smops_intersection_xr['weights_mask'])\n",
    "\n",
    "    #load in rave data associated with the fire\n",
    "    times = pd.date_range(np.datetime64(df[str(day_start_hour)+ 'Z Start Day'].iloc[0]),\n",
    "                        np.datetime64(df[str(day_start_hour)+ 'Z Start Day'].iloc[len(df)-1])+\n",
    "                        np.timedelta64(1,'D'))\n",
    "    smops_filenames,times_back_used = make_file_namelist(times,'/data2/lthapa/YYYY/SMOPS/NPR_SMOPS_CMAP_DYYYYMMDD.nc')\n",
    "\n",
    "    dat_smops = xr.open_mfdataset(smops_filenames,concat_dim='Time',combine='nested',compat='override', coords='all')\n",
    "    dat_smops = dat_smops.assign_coords({'Time': times_back_used}) #assign coords so we can select in time\n",
    "    #print(dat_smops['Time'].values)\n",
    "    #print(fire_smops_intersection_xr[str(day_start_hour)+'Z Start Day'].values)\n",
    "    #select the locations and times we want\n",
    "    dat_smops_sub = dat_smops.isel(Latitude = fire_smops_intersection_xr['row'].values.astype(int), \n",
    "                    Longitude = fire_smops_intersection_xr['col'].values.astype(int)).sel(\n",
    "                    Time = pd.to_datetime(fire_smops_intersection_xr[str(day_start_hour)+'Z Start Day'].values))#these should be lined up correctly\n",
    "\n",
    "    df_smops['day'].iloc[:] = pd.to_datetime(fire_smops_intersection_xr[str(day_start_hour)+'Z Start Day'].values)\n",
    "    for var in varis_smops[1:]:\n",
    "        dat_smops_sub[var]=dat_smops_sub[var].where(dat_smops_sub[var] != -0.0999) #mask out the ocean, there is no soil moisture here\n",
    "        if use_weights==True:\n",
    "            df_smops[var] = np.nansum(fire_smops_intersection_xr['weights'].values*dat_smops_sub[var].values, axis=(1,2))\n",
    "        else:\n",
    "            df_smops[var] = np.nanmean(fire_smops_intersection_xr['weights_mask'].values*dat_smops_sub[var].values, axis=(1,2)) #MASK AND AVERAGE\n",
    "    \n",
    "    return df_smops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73d26ed",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### RAVE (3KM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "58e50a30",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rave_timeseries(df, day_start_hour, sum_interval, use_weights):\n",
    "    varis = ['day','FRP_MEAN']#, 'FRP_SD', 'FRE']#, 'CO2', 'CO', 'SO2', 'OC','BC', 'PM25', 'NOx', 'NH3','TPM', 'VOCs', 'CH4'] #don't need 'area', it's the area of each cell\n",
    "    df_rave = generate_df(varis, len(df))\n",
    "\n",
    "    #do the intersection, in parallel\n",
    "    rave_intersections = Parallel(n_jobs=8)(delayed(calculate_intersection)\n",
    "                                 (df.iloc[ii:ii+1],'RAVE_GRID_3KM',3000) \n",
    "                                 for ii in range(len(df)))\n",
    "    print([rave_intersections[jj]['weights'].sum() for jj in range(len(rave_intersections))])\n",
    "\n",
    "    fire_rave_intersection=gpd.GeoDataFrame(pd.concat(rave_intersections, ignore_index=True))\n",
    "    fire_rave_intersection.set_geometry(col='geometry')    \n",
    "    #print(fire_rave_intersection)\n",
    "    fire_rave_intersection = fire_rave_intersection.set_index([str(day_start_hour)+ 'Z Start Day', 'row', 'col'])\n",
    "    fire_rave_intersection=fire_rave_intersection[~fire_rave_intersection.index.duplicated()]\n",
    "\n",
    "    fire_rave_intersection_xr = fire_rave_intersection.to_xarray()\n",
    "    fire_rave_intersection_xr['weights_mask'] = xr.where(fire_rave_intersection_xr['weights']>0,1, np.nan)\n",
    "\n",
    "    #load in rave data associated with the fire\n",
    "    times = pd.date_range(np.datetime64(df[str(day_start_hour)+ 'Z Start Day'].iloc[0]),\n",
    "                        np.datetime64(df[str(day_start_hour)+ 'Z Start Day'].iloc[len(df)-1])+\n",
    "                        np.timedelta64(1,'D'))\n",
    "    rave_filenames,times_back_used = make_file_namelist(times,\n",
    "                                                        '/data2/lthapa/YYYY/RAVE/MM/RAVE-HrlyEmiss-3km-CONUS_v1r1_blend_sYYYYMMDD.nc')                                                 \n",
    "    \n",
    "    #print(rave_filenames)\n",
    "    dat_rave = xr.open_mfdataset(rave_filenames,concat_dim='time',combine='nested',compat='override', coords='all')\n",
    "\n",
    "    dat_rave = dat_rave.resample(time=str(sum_interval)+'H',base=day_start_hour).sum(dim='time') #take the daily sum\n",
    "    \n",
    "    #select the locations and times we want\n",
    "    dat_rave_sub = dat_rave.isel(grid_yt = fire_rave_intersection_xr['row'].values.astype(int), \n",
    "                    grid_xt = fire_rave_intersection_xr['col'].values.astype(int)).sel(\n",
    "                    time = pd.to_datetime(fire_rave_intersection_xr[str(day_start_hour)+ 'Z Start Day'].values+\n",
    "                                         'T12:00:00.000000000'))#these should be lined up correctly\n",
    "    ndays = len(fire_rave_intersection_xr[str(day_start_hour)+ 'Z Start Day'])\n",
    "\n",
    "    df_rave['day'].iloc[:] = pd.to_datetime(fire_rave_intersection_xr[str(day_start_hour)+ 'Z Start Day'].values)\n",
    "    for var in varis[1:]:\n",
    "        if use_weights==True:\n",
    "            df_rave[var] = np.nansum(fire_rave_intersection_xr['weights'].values*dat_rave_sub[var].values, axis=(1,2))\n",
    "        else:\n",
    "            df_rave[var] = np.nansum(fire_rave_intersection_xr['weights_mask'].values*dat_rave_sub[var].values,axis=(1,2))\n",
    "    \n",
    "    return df_rave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8302673",
   "metadata": {},
   "source": [
    "### NCAR Moisture Data (DON'T WORRY ABOUT THIS FOR NOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3ae48fff",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a6f772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e07bdd3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dataset-Independent Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "779c4a0f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#makes and saves a geodataframe of a grid given the center and corner points for that grid as 2D matrices\n",
    "\n",
    "def build_one_gridcell(LAT_COR, LON_COR, LAT_CTR, LON_CTR, loc):\n",
    "    ii=loc[0]\n",
    "    jj=loc[1]\n",
    "    #corners of the grid box\n",
    "    sw = (LON_COR[ii, jj],LAT_COR[ii, jj]) #SW\n",
    "    se =(LON_COR[ii, jj+1],LAT_COR[ii, jj+1]) #SE\n",
    "    nw = (LON_COR[ii+1, jj],LAT_COR[ii+1, jj]) #NW\n",
    "    ne = (LON_COR[ii+1, jj+1],LAT_COR[ii+1, jj+1]) #NE\n",
    "    #polygon of the grid box        \n",
    "    poly_cell = Polygon([sw,nw,ne,se])\n",
    "    return LAT_CTR[ii,jj], LON_CTR[ii,jj],ii,jj,poly_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "02e97a26",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#poly is the polygon for one timestep (in lambert conformal conic)\n",
    "#dataset_name is the name of a model grid nc file\n",
    "#bf is the size of the polygon buffer\n",
    "\n",
    "def calculate_intersection(poly,dataset_name,bf):\n",
    "    print(poly['12Z Start Day'])\n",
    "    #load in the merra grid\n",
    "    grid = xr.open_dataset(dataset_name+'.nc')\n",
    "    \n",
    "    #get the bounds of the buffered polygons\n",
    "    #poly_latlon =poly#.to_crs(epsg=4326)\n",
    "    #bounds = poly_latlon.buffer(bf).bounds\n",
    "    bounds = poly.buffer(bf).to_crs(epsg=4326).bounds\n",
    "    print(bounds)\n",
    "    #first check for rows and cols, filtering near the polygon\n",
    "    [rows,cols] = np.where((grid.LAT_CTR.values>bounds['miny'].values)&\n",
    "                    (grid.LAT_CTR.values<bounds['maxy'].values)&\n",
    "                    (grid.LON_CTR.values>bounds['minx'].values)&\n",
    "                    (grid.LON_CTR.values<bounds['maxx'].values))\n",
    "    locs = zip(rows,cols)\n",
    "\n",
    "\n",
    "    #make a geodataframe (in paralell of the rows and cols)\n",
    "    results = Parallel(n_jobs=8)(delayed(build_one_gridcell)\n",
    "                                 (grid['LAT_COR'].values, grid['LON_COR'].values,\n",
    "                                  grid['LAT_CTR'].values, grid['LON_CTR'].values,loc) \n",
    "                                 for loc in locs)\n",
    "\n",
    "    #format the grid subset into a dataframs\n",
    "    df_grid=gpd.GeoDataFrame(results)\n",
    "    df_grid.columns = ['lat', 'lon', 'row', 'col', 'geometry']\n",
    "    df_grid.set_geometry(col='geometry',inplace=True,crs='EPSG:4326') #need to say it's in lat/lon before transform to LCC\n",
    "    df_grid=df_grid.to_crs(epsg=3347)\n",
    "\n",
    "    #intersect the polygon with the grid subset\n",
    "    intersection = gpd.overlay(df_grid, poly, how='intersection',keep_geom_type=False).drop_duplicates()\n",
    "    intersection['grid intersection area (ha)'] =intersection['geometry'].area/10000\n",
    "    intersection['weights'] = intersection['grid intersection area (ha)']/intersection['fire area (ha)'] \n",
    "    \n",
    "    return intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aab953a6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#LAT and LON are 2d arrays\n",
    "def calculate_grid_cell_corners(LAT, LON):\n",
    "    #we will assume the very edges of the polygons don't touch the boundary of the domain\n",
    "    lat_corners = (LAT[0:(LAT.shape[0]-1),  0:(LAT.shape[1])-1] + LAT[1:(LAT.shape[0]), 1:(LAT.shape[1])])/2\n",
    "    lon_corners = (LON[0:(LAT.shape[0]-1),  0:(LAT.shape[1])-1] + LON[1:(LAT.shape[0]), 1:(LAT.shape[1])])/2\n",
    "    return lat_corners, lon_corners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12b528a3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_file_namelist(time,base_filename):\n",
    "    filename_list = np.array([])\n",
    "    times_back_used = np.array([])\n",
    "    for jj in range(len(time)):\n",
    "        fname = base_filename.replace('YYYY',time[jj].strftime('%Y')).\\\n",
    "                                replace('MM',time[jj].strftime('%m')).\\\n",
    "                                replace('DD',time[jj].strftime('%d')).\\\n",
    "                                replace('HH',time[jj].strftime('%H')).\\\n",
    "                                replace('JJJ',time[jj].strftime('%j'))\n",
    "        #print(fname)\n",
    "        if exists(fname):\n",
    "            filename_list = np.append(filename_list,fname)\n",
    "            times_back_used = np.append(times_back_used,time[jj])\n",
    "    return filename_list, times_back_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9fe89e5a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_df(variables, length):\n",
    "    df = pd.DataFrame()\n",
    "    for vv in variables:\n",
    "        df[vv] = np.zeros(length)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e540d9c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84b02d9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
